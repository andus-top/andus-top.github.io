<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JAVA多线程-Object.wait()，Object.notify()，Object.notifyAll()]]></title>
    <url>%2F2019%2F11%2F18%2FJAVA%E5%A4%9A%E7%BA%BF%E7%A8%8B-Object-wait-%EF%BC%8CObject-notify-%EF%BC%8CObject-notifyAll%2F</url>
    <content type="text"><![CDATA[JAVA多线程学习笔记-Object.wait()，Object.notify()，Object.notifyAll() 深入理解 初步理解wait() 使线程停止运行，进入等待队列，会立刻释放对象锁。 wait(0) 代表的是无限期的等待，不能自动唤醒。 java.lang.Object类方法。 notify() 唤醒一个正在等待当前对象的waiting线程，遵循FIFO（先进先出）策略。 notify()调用后，并不是马上就释放对象锁，而是在相应的synchronized{…}语句块执行结束后，自动释放锁后，JVM在wait()对象锁的线程中随机选取一线程赋予其对象锁，唤醒线程，继续执行。这样就提供了在线间同步、唤醒的操作。 java.lang.Object类方法。 notifyAll() 唤醒正在等待当前对象的所有等待线程。 默认的唤醒策略是：LIFO（后进先出）。 java.lang.Object类方法。 wait()与notify() Obj.wait()，与Obj.notify()必须与synchronized(Obj)一起使用，也就是wait()是针对已经获取了Obj锁进行操作。 从语法角度来说就是Obj.wait()，Obj.notify()必须在、synchronized(Obj){…}语句块内。 从功能上来说wait就是指在该线程获取对象锁后，主动释放对象锁，同时本线程休眠。notify()就是对对象锁的唤醒操作。 wait()与sleep()共同点 都可以指定线程阻塞的时间。 都可以通过interrupt()方法打断(不建议使用)线程的暂停状态 ，从而使线程立刻抛出InterruptedException。如果线程A希望立即结束线程B，则可以对线程B对应的Thread实例调用interrupt方法。如果此刻线程B正在wait/sleep /join，则线程B会立刻抛出InterruptedException，在catch() {} 中直接return即可安全地结束线程。需要注意的是，InterruptedException是线程自己从内部抛出的，并不是interrupt()方法抛出的。对某一线程调用 interrupt()时，如果该线程正在执行普通的代码，那么该线程根本就不会抛出InterruptedException。但是，一旦该线程进入到 wait()/sleep()/join()后，就会立刻抛出InterruptedException 。 不同点 sleep()是Thread类的方法，wait()是Object类的方法。 sleep()方法没有释放锁，而wait()方法释放了锁。 wait，notify和notifyAll只能在同步控制方法或者同步控制块里面使用，而sleep可以在任何地方使用。 执行notify()后，waiting线程可能的状态线程a先抢到了对象o的锁，然后wait，然后b抢到了o的锁，然后b中调用o.notify并释放锁，此时a是running状态还是blocked状态？？如果b在执行完notify()后没有释放锁则线程a是阻塞等待，如果线程b执行完同步代码块（释放锁）后，则线程a就是就绪态，不一定是运行态 代码实操，三线程打印ABC 建立三个线程，A线程打印10次A，B线程打印10次B,C线程打印10次C，要求线程同时运行，交替打印10次ABC。这个问题用Object的wait()，notify()就可以很方便的解决。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class T08WaitNotify &#123; class T implements Runnable&#123; // 自定义线程名称 private String name; // 前一个obj private Object prev; // 当前obj private Object self; T(String name, Object prev, Object self)&#123; this.name = name; this.prev = prev; this.self = self; &#125; @Override public void run() &#123; int count = 0; while (count &lt; 10) &#123; synchronized (prev) &#123; synchronized (self) &#123; System.out.print(name); count++; self.notify(); &#125; try &#123; prev.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; T08WaitNotify obj = new T08WaitNotify(); Object oa = new Object(); Object ob = new Object(); Object oc = new Object(); T a = obj.new T("A", oc, oa); T b = obj.new T("B", oa, ob); T c = obj.new T("C", ob, oc); Thread ta = new Thread(a); Thread tb = new Thread(b); Thread tc = new Thread(c); ta.start(); Thread.sleep(1000); // 确保按照A、B、C顺序执行 tb.start(); Thread.sleep(1000); tc.start(); Thread.sleep(1000); &#125;&#125; 代码流程说明A线程启动，依次取得c、a锁，打印A，执行notify()唤醒任意一个a对象等待池中的线程，待synchronized(self){}块执行完后释放a锁，执行wait()，将A线程放入c对象相关的等待池中等待被唤醒，同时释放c锁； 1秒后，B线程启动，依次取得a、b锁，打印B，执行notify()唤醒任意一个b对象等待池中的线程，待synchronized(self){}块执行完后释放b锁，执行wait()，将B线程放入a对象相关的等待池中等待被唤醒，同时释放a锁；1秒后，C线程启动，依次取得b、c锁，打印C，执行notify()唤醒任意一个c对象等待池中的线程(此时A线程被唤醒)，待synchronized(self){}块执行完后释放c锁，执行wait()，将C线程放入b对象相关的等待池中等待被唤醒，同时释放b锁。 至此，完成一次ABC的打印。继续执行，从A线程开始，执行各自线程中的下一次循环。 特别说明Java源码中wait()默认调用的是wait(0)，表明线程无限等待，只能被其他线程唤醒。 参考https://blog.csdn.net/u010002184/article/details/82912225https://blog.csdn.net/hj1997a/article/details/84284973]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程(一)-基础篇]]></title>
    <url>%2F2019%2F11%2F08%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E4%B8%80-%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[JAVA多线程学习笔记-基础篇 基本概念进程 正在执行的程序，拥有独立的代码和数据空间 进程间的切换会有较大的开销 是资源分配的最小单位 一个进程可以包含一个或多个线程 至少包含一个线程 线程 程序中单独顺序的控制流 线程本身依靠进程进行运行，只能用分配给进程的资源和环境 同一类线程共享代码和数据空间，每个线程有独立的运行栈和程序计数器(PC) 是cpu调度的最小单位 单线程 进程中只存在一个线程，实际上主方法就是一个主线程 多线程 一个进程中运行多个线程 目的：更好的使用CPU资源 并行 真正的同时，多个cpu实例或者多台机器同时执行一段处理逻辑。 并发 通过cpu调度算法，让用户看上去同时执行，实际上从cpu操作层面不是真正的同时。使用TPS或者QPS来反应这个系统的并发处理能力 线程安全 并发的情况之下，该代码经过多线程使用，线程的调度顺序不影响任何结果 同步 保证共享资源的多线程访问成为线程安全 死锁 两个线程或两个以上线程都在等待对方执行完毕才能继续往下执行的时候就发生了死锁 结果就是这些线程都陷入了无限的等待中 常见线程名词主线程JVM调用程序main()所产生的线程。 当前线程无特别说明，一般指通过Thread.currentThread()来获取的进程。 后台线程 为其他线程提供服务的线程，也称为守护线程。 例如，JVM的垃圾回收线程就是一个后台线程。 区别在于，守护线程等待主线程，依赖于主线程结束而结束。 前台线程 接受后台线程服务的线程。 前台后台线程关系就像傀儡和幕后操纵者一样的关系。傀儡是前台线程、幕后操纵者是后台线程。 由前台线程创建的线程默认也是前台线程。可以通过isDaemon()和setDaemon()方法来判断和设置一个线程是否为后台线程。 Java中线程基础知识 main()方法也是一个线程，Thread.currentThread().getName() 得到的值为main，所有俗称main线程。 Java中，每次程序至少启动2个线程。一个main线程，一个是垃圾收集线程。 每当使用Java命令执行一个类的时候，实际上都会启动一个JVM，而每一个JVM就是在操作系统中启动了一个进程。（待验证） 每个对象都有一个锁来控制同步访问。Synchronized关键字可以和对象的锁交互，来实现线程的同步。 线程的实现继承java.lang.Thread类1234567891011121314151617181920212223242526272829public class MyThread01 extends Thread&#123; // 自定义线程名称 private String name; public MyThread01(String name)&#123; this.name = name; &#125; @Override public void run() &#123; super.run(); for (int i = 0; i &lt; 1000; i++) &#123; System.out.println(name+"："+i); &#125; &#125; public static void main(String[] args) &#123; MyThread01 t1 = new MyThread01("A"); MyThread01 t2 = new MyThread01("B"); // 此时程序依然是顺序执行 //t1.run(); //t2.run(); // System.out.println("----------------华丽分割线----------------"); // 通过start()方法启动线程，此时t1、t2线程交替执行 t1.start(); t2.start(); &#125;&#125; 说明 程序启动运行main()时候，Java虚拟机启动一个进程，主线程在main()方法调用时候被创建。 随着调用t1、t2的start()方法，另外两个线程也启动了，这样，整个应用就在多线程下环境下运行了。 调用start()方法并不会立刻执行线程的代码，而是使该线程变为可运行状态(后面线程生命周期章节会有讲解)，什么时候执行是由操作系统决定的。 线程的启动是通过start()方法，且不能重复调用。直接通过对象调用run()方法程序依然是顺序执行。 多运行几次代码，你会发现，多线程的执行顺序是不固定的，每次执行哪个线程是随机的。 查看Thread类的源码，可以发现Thread类是Runable接口的一个实现类。 实现Runable接口12345678910111213141516171819public class MyRunable02 implements Runnable &#123; @Override public void run() &#123; for (int i = 0; i &lt; 1000; i++) &#123; System.out.println(Thread.currentThread().getName()+"："+i); &#125; &#125; public static void main(String[] args) &#123; MyRunable02 r1 = new MyRunable02(); Thread t1 = new Thread(r1); Thread t2 = new Thread(r1); // 通过start()方法启动线程，此时t1、t2线程交替执行 t1.start(); t2.start(); &#125;&#125; 说明 通过实现Runable接口，使普通的Java类具有了多线程的特性。 run()方法是多线程程序的一个约定，所有多线程的代码都在run()方法里面。 仔细观察代码可以发现，最终线程的启动还是通过Thread类的start()方法。 实际上Java中所有多线程代码都是通过Thread.start()方法来启动的。因此，熟悉Thread类的API是Java并发编程的基础。 推荐使用Runable的方式 避免Java单继承的限制。 降低数据与代码的耦合度，代码与数据独立，代码逻辑可被多个线程共享。 线程池不能直接放入Thread类对象，可以放入Runable、Callable对象。 实现Callable接口 这里先简单普及一下，在Java中Runnable的run()方法没有返回值，而Callable接口里的call()方法可返回值。 Java常用Future接口来代表Callable接口里的call()方法的返回值，并为Future接口提供了一个FutureTask实现类。 FutureTask类同时实现了Future、Runnable接口。 并发执行同一个FutureTask123456789101112131415161718192021222324252627282930313233343536373839404142434445public class MyCallable03 implements Callable&lt;String&gt; &#123; private int num = 5; @Override public String call() throws Exception &#123; int sum = 0; for (int i = 0; i &lt; 10; i++) &#123; sum+=i; System.out.println(Thread.currentThread().getName()+": "+sum); &#125; // 虽然是并发执行，但是最终只会执行其中一个 if("Thread-0".equals(Thread.currentThread().getName()))&#123; this.num = 0; // 此处this就是c1 System.out.println("now Thread: Thread-0"); &#125;else if("Thread-1".equals(Thread.currentThread().getName()))&#123; this.num=10; // 此处this就是c1 System.out.println("now Thread: Thread-1"); &#125; return Thread.currentThread().getName()+" result："+sum; &#125; public static void main(String[] args) throws Exception &#123; MyCallable03 c1 = new MyCallable03(); FutureTask&lt;String&gt; f1 = new FutureTask&lt;String&gt;(c1); // t1、t2并发执行 Thread t1 = new Thread(f1); Thread t2 = new Thread(f1); t1.start(); t2.start(); System.out.println(f1.isDone()); // f1执行完毕才是true Thread.sleep(2000); // main线程sleep，保证t1、t2执行完毕 if(f1.isDone())&#123; System.out.println("结果： "+f1.get()); // 结果： Thread-1 result：45 System.out.println(t1.getState().toString()); // TERMINATED System.out.println(t2.getState().toString()); // TERMINATED &#125; System.out.println("num："+c1.num); // 执行的t1为num：0，执行的t2为num：10 &#125;&#125; 说明 由t1、t2的状态得出，两个线程都执行了。 根据num的值与输出的“now Thread: Thread-?”结果得出，一个FutureTask只会被执行一次。 先后执行同一个FutureTask123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class MyCallable0302 implements Callable&lt;String&gt; &#123; private static int num = 5; @Override public String call() throws Exception &#123; int sum = 0; for (int i = 0; i &lt; 10; i++) &#123; sum+=i; System.out.println(Thread.currentThread().getName()+": "+sum); &#125; // 最终只会执行其中一个 if(Thread.currentThread().getName().equals("Thread-0"))&#123; this.num = 0; // 此处this就是c1 System.out.println("now Thread: Thread-0"); &#125;else if("Thread-1".equals(Thread.currentThread().getName()))&#123; this.num=10; // 此处this就是c1 System.out.println("now Thread: Thread-1"); &#125; return Thread.currentThread().getName()+" result："+sum; &#125; public static void main(String[] args) throws Exception &#123; MyCallable0302 c1 = new MyCallable0302(); FutureTask&lt;String&gt; f1 = new FutureTask&lt;String&gt;(c1); Thread t1 = new Thread(f1); t1.start(); // 此时会继续执行主线程的代码。 System.out.println(f1.isDone()); // f1执行完毕才是true Thread.sleep(2000); // main线程sleep，保证t1执行完毕 if(f1.isDone())&#123; System.out.println("第一次： "+f1.get()); // 第一次： Thread-0 result：45 System.out.println("num："+c1.num); // num：0 System.out.println(t1.getState().toString()); // TERMINATED Thread t2 = new Thread(f1);// 此时，f1中的call方法都不会被执行，相当于没传f1 t2.start(); Thread.sleep(2000); // main线程sleep，保证t2执行完毕 System.out.println("第二次： "+f1.get()); // 第二次： Thread-0 result：45（依然是t1执行f1的结果） System.out.println(t2.getState().toString()); // TERMINATED &#125; System.out.println("num："+c1.num); // num：0 (依然是t1执行f1的结果) // 第二个线程执行第一个线程已经执行完的f1，第二个线程会执行,但是不是执行f1中的call方法 &#125;&#125; 说明 由两个“依然是t1执行f1的结果”进一步验证了：一个FutureTask只会被执行一次。 线程中常用方法 方法名 作用 详解 getName() 获取线程名称 Thread.currentThread().getName() currentThread() 获取当前线程对象 currentThread()方法是Thread类的静态方法，如果调用线程对象.currentThread()方法并不能获取到调用的线程对象，反正在哪一个线程里面执行了currentThread()方法得到的就是哪个线程对象 isAlive() 线程是否存活 如果线程已经启动，并且没有died返回true join() 等待该线程终止 在一个线程中调用other.join(),将等待other执行完后才继续本线程。(见补充说明) sleep() 线程的休眠 见补充说明 yield() 线程礼让 当前线程可转让cpu控制权，让别的就绪状态线程运行(见补充说明) interrupte() 友好的终止线程执行 保证程序逻辑完整性(见补充说明) wait() 线程挂起，进入等待队列 JAVA多线程-Object.wait()，Object.notify()，Object.notifyAll() notify() 唤醒等待队列中任意一个线程 JAVA多线程-Object.wait()，Object.notify()，Object.notifyAll() notifyAll() 唤醒等待队列中所有线程 JAVA多线程-Object.wait()，Object.notify()，Object.notifyAll() suspend() 线程挂起 不会释放对象锁，不推荐使用，常与resume()配套使用 resume() 唤醒挂起线程 不推荐使用，常与suspend()配套使用。如果 resume() 操作出现在 suspend() 之前执行，很容易造成死锁 activeCount() 进程中活跃的线程数 enumerate() 枚举程序中的线程 isDaemon() 一个线程是否为守护线程 setDaemon() 设置一个线程为守护线程 用户线程和守护线程的区别在于，是否等待主线程依赖于主线程结束而结束 setPriority() 设置一个线程的优先级 取值1-10 补充说明 join()这里是指的主线程等待子线程的终止。如果还要其他线程的话，调用join()方法的线程会与除主线程外的其他线程并发执行。所以，当主线程需要用到子线程的处理结果，这个时候就要用到join()方法。 yield()让当前运行线程从运行状态(Running)回到可运行状态(Runable)，以允许具有相同优先级的其他线程获得运行机会。因此，使用yield()的目的是让相同优先级的线程之间能适当的轮转执行。但是，实际中无法保证yield()达到让步目的，因为让步的线程还有可能被线程调度程序再次选中。 sleep()线程休眠，不会释放锁sleep()方法是Thread类的静态方法，如果调用线程对象.sleep()方法并不是该线程就休眠，反正在哪一个线程里面执行了sleep()方法哪一个线程就休眠。在sleep()休眠时间期满后，该线程不一定会立刻获得cpu资源，除非此线程具有更高的优先级。 sleep()、yield()的区别sleep()使当前线程进入阻塞状态，所以执行sleep()的线程在指定的时间内肯定不会被执行。这段时间是通过程序设定的。yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程可能进入到可执行状态后又马上被执行，这段时间是不可设定的。实际上，yield()方法对应了如下操作：先检测当前是否有相同优先级的线程处于同可运行状态，如有，则把 CPU 的占有权交给此线程，否则，继续运行原来的线程。所以yield()方法称为“退让”，它把运行机会让给了同等优先级的其他线程。sleep() 方法允许较低优先级的线程获得运行机会，但 yield() 方法执行时，当前线程仍处在可运行状态，所以，不可能让出较低优先级的线程些时获得 CPU 占有权。所以，如果较高优先级的线程没有调用 sleep 方法，又没有受到 I\O 阻塞，那么，较低优先级线程只能等待所有较高优先级的线程运行结束，才有机会运行。 interrupte()不要以为它是中断某个线程！它只是向线程发送一个中断信号。正常运行的程序不去检测状态就不会终止。只会影响到wait状态、sleep状态和join状态。被打断的线程会抛出InterruptedException。 stop()是一种”恶意” 的中断,一旦执行stop方法,即终止当前正在运行的线程,不管线程逻辑是否完整,这是非常危险的。 综合interrupte()、stop()，建议使用自定义的标志位决定线程的执行情况 12345678910111213141516class SafeStopThread extends Thread &#123; //此变量必须加上volatile private volatile boolean stop = true; @Override public void run() &#123; //判断线程体是否运行 while (stop) &#123; // Do Something System.out.println("Stop"); &#125; &#125; //线程终止 public void terminate() &#123; stop = false; &#125;&#125; 线程优先级 Java线程的优先级用整数表示，取值范围是1~10，数值越大优先级越高。优先级高的线程获得更多运行机会的机会越大。 Thread类的setPriority()和getPriority()方法分别用来设置和获取线程的优先级。 线程的优先级有继承关系，比如A线程中创建了B线程，那么B将和A具有相同的优先级。 JVM提供了10个线程优先级，但与常见的操作系统都不能很好的映射。如果希望程序能移植到各个操作系统中，应该仅仅使用Thread类有以下三个静态常量作为优先级，这样能保证同样的优先级采用了同样的调度方式。 123Thread.MIN_PRIORITY =&gt; 1Thread.MAX_PRIORITY=&gt; 10Thread.NORM_PRIORITY=&gt; 5(默认) 说明 线程的优先级有可能影响线程的执行顺序，不是绝对的。 线程同步有共享数据时就需要同步！！！ 同步代码块在代码块上加上”synchronized” 关键字，则此代码块就成为同步代码块 123synchronized(同步对象)&#123; 需要同步的代码块; &#125; 同步方法在方法返回修饰符之前加上”synchronized” 关键字，则此方法就成为同步方法 123synchronized void 方法名()&#123; ....&#125; 线程生命周期 新建(New)：新建一个线程对象 可运行(Runable)其他线程调用该线程的start()方法。不能对同一线程对象两次调用start()方法。该线程位于可运行线程池中，等待获取cpu使用权。 运行(running)：获取了cpu使用权，执行程序代码 阻塞(block)：因某种原因[]放弃了cpu使用权，暂时停止运行。直到线程再次进入可运行状态才有可能获取cpu使用权，转为运行状态。 等待阻塞：执行了wait()方法。jvm把线程放入等待队列，释放锁。被notify(), notifyAll()进入锁池中 同步阻塞：获取同步锁时，该锁被别的线程占用。jvm把线程放入锁池中 其他阻塞：执行sleep(毫秒)、join方法、或者发出I/O请求。不释放锁 死亡(dead)：线程执行完成或因异常退出，该线程结束生命周期。死亡的线程不可再次恢复。]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>多线程</tag>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】Docker，Docker Compose，Docker Swarm，Kubernetes(k8s)之间的区别]]></title>
    <url>%2F2019%2F11%2F01%2FDocker%EF%BC%8CDocker-Compose%EF%BC%8CDocker-Swarm%EF%BC%8CKubernetes-k8s-%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[Docker相关技术的角色定位 原文链接：https://blog.csdn.net/notsaltedfish/article/details/80959913 最近在学习Docker容器，了解到一些相关的技术，像是Kubernetes，Docker-compose，Docker Swarm，分不清这些东西之间的区别，特意去研究了一下，分享一下，适合刚入门学习容器的同学了解。 DcokerDocker 这个东西所扮演的角色，容易理解，它是一个容器引擎，也就是说实际上我们的容器最终是由Docker创建，运行在Docker中，其他相关的容器技术都是以Docker为基础，它是我们使用其他容器技术的核心。 Docker ComposeDocker Compose 是用来管理你的容器的，有点像一个容器的管家，想象一下当你的Docker中有成百上千的容器需要启动，如果一个一个的启动那得多费时间。有了Docker-Compose你只需要编写一个文件，在这个文件里面声明好要启动的容器，配置一些参数，执行一下这个文件，Docker就会按照你声明的配置去把所有的容器启动起来，但是Docker-Compose只能管理当前主机上的Docker，也就是说不能去启动其他主机上的Docker容器 Docker SwarmDocker Swarm 是一款用来管理多主机上的Docker容器的工具，可以负责帮你启动容器，监控容器状态，如果容器的状态不正常它会帮你重新帮你启动一个新的容器，来提供服务，同时也提供服务之间的负载均衡，而这些东西Docker-Compose 是做不到的 KubernetesKubernetes它本身的角色定位是和Docker Swarm 是一样的，也就是说他们负责的工作在容器领域来说是相同的部分，当然也有自己一些不一样的特点。这个就像是Eclipse和IDEA一样，也是一个跨主机的容器管理平台。它是谷歌公司根据自身的多年的运维经验研发的一款容器管理平台。而Docker Swarm则是由Docker 公司研发的。 既然这两个东西是一样的，那就面临选择的问题，应该学习哪一个技术呢?实际上这两年Kubernetes已经成为了很多大公司的默认使用的容器管理技术，而Docker Swarm已经在这场与Kubernetes竞争中已经逐渐失势，如今容器管理领域已经开始已经逐渐被Kubernetes一统天下了。所以建议大家学习的时候，应该多考虑一下这门技术在行业里面是不是有很多人在使用。 需要注意的是，虽然Docker Swarm在与Kubernetes的竞争中败下阵来，但是这个跟Docker这个容器引擎没有太大关系，它还是整个容器领域技术的基石，Kubernetes离开他什么也不是。 总结Docker是容器技术的核心、基础，Docker Compose是一个基于Docker的单主机容器编排工具，功能并不像Docker Swarm和Kubernetes是基于Dcoker的跨主机的容器管理平台那么丰富。]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Docker Compose</tag>
        <tag>Docker Swarm</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue SpringBoot实现Html和Markdown格式内容(含图片上传)保存到MySQL]]></title>
    <url>%2F2019%2F10%2F24%2FVue%20SpringBoot%E5%AE%9E%E7%8E%B0Html%E5%92%8CMarkdown%E6%A0%BC%E5%BC%8F%E5%86%85%E5%AE%B9(%E5%90%AB%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0)%E4%BF%9D%E5%AD%98%E5%88%B0MySQL%2F</url>
    <content type="text"><![CDATA[Vue+Mavon-editor+SpringBoot实现Markdown、Html格式内容保存到MySQL 实现功能 本文代码实现了前端Markdown格式的博文保存到MySQL的功能。 包括文章中图片的上传，在用户选择图片后就将其传到后端并将图片的链接返回给前端，填入到指定的位置。 遇到的问题 由于Markdown编辑器原因，返回的图片路径不能有\与空格 如果遇到第二次进入编辑页面不能显示文章内容，那么在下方getArticle()方法中，处理响应的最后一行加入 12// 解决第二次进入不能显示内容bugthis.$refs.md.d_value = response.data.markdownContent Html格式内容中的部分特殊符号会被JAVA替换掉，导致回显的页面样式有出入。建议不使用Html格式，使用Markdown格式 前端 前端是用Vue-cli搭建的项目工程，运行在WebStorm中。 安装依赖 数据请求相关 12npm install axios --savenpm install qs --save 安装Markdown编辑器 1npm install mavon-editor --save 使用mavon-editor，请自行参考如何使用。目前不支持流程图、序列图、甘特图 其他依赖 1234npm install style-loadernpm install css-loadernpm install sass-loadernpm install babel-loader --save 在main.js中引入mavonEditor123import mavonEditor from 'mavon-editor'import 'mavon-editor/dist/css/index.css'Vue.use(mavonEditor) ArticleMarkdown.vue组件，实现Markdown博文的存取代码导读getArticle()：通过文章id获取内容saveArticle()：提交博文内容到后端。同时提交了html、markdown格式的内容，见 12var htmlCode = this.$refs.md.d_render; var markdownCode = this.$refs.md.d_value; imgAdd(pos, file)：上传单张图片。file图片对象，pos图片下标，后端返回图片链接地址时，用于定位imgDel(pos)：删除图片mulUploadimg() ：上传多张图片。图片对象存放在data中img_file对象中imgDelMul(pos)：删除多张图片。 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127&lt;template&gt; &lt;div&gt; &lt;mavon-editor ref="md" class="md" v-model="sqlData.markdown" @imgAdd="imgAdd" @imgDel="imgDel" @save="saveArticle"/&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; import axios from 'axios' import qs from 'qs' const area_axios = axios.create(&#123; headers: &#123;'Content-Type': 'application/json;charset=utf-8',&#125;,// 设置传输内容的类型和编码 withCredentials: true,// 指定某个请求应该发送凭据 &#125;) const file_axios = axios.create(&#123; headers: &#123;'Content-Type': 'multipart/form-data',&#125;,// 设置传输内容的类型和编码 withCredentials: true,// 指定某个请求应该发送凭据 &#125;) const area_form_axios = axios.create(&#123; headers: &#123;'Content-Type': 'application/x-www-form-urlencoded',&#125;,// 设置传输内容的类型和编码 withCredentials: true,// 指定某个请求应该发送凭据 &#125;) export default &#123; name: "Markdown", data() &#123; return &#123; sqlData:&#123; markdown:'', html:'' &#125;, img_file: &#123;&#125;,// 一次上次多张图片时使用 &#125;; &#125;, mounted:function ()&#123; getArticle() &#125;, methods: &#123; // 获取文章 getArticle()&#123; area_form_axios.get('/api/get',&#123; params:&#123;id: 12 &#125; &#125;,) .then(response =&gt; &#123; console.log(this.sqlData) this.sqlData = response.data &#125;) .catch(err =&gt; &#123; alert("请求失败") &#125;) &#125;, // 保存文章 saveArticle()&#123; var htmlCode = this.$refs.md.d_render; var markdownCode = this.$refs.md.d_value; if(htmlCode.length == 0 || markdownCode.length == 0)&#123; alert("请填写") return; &#125; area_axios(&#123; url: '/api/add', method: 'post', data: JSON.stringify(&#123;'markdown':markdownCode,'html':htmlCode&#125;), &#125;).then((response) =&gt; &#123; if(response.data &gt; 0)&#123; alert("成功") &#125;else &#123; alert("失败") &#125; &#125;) &#125;, // 添加图片 imgAdd(pos, file)&#123; console.log("pos:"+pos) // 第一步.将图片上传到服务器. var formdata = new FormData(); formdata.append('pic', file); file_axios(&#123; url: '/api/img_upload', method: 'post', data: formdata, &#125;).then((response) =&gt; &#123; // 第二步.将返回的url替换到文本原位置 var url = response.data; //通过引入对象获取: import &#123;mavonEditor&#125; from ... 等方式引入后，此时$vm即为mavonEditor //通过$refs获取: html声明ref : &lt;mavon-editor ref=md &gt;&lt;/mavon-editor&gt;， 此时$vm为 this.$refs.md` this.$refs.md.$img2Url(pos, url); &#125;) &#125;, // 删除图片 imgDel(pos)&#123; console.log("imgDel pos:"+pos) &#125;, // 多张图片 mulUploadimg()&#123; // 第一步.将图片上传到服务器. var formdata = new FormData(); for(var _img in this.img_file)&#123; debugger // 后台需要图片的key一致 formdata.append('pics', this.img_file[_img]); &#125; file_axios(&#123; url: '/api/mul_img_upload', method: 'post', data: formdata, &#125;).then((res) =&gt; &#123; /** * 例如：返回数据为 res = [[pos, url], [pos, url]...] * pos 为原图片标志（0） * url 为上传后图片的url地址 */ // 第二步.将返回的url替换到文本原位置![...](0) -&gt; ![...](url) var idx_url = res.data; idx_url.forEach(item =&gt; &#123; //通过引入对象获取: import &#123;mavonEditor&#125; from ... 等方式引入后，此时$vm即为mavonEditor //通过$refs获取: html声明ref : &lt;mavon-editor ref=md &gt;&lt;/mavon-editor&gt;， 此时$vm为 this.$refs.md` this.$refs.md.$img2Url(item[0], item[1]); &#125;); &#125;) &#125;, // 多张图片 imgDelMul(pos)&#123; console.log("imgDel pos:"+pos) delete this.img_file[pos]; &#125;, &#125; &#125;&lt;/script&gt; 跨域配置vue-axios 前后端分离 跨域访问的实现 后端文件上传相关配置application.properties文件中 12345spring.servlet.multipart.enabled=true# 最大支持文件大小spring.servlet.multipart.max-file-size=10MB# 最大支持请求大小spring.servlet.multipart.max-request-size=50MB 配置拦截器1234567891011121314151617181920@Componentpublic class CrossDomainInterceptor extends HandlerInterceptorAdapter &#123; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; // 允许客户端携带跨域cookie，此时origin值不能为“*”，只能为指定单一域名。！！开发时不要使用localhost访问 response.setHeader("Access-Control-Allow-Credentials", "true"); // 允许指定域访问跨域资源 //response.setHeader("Access-Control-Allow-Origin", "http://127.0.0.1:9006, http://127.0.0.1:8080"); response.setHeader("Access-Control-Allow-Origin", origin);// * // 允许浏览器发送的请求消息头 //response.setHeader("Access-Control-Allow-Headers", "*"); response.setHeader("Access-Control-Allow-Headers", request.getHeader("Access-Control-Request-Headers")); // 允许浏览器在预检请求成功之后发送的实际请求方法名 //response.setHeader("Access-Control-Allow-Methods", "DEFAULT,POST,PATCH,PUT,OPTIONS,DELETE,HEAD"); response.setHeader("Access-Control-Allow-Methods", request.getHeader("Access-Control-Request-Method")); // 浏览器缓存预检请求结果时间,单位:秒 response.setHeader("Access-Control-Max-Age", "86400"); return true; &#125;&#125; 数据接口12345678910111213141516171819202122232425262728@Controller@RequestMapping("")public class MarkdownController &#123; /** * 获取文章 * id: 文章id * @author YSL * 2019-03-04 15:38 */ @GetMapping("/get") @ResponseBody public Bean test(@RequestParam("id")Integer id)&#123; // 获取数据库中的数据，请自行实现。 return vueMarkdownMapper.query(id); &#125; /** * 保存文章到数据库。 * bean：前端传回JSON.stringify(&#123;'markdown':markdownCode,'html':htmlCode&#125;)格式的数据即可 * @author YSL * 2019-03-04 15:26 */ @PostMapping("/add") @ResponseBody public int test(@RequestBody Bean bean)&#123; return vueMarkdownMapper.add(bean);// 保存数据到数据库，请自行实现 &#125; &#125; 图片上传代码导读@RequestMapping(“/img_upload”)：单张图片上传，上传到blog_files/pictures目录下，返回图片url。@RequestMapping(“/mul_img_upload”)：多张图片上传。上传到blog_files/pictures目录下，返回new String[]{图片下标, 图片url}格式的list。fileUpload()：文件上传。上传到blog_files/files目录下。upload()：真正实现文件上传的方法，基于MultipartFile实现 说明 图片与文件都是上传到tomcat/webapps/blog_files/目录下，blog_files是我专门用来保存图片的一个web工程，方便通过http访问到图片，给前端返回的图片地址也是http格式的。 在图片和文件上传的同时会备份，案例总备份路径：D:/webserver_bak/blog/ 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155public class FileController &#123; /** * 图片上传(一张) * @param pic 需要上传的图片 * @return 图片url * @author YSL * 2019-03-01 17:14 */ @RequestMapping("/img_upload") @ResponseBody public String imgUpload(@RequestParam(value = "pic", required = false) MultipartFile pic, HttpServletRequest request)&#123; List&lt;String&gt; urlList = upload(new MultipartFile[]&#123;pic&#125;, "pictures", request); return urlList != null ? urlList.get(0) : ""; &#125; /** * 图片上传(多张) * @param pics 需要上传的图片 * @return 图片下标和url * @author YSL * 2019-03-01 17:14 */ @RequestMapping("/mul_img_upload") @ResponseBody public List&lt;String[]&gt; imgUpload(@RequestParam(value = "pics", required = false) MultipartFile[] pics, HttpServletRequest request)&#123; List&lt;String&gt; urlList = upload(pics, "pictures", request); List&lt;String[]&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; urlList.size() ; i++) &#123; String[] idx_url = new String[2]; // 图片下标 idx_url[0]=i+""; // 拼接url idx_url[1] = urlList.get(i); list.add(idx_url); &#125; return list; &#125; /** * 文件上传 * @param files 需要上传的文件 * @return 文件url * @author YSL * 2019-03-01 17:14 */ @RequestMapping("/file_upload") @ResponseBody public List&lt;String&gt; fileUpload(@RequestParam(value = "files", required = false) MultipartFile[] files, HttpServletRequest request)&#123; List&lt;String&gt; urlList = upload(files, "pictures", request); return urlList; &#125; /** * 文件/图片上传。并做备份&lt;br/&gt; * 路径不能有反斜线和空格 &lt;br/&gt; * 上传路径：.../webapps/blog_files/pictures/20190301/图片 &lt;br/&gt; * 上传路径：.../webapps/blog_files/files/20190301/文件 &lt;br/&gt; * 备份路径：.../webserver_bak/blog/pictures/20190301/图片 &lt;br/&gt; * 备份路径：.../webserver_bak/blog/files/20190301/文件 * @param files 需要上传的文件 * @param categoryPath 类别路径，pictures/files * @return 上传成功，返回文件url * @author YSL * 2019-03-01 16:45 */ public List&lt;String&gt; upload(MultipartFile[] files, String categoryPath, HttpServletRequest request)&#123; // 非空判定 if(files == null || files.length == 0)&#123; return new ArrayList&lt;&gt;(); &#125; // 专门存放文件工程名称（是一个javaweb工程，方便图片直接通过http访问） String fileProject = "blog_files"; // 备份路径 String bakPath = "D:/webserver_bak/blog/";blog_files //http://localhost:7989/ String ipPort = request.getScheme()+"://"+request.getServerName()+":"+request.getServerPort() + "/"; /** * 获取项目绝对路径，格式，D:\tomcats\apache-tomcat-8.0.52\webapps\boot\。 * markdown编辑器图片路径不能有\，所以替换为/ * 注意：.replace("//", "/"); 与 replaceAll("\\\\", "/"); */ String rootPath = request.getSession().getServletContext().getRealPath("").replaceAll("\\\\", "/"); // 项目路径。/boot String contextPath = request.getContextPath(); rootPath = rootPath.substring(0, rootPath.lastIndexOf(contextPath.replace("/",""))); StringBuilder fileRoot = new StringBuilder(""); // 工程名称 fileRoot.append(fileProject); fileRoot.append("/"); // 类别目录 fileRoot.append(categoryPath); fileRoot.append("/"); // 文件目录，图片上传失败时使用 String picRootPath = fileRoot.toString(); String day = new SimpleDateFormat("yyyyMMdd").format(new Date()); // 日期目录 fileRoot.append(day); fileRoot.append("/"); // 文件最终保存目录 String fileDir = fileRoot.toString(); List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (MultipartFile multipartFile : files) &#123; // 文件名称。markdown编辑器图片路径不能有空格 String upFileName = multipartFile.getOriginalFilename().replaceAll("\\s+", ""); String filename = new SimpleDateFormat("HHmmss").format(new Date()) + "_" + UUID.randomUUID().toString() + "_" + upFileName; String filePathName = rootPath + fileDir + filename; File destFile = new File(filePathName); try &#123; // 复制临时文件到指定目录下, 会创建没有的目录 FileUtils.copyInputStreamToFile(multipartFile.getInputStream(), destFile); // 拼接url list.add(ipPort + fileDir + filename); // 备份 File bakFile = new File(bakPath + fileDir + filename); FileUtils.copyInputStreamToFile(multipartFile.getInputStream(), bakFile); &#125; catch (UnsupportedEncodingException e2) &#123; e2.printStackTrace(); if("pictures".equals(categoryPath))&#123; // 默认图片 list.add(picRootPath+"default.jpg"); &#125;else&#123; list.add(""); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); if("pictures".equals(categoryPath))&#123; // 默认图片 list.add(picRootPath+"default.jpg"); &#125;else&#123; list.add(""); &#125; &#125; &#125; return list; &#125; &#125; 数据库 字段名 类型 长度 备注 id int 默认 文章id markdown text markdown格式内容 html text html格式内容 参考https://blog.csdn.net/qq_32407233/article/details/84656914https://blog.csdn.net/wangjun5159/article/details/48809427https://segmentfault.com/q/1010000016563395]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前端记录(自用)]]></title>
    <url>%2F2019%2F10%2F18%2F%E5%89%8D%E7%AB%AF%E8%AE%B0%E5%BD%95-%E8%87%AA%E7%94%A8%2F</url>
    <content type="text"><![CDATA[自己实在是太菜了，一些常规操作代码始终记不住。在这里一一记录，方便查看。 Jquery使用 $(‘img:not(#andus-head-img)’)：所有img元素，但排除ID为#andus-head-img的元素 $(“.intro.demo”)：选择同时包含class=”intro”和class=”demo”的元素 $(“.a, .b”)：同时选择多个类型元素。(选择包含a或者包含b的元素) $(‘父标签’).on(‘click’, ‘子标签’, function(){})：监听动态添加的标签 $(this)：当前 HTML 元素 $(“p”)：所有 &lt;p&gt; 元素 $(“p.intro”)：所有 class=”intro” 的&lt;p&gt; 元素 $(“.intro”)：所有 class=”intro” 的元素 $(“#intro”) id=”intro” 的元素 $(“[href$=’.jpg’]”)：所有带有以 “.jpg” 结尾的属性值的 href 属性 $(“div#intro .head”) id=”intro” 的 &lt;div&gt; 元素中的所有 class=”head” 的元素 $(“.good”).filter(“.list”).filter(“.Card”)：依次过滤参考：Jquery选择器 Css 选择奇偶 1234tr:nth-child(2n) 偶tr:nth-child(2n+1) 奇tr:nth-child(even) 偶tr:nth-child(odd) 奇]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>HTML</tag>
        <tag>CSS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS监听浏览器地址栏URL变化]]></title>
    <url>%2F2019%2F10%2F18%2FJS%E7%9B%91%E5%90%AC%E6%B5%8F%E8%A7%88%E5%99%A8%E5%9C%B0%E5%9D%80%E6%A0%8FURL%E5%8F%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[JS实现JS监听浏览器地址栏URL变化的代码 假设业务场景：视屏or图片只在首页展示。URL格式：http://localhost:4000/page/2/ HTML中实现1234567891011&lt;body onhashchange="myFunction()"&gt;&lt;/body&gt;&lt;script&gt;function myFunction() &#123; var href = window.location.href; var idx = href.indexOf('page/'); if(href.substring(idx+5,idx+6)&gt;1)&#123; //设置视频or图片不可见 &#125;&#125;&lt;/script&gt; JS实现 方式一 1234567if( ('onhashchange' in window) &amp;&amp; ((typeof document.documentMode==='undefined') || document.documentMode==8)) &#123; var href = window.location.href; var idx = href.indexOf('page/'); if(href.substring(idx+5,idx+6)&gt;1)&#123; //设置视频or图片不可见 &#125;&#125; 方式二 123456789101112&lt;script&gt;document.getElementsById("bd").onhashchange = function() &#123; myFunction()&#125;;function myFunction() &#123; var href = window.location.href; var idx = href.indexOf('page/'); if(href.substring(idx+5,idx+6)&gt;1)&#123; //设置视频or图片不可见 &#125;&#125;&lt;/script&gt; 方式三 12345678910&lt;script&gt;window.addEventListener("hashchange", myFunction);function myFunction() &#123; var href = window.location.href; var idx = href.indexOf('page/'); if(href.substring(idx+5,idx+6)&gt;1)&#123; //设置视频or图片不可见 &#125;&#125;&lt;/script&gt;]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS实现媒体查询功能]]></title>
    <url>%2F2019%2F10%2F18%2FJS%E5%AE%9E%E7%8E%B0%E5%AA%92%E4%BD%93%E6%9F%A5%E8%AF%A2%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[JS实现类似CSS3媒体查询功能 记录一下用JS实现媒体查询功能的代码。 用JS实现遇到过window.οnresize方法失效的情况，用Jquery方式实现没问题。 12345678910// 窗体变化监听window.οnresize=function()&#123; if(document.body.clientWidth &lt;= 768)&#123; &#125;else if(document.body.clientWidth &lt;= 900)&#123; &#125;else if(document.body.clientWidth &lt;= 1200)&#123; &#125; ... if(document.body.clientHeight &lt;= 768)&#123; &#125;&#125; 用Jquery实现12345678910// 窗体变化监听$(window).resize(function()&#123; if($(window).width() &lt;= 768)&#123; &#125;else if($(window).width() &lt;= 900)&#123; &#125;else if($(window).width() &lt;= 1200)&#123; &#125; ... if($(window).height() &lt;= 768)&#123; &#125;&#125;) 其他说明如果结合媒体查询时会设置元素是否可见，通过css设置元素display=none，可防止页面闪烁问题。]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>JavaScript</tag>
        <tag>媒体查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot应用部署到Glassfish上(dev环境)]]></title>
    <url>%2F2019%2F10%2F17%2FSpringBoot%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%E5%88%B0Glassfish%E4%B8%8A-dev%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[SpringBoot应用部署到Glassfish上的配置教程 一. 需要将springboot打成一个war包，默认打成jar包。 二. 做如下修改 1.将pom.xml文件首部的jar改成war 123456&lt;groupId&gt;com.ysl&lt;/groupId&gt;&lt;artifactId&gt;springboot&lt;/artifactId&gt;&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;&lt;name&gt;springboot&lt;/name&gt;&lt;!--&lt;packaging&gt;jar&lt;/packaging&gt; --&gt;&lt;packaging&gt;war&lt;/packaging&gt; 2.pom.xml中设置war包名称 123&lt;build&gt; &lt;finalName&gt;boot&lt;/finalName&gt;&lt;/build&gt; 3.pom.xml中spring-boot-starter-web依赖中移除tomcat模块 12345678910 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 4.pom.xml中添加如下依赖。否则打包时会报javax.servlet.ServletException 12345 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; spring-boot-starter-tomcat 是原来被传递过来的依赖，默认会打到包里，所以我们再次引入此依赖，并指定依赖范围为provided，这样tomcat 相关的jar就不会打包到war 里了。 5.修改启动类如下 12345678910 @SpringBootApplicationpublic class SpringbootApplication extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(SpringbootApplication.class); &#125; public static void main(String[] args) &#123; SpringApplication.run(SpringbootApplication.class, args); &#125;&#125; 三. glassfish中启动 四. 访问服务，正常启动但是这里的context-path和port都不是springboot中指定的，这里的路径端口都是glassfish配置的。即application.properties中如下配置是没有生效的。 12server.port=7989server.servlet.context-path=/me 如果需要使用springboot 中的context-path和port，可将springboot做成wrapper 服务，参考教程：SpringBoot-jar-作为Wrapper服务启动-WindowsSpringBoot-jar包作为Wrapper服务启动-Linux 五. 查看当前domain下的日志，发现日志中报错： 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.batch&lt;/groupId&gt; &lt;artifactId&gt;spring-batch-core&lt;/artifactId&gt; &lt;version&gt;4.1.1.RELEASE&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>GlassFish</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Hexo-Ocean主题博客搭建]]></title>
    <url>%2F2019%2F09%2F30%2F%E5%9F%BA%E4%BA%8EHexo-Ocean%E4%B8%BB%E9%A2%98%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Hexo-Ocean主题博客搭建改动记录 本文主要是Hexo Ocean主题博客搭建过程的记录博客地址：松林羊 hexo 的基本命令hexo init [folder] ：创建一个存放网站源码的文件夹。hexo new [layout] &lt;title&gt;：新建一篇文章。layout：默认post，可以是page、draft。title：文章标题，标题包含空格的话，请使用引号括起来。hexo clean：清除缓存文件db.json和已生成的静态文件public,尤其是更换主题后hexo g：生成静态html文件hexo s：启动服务器hexo d：部署网站传送门：官方文档 下载主题源码在hexo init [folder] 的folder目录下执行： 1git clone https://github.com/zhwangart/hexo-theme-ocean.git themes/ocean 参照主题文档进行配置Ocean 中文文档关于 Ocean 使用中的问题 文章自动添加Read More将\themes\ocean\layout\_partial\article.ejs文中的&lt;% if (post.excerpt &amp;&amp; index){ %&gt;的else部分替换为如下内容： 123456789&lt;% var br = post.content.indexOf('&lt;br&gt;') %&gt;&lt;% if(br &lt; 0 || !index) &#123; %&gt; &lt;%- post.more %&gt;&lt;% &#125; else &#123; %&gt; &lt;%- post.content.substring(0, br) %&gt;&lt;br/&gt; &lt;% if (theme.excerpt_link) &#123; %&gt; &lt;a class="article-more-link" href="&lt;%- url_for(post.path) %&gt;"&gt;&lt;%= theme.excerpt_link %&gt;&lt;/a&gt; &lt;% &#125; %&gt;&lt;% &#125; %&gt; 搜索功能不起作用? or 除开首页不能正常搜索 本地检索需要安装插件 1npm install hexo-generator-searchdb --save 另外一个问题：插件搜索函数返回的url 地址有问题 ，作者说是因为 “中文字符被URL encode了 ” 。后来我找到一个解决办法：将 [folder]/node_modules/hexo-generator-searchdb/templates/xml.ejs 文件中的： 1&lt;url&gt;&lt;%- encodeURIComponent(config.root + post.path) %&gt;&lt;/url&gt; 修改为 1&lt;url&gt;&lt;%- encodeURI(config.root + post.path) %&gt;&lt;/url&gt; 修改导航栏修改 themes/ocean/source/css/_partial/navbar.styl 文件 文章添加封面图片12345678---title: Post namedate: 2019-07-24 22:01:03photos: [ [&quot;/images/相机.jpg&quot;], // themes/ocean/source/images目录下 [&quot;https://tuchong.pstatp.com/2716763/f/531173888.jpg&quot;] ]--- 在首页只会显示第一张，详情页会按顺序显示这两张 为文章添加Gitalk评论参考：https://zhwangart.github.io/2018/12/06/Gitalk/ 在右上角或者左上角实现fork me on github选样式：GitHub Corners-1 &emsp;GitHub Corners-2然后粘贴刚才复制的代码到themes/ocean/layout/index.ejs文件中，放在&lt;div id=”landingpage”&gt;&lt;/div&gt;的第一行，并把href改为你的github地址对样式做出修改：&lt;div id=”landingpage”&gt; 修改为 &lt;div id=”landingpage” style=”position:relative;”&gt;复制的&lt;a&gt;标签添加如下样式：style=”position:absolute;left:0;top:100;z-index=1000;” 实现点击出现特效 点击桃心下载：桃心Js然后将里面的代码copy一下，新建love.js文件并且将代码复制进去，然后保存。在themes/ocean/_config.yml最后一行写入： 1lovejs: true 将love.js文件放到路径/themes/ocean/source/js/src里面，然后打开\themes\ocean\layout\_partial\ after-footer.ejs文件, 在最后一行写入： 123&lt;% if (theme.lovejs)&#123; %&gt; &lt;%- js('/js/love.js') %&gt;&lt;% &#125; %&gt; 爆炸效果下载：fireworks.js&emsp;anime.min.js步骤与上面类似保存js到/themes/ocean/source/js/src，最后在\themes\ocean\layout\_partial\ after-footer.ejs文件内容未写入： 12345&lt;% if (theme.fireworks)&#123; %&gt; &lt;canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" &gt;&lt;/canvas&gt; &lt;%- js('/js/anime.min.js') %&gt; &lt;%- js('/js/fireworks.js') %&gt;&lt;% &#125; %&gt; 头像旋转效果打开\themes\ocean\layout\_partial\sidebar.ejs，给&lt;div class=logo&gt;下的img加上一个id。然后参考：css + js实现图片不停旋转 鼠标悬停停止旋转然后将里面的js代码copy到一个my.js文件中（注意替换img的id），保存到/themes/ocean/source/js/src目录。然后在\themes\ocean\layout\_partial\after-footer.ejs中最后一行写入： 1&lt;%- js('/js/my.js') %&gt; 网站底部加上访问量\themes\ocean\layout\_partial\after-footer.ejs 中默认是导入了统计脚本的(busuanzi-2.3.pure.min.js ) 修改访问量统计的样式：将\themes\ocean\layout\_partial\post\busuanzi.ejs修改为： 123456789&lt;div class="powered-by"&gt; &lt;% if (is_home()) &#123; %&gt; &lt;span id="busuanzi_container_site_pv"&gt;访问量:&lt;span id="busuanzi_value_site_pv"&gt;&lt;/span&gt;&lt;/span&gt; &lt;%&#125; %&gt; &amp;emsp;&lt;i class="fa fa-user-md"&gt;&lt;/i&gt;&lt;span id="busuanzi_container_site_uv"&gt;访客数:&lt;span id="busuanzi_value_site_uv"&gt;&lt;/span&gt;&lt;/span&gt;&amp;emsp; &lt;% if (is_post()) &#123; %&gt; &lt;i class="fe fe-bookmark"&gt;&lt;/i&gt;文章访问量:&lt;span id="busuanzi_value_page_pv"&gt;&lt;/span&gt; &lt;%&#125; %&gt;&lt;/div&gt; 添加网站字数，阅读时间统计在根目录下运行： 1npm install hexo-wordcount –save 在\themes\ocean\_config.yml主题配置文件中加入： 123456post_wordcount: item_text: true wordcount: true min2read: true totalcount: true separated_meta: true 在\themes\ocean\layout\_partial\footer.ejs文件中，在&lt;ul class=”list-inline”&gt;标签后加入： 123&lt;ul class="list-inline"&gt; &lt;li&gt;全站共&lt;span class="post-count"&gt;&lt;%= totalcount(site) %&gt;&lt;/span&gt;字&lt;/li&gt;&lt;/ul&gt; 在\themes\ocean\layout\_partial\article.ejs文件中，在标签中加入： 12&amp;emsp;&lt;i class="fe fe-bar-chart"&gt;&lt;/i&gt; &lt;span class="post-count"&gt;&lt;%- wordcount(post.content) %&gt;&lt;/span&gt;字&amp;emsp;&lt;i class="fe fe-clock"&gt;&lt;/i&gt; &lt;span class="post-count"&gt;&lt;%- min2read(post.content) %&gt;&lt;/span&gt;分钟 Ocean主题的图标图标在\themes\ocean\source\css\_feathericon.styl中查找官网：feathericons 为博客加上萌萌的宠物hexo-helper-live2d在网站根目录下执行 12npm install -save hexo-helper-live2dnpm install live2d-widget-model-haruto(自己选择的萌宠模型) 在hexo的配置文件中添加： 123456789101112131415161718192021# 萌宠 live2d: enable: true scriptFrom: local model: use: live2d-widget-model-haruto scale: 1 hHeadPos: 0.5 vHeadPos: 0.618 display: superSample: 2 width: 150 height: 300 position: left hOffset: 0 vOffset: -20 mobile: show: true react: opacityDefault: 0.5 opacityOnHover: 0.2 重启服务： 1hexo clean &amp;&amp; hexo g &amp;&amp; hexo s 添加网站运行时间1234567891011121314151617&lt;span id="timeDate"&gt;载入天数...&lt;/span&gt;&lt;span id="times"&gt;载入时分秒...&lt;/span&gt;&lt;script&gt; var now = new Date(); function createtime() &#123; var grt= new Date("04/26/2019 15:49:00");//此处修改你的建站时间或者网站上线时间 now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 )&#123;hnum = "0" + hnum;&#125; minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 )&#123;mnum = "0" + mnum;&#125; seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 )&#123;snum = "0" + snum;&#125; document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; &#125; setInterval("createtime()",250);&lt;/script&gt; 将以上代码保存到\themes\ocean\layout\_partial\post\runtime.ejs 在\themes\ocean\layout\_partial\footer.ejs文件中，第一个&lt;ul class=”list-inline”&gt;&lt;/ul&gt;标签后加入： 123&lt;div class="float-right"&gt; &lt;%- partial('post/runtime') %&gt;&lt;/div&gt; 添加DaoVoice 在线联系首先在https://account.daocloud.io/signin &emsp;注册账号然后点击下方链接http://dashboard.daovoice.io/get-started?invite_code=0f81ff2f之后会得到一个app_id 在主题配置文件写入： 123# Online contact daovoice: truedaovoice_app_id: 这里填你的刚才获得的 app_id 在\themes\ocean\layout\_partial\footer.ejs中，&lt;/head&gt;前写入： 123456789&lt;% if (theme.daovoice)&#123; %&gt; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i["DaoVoiceObject"]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)&#125;)(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/0f81ff2f.js","daovoice") daovoice('init', &#123; app_id: "&lt;%- theme.daovoice_app_id %&gt;" &#125;); daovoice('update'); &lt;/script&gt;&lt;% &#125; %&gt; 取消文章下面的分享连接，添加本文结束去掉\themes\ocean\layout\_partial\article.ejs中的以下代码： 1&lt;a data-url="&lt;%- post.permalink %&gt;" data-id="&lt;%= post._id %&gt;" class="article-share-link"&gt;&lt;%- theme.share_text %&gt;&lt;/a&gt; 同时还可以添加本文结束 感谢阅读等说明:如在&lt;%- partial(‘post/tag’) %&gt;下面一行添加： 123&lt;% if (!index &amp;&amp; is_post()) &#123; %&gt; &lt;div style="text-align:center;color: #ccc;font-size:14px;"&gt;------------- 本文结束&amp;nbsp;&lt;i class="fe fe-smile"&gt;&lt;/i&gt;&amp;nbsp;感谢您的阅读 -------------&lt;/div&gt;&lt;% &#125; %&gt; 博文压缩在根目录下执行： 12npm install gulp -gnpm install gulp-minify-css gulp-uglify gulp-htmlmin gulp-htmlclean gulp –save 在根目录下新建gulpfile.js，写入一下内容： 123456789101112131415161718192021222324252627282930313233343536373839404142var gulp = require('gulp');//Plugins模块获取var minifycss = require('gulp-minify-css');var uglify = require('gulp-uglify');var htmlmin = require('gulp-htmlmin');var htmlclean = require('gulp-htmlclean');//压缩cssgulp.task('minify-css',function() &#123; return gulp.src('./public/**/*.css').pipe(minifycss()).pipe(gulp.dest('./public'));&#125;);//压缩htmlgulp.task('minify-html',function() &#123; return gulp.src('./public/**/*.html').pipe(htmlclean()).pipe(htmlmin(&#123; removeComments: true, minifyJS: true, minifyCSS: true, minifyURLs: true, &#125;)) .pipe(gulp.dest('./public'))&#125;);//压缩js 不压缩min.jsgulp.task('minify-js',function() &#123; return gulp.src(['./public/**/*.js', '!./public/**/*.min.js']).pipe(uglify()).pipe(gulp.dest('./public'));&#125;);//4.0以前的写法 //gulp.task('default', [// 'minify-html', 'minify-css', 'minify-js'//]);//4.0以后的写法// 执行 gulp 命令时执行的任务//gulp.task('default', gulp.parallel('minify-html', 'minify-css', 'minify-js',//function() &#123; // Do something after a, b, and c are finished.// console.log('success')//&#125;));gulp.task('default',gulp.series(gulp.parallel('minify-html','minify-css','minify-js'))); 生成博文时执行 1hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo d 就会根据 gulpfile.js 中的配置，对 public 目录中的静态资源文件进行压缩。 修改文章封面图片的引入方式，同时支持相对地址与http的图片将\themes\ocean\layout\_partial\post\gallery.ejs中的&lt;% if (index){ %&gt;的上一行写入： 1&lt;% var idx = url_for(photo).indexOf('http') %&gt; 将文中两处的： 1&lt;img src="&lt;%- url_for(photo) %&gt;" itemprop="image"&gt; 修改为： 12345&lt;% if(idx &lt; 0) &#123; %&gt; &lt;img src="&lt;%- url_for(post.path)+url_for(photo).substring(1) %&gt;" itemprop="image"&gt;&lt;% &#125; else &#123; %&gt; &lt;img src="&lt;%- url_for(photo) %&gt;" itemprop="image"&gt;&lt;% &#125; %&gt; 设置封面图片不在内容详情页展示将\themes\ocean\layout\_partial\article.ejs中的 1&lt;%- partial('post/gallery') %&gt; 修改为： 123&lt;% if (index)&#123; %&gt; &lt;%- partial('post/gallery') %&gt;&lt;% &#125; %&gt; 去掉rss订阅根目录下执行： 1npm uninstall hexo-generator-feed –save 将\themes\ocean\_config.yml文件中修改为：rss: false 导航栏图表改为在文字左侧修改\themes\ocean\source\css\_partial\navbar.styl 1234567&amp;.nav-main .nav-item-link &amp;::before, i.fe display block line-height 1 &amp;::before font-family 'feathericon' 修改为： 12345678&amp;.nav-main .nav-item-link &amp;::before, i.fe // display block line-height 1 margin-right: 10px; &amp;::before font-family 'feathericon' 添加readme.md不被渲染在Hexo目录下的source根目录下添加一个, README.md。修改Hexo目录下的_config.yml。将skip_render参数的值设置上。skip_render: README.md保存退出即可。 首页视屏换成图片将\themes\ocean\layout\_partial\ocean.ejs中 1234567&lt;video playsinline="" autoplay="" loop="" muted="" data-autoplay="" poster="&lt;%- theme.ocean.path %&gt;ocean.png" x5-video-player-type="h5"&gt; &lt;source src="&lt;%- theme.ocean.path %&gt;ocean.mp4" type="video/mp4"&gt; &lt;source src="&lt;%- theme.ocean.path %&gt;ocean.ogv" type="video/ogg"&gt; &lt;source src="&lt;%- theme.ocean.path %&gt;ocean.webm" type="video/webm"&gt; &lt;p&gt;Your user agent does not support the HTML5 Video element.&lt;/p&gt;&lt;/video&gt; 修改为： 1&lt;img src="&lt;%- theme.ocean.path %&gt;ocean.png"&gt; 去掉首页视频或图片去掉themes/ocean/layout/index.ejs中的 1&lt;%- partial('_partial/ocean') %&gt; is_home()、is_post() 函数判断不正确将\themes\ocean\layout\layout.ejs中 1&lt;%- partial('_partial/footer', null, &#123;cache: !config.relative_link&#125;) %&gt; 修改为 1&lt;%- partial('_partial/footer', null, &#123;cache: config.relative_link&#125;) %&gt; 就是将partial中的cache设置为false。参考：is_home()、is_post() 函数判断不正确 视频 or 图片只在pc端显示，不在移动端显示修改themes\ocean\source\css_partial\layou.styl文件 12345// Media Query@media (min-width: 768px) .jumbotron margin-bottom 6rem display block //此处为新增。设备宽度大于等于768像素时显示视频or图片 在文件末尾新增 123@media (max-width: 768px) .jumbotron display none // 设备宽度小于等于768像素时不显示视频or图片 此时会发现移动端左上角有点空旷，则进行如下修改修改themes\ocean\layout_partial\archive.ejs文件 12345var title = '';var mobile_title = "松林羊 Blog"; // 此处为新增...&lt;h1 class="page-type-title pc"&gt;&lt;%- title %&gt;&lt;/h1&gt; // 新增类名为pc的样式&lt;h1 class="page-type-title mobile"&gt;&lt;%- mobile_title %&gt;&lt;/h1&gt; // 此处为新增 修改themes\ocean\source\css_partial\archive.styl文件 123456789101112131415.page-type-title margin 0 padding 3rem 0 // 以下内容为新增 &amp;.pc @media screen and (max-width: 768px) display none @media screen and (min-width: 768px) display block &amp;.mobile @media screen and (max-width: 768px) display block font-family 'STXingkai' @media screen and (min-width: 768px) display none 配置fancybox展示文章图集在themes\ocean\layout\_partial\after-footer.ejs文件末尾加入如下代码 12345678&lt;% if (is_post())&#123; %&gt;&lt;script&gt;// 使用fancybox来显示post图片集(#andus-head-img为头像id)$('img:not(#andus-head-img)').each(function() &#123; $(this).wrap('&lt;a class="fancybox" data-fancybox="gallery" href="' + $(this).prop("src") + '"&gt;&lt;/a&gt;'); &#125;)&lt;/script&gt;&lt;% &#125; %&gt; 更换评论系统，由gitalk跟换为valine, 并增加邮件通知功能依次参考Valine–一款极简的评论系统ValineValine-Admin在themes/ocean/_config.yml文件中新增以下内容，同时确保gitalk.enable为false 1234567891011# Valine 不能与gitalk同时开启valine: enable: true app_id: # 这里填写得到的APP ID app_key: # 这里填写得到的APP KEY placeholder: 记得留下你的昵称和邮箱...可以快速收到回复ヾﾉ≧∀≦)o # [v1.0.7 new]留言框占位提示文字 notify: true # 评论回复邮件提醒 。 第三方支持：https://github.com/zhaojun1998/Valine-Admin verify: false # 验证码 。 开启邮件提醒会默认开启验证码选项 avatar: monsterid # Gravatar头像。可选项：[identicon monsterid wavatar retro robohash mp &apos;&apos;] ，见 https://valine.js.org/avatar.html recordIP: true # 是否记录评论者ip visitor: false # 阅读量统计 https://valine.js.org/visitor.html 拷贝themes\ocean\layout\_partial\post\gitalk.ejs重命名为valine.ejs全部内容修改为 12345678910111213141516171819202122232425262728293031&lt;% if (theme.valine.enable) &#123; %&gt; &lt;div class="comment_headling" style="margin-top:5rem;"&gt; &lt;font size="5"&gt;&lt;i class="fe fe-comments"&gt;&lt;/i&gt; 评论&lt;/font&gt; &lt;/div&gt; &lt;div class="comment"&gt;&lt;/div&gt; &lt;%- js('https://cdn1.lncld.net/static/js/3.0.4/av-min.js') %&gt; &lt;%- js('js/Valine.min.js') %&gt; &lt;script type="text/javascript"&gt; // https://cdnjs.cloudflare.com/ajax/libs/valine/1.3.10/Valine.min.js GUEST_INFO = ["nick", "mail", "link"], guest_info = "nick,mail,link".split(",").filter(function(i) &#123; return - 1 &lt; GUEST_INFO.indexOf(i) &#125;); guest_info = 0 == guest_info.length ? GUEST_INFO: guest_info, new Valine(&#123; // AV 对象来自上面引入av-min.js av: AV, el: '.comment', app_id: '&lt;%- theme.valine.app_id %&gt;', app_key: '&lt;%- theme.valine.app_key %&gt;', placeholder: '&lt;%- theme.valine.placeholder %&gt;', meta: guest_info, notify: &lt;%- theme.valine.notify %&gt;, verify: &lt;%- theme.valine.verify %&gt;, avatar: '&lt;%- theme.valine.avatar %&gt;', recordIP: &lt;%- theme.valine.recordIP %&gt;, visitor: &lt;%- theme.valine.visitor %&gt;, lang: 'zh-cn' &#125;); &lt;/script&gt;&lt;% &#125; %&gt; 修改themes\ocean\layout\_partial\article.ejs文件 12345&lt;% if (is_post()) &#123; %&gt; &lt;%- partial('post/gitalk') %&gt; &lt;%# 下面为新增内容 %&gt; &lt;%- partial('post/valine') %&gt;&lt;%&#125; %&gt; 关闭评论时的验证码，将Valine.min.js保存到themes\ocean\source\js目录下，将其中 12345678u.attr(Q, "style", "display:block;"), t &amp;&amp; t.type) &#123; var o = u.find(Q, ".vsure"); u.on("click", o, function(n) &#123; e.alert.hide(), t.cb &amp;&amp; t.cb() &#125;)&#125; 修改为 1234u.attr(Q, "style", "display:none;"), t &amp;&amp; t.type) &#123; e.alert.hide(), t.cb &amp;&amp; t.cb()&#125; 记录(自用)12&lt;%- page.content.substring(0,tips) %&gt; 显示html的样式结果&lt;%= page.content.substring(0,tips) %&gt;显示html源代码]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>ocean</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile使用-集成JAVA Python Redis Tomcat Nginx镜像，Based on Ubuntu18.04]]></title>
    <url>%2F2019%2F08%2F22%2FDockerfile%E4%BD%BF%E7%94%A8-%E9%9B%86%E6%88%90JAVA-Python-Redis-Tomcat-Nginx%E9%95%9C%E5%83%8F%EF%BC%8CBased-on-Ubuntu18-04%2F</url>
    <content type="text"><![CDATA[DockerFile使用案例，搭建常用JAVA环境镜像。 镜像信息 Based on ubuntu18.04 Environment jdk8 &gt;&gt; apt install -y openjdk-8-jdk tomcat7 &gt;&gt; install dir：/opt/tomcat-7.0.96 python3.7 &gt;&gt; install dir：/opt/Python-3.7.4 nginx1.14 &gt;&gt; apt-get install -y nginx redis5.0.5 &gt;&gt; install dir：/opt/redis-5.0.5 general-tools vim &gt;&gt; apt-get install -y vim net-tools &gt;&gt; apt-get install -y net-tools curl &gt;&gt; apt install -y curl cron &gt;&gt; apt-get install -y cron inetutils-ping &gt;&gt; apt-get install -y inetutils-ping wget &gt;&gt; apt-get install -y wget Dockerfile123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131# BASE IMAGEFROM ubuntu:latest# tzdata时区ENV DEBIAN_FRONTEND=noninteractive TIME_ZONE=Asia/Shanghai# 镜像标签, 起说明作用LABEL name=&quot;Prod Ubuntu&quot; \ build-data=&quot;2019-08-21&quot; \ maintainer=&quot;614580167@qq.com&quot; \ version=&quot;1.0&quot; # 添加阿里源RUN /bin/bash -c awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb http://mirrors.aliyun.com/ubuntu/ &#123;&#125; main multiverse restricted universe \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb http://mirrors.aliyun.com/ubuntu/ &#123;&#125; main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-backports main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-proposed main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-security main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-updates main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb-src http://mirrors.aliyun.com/ubuntu/ &#123;&#125; main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb-src http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-backports main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb-src http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-proposed main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb-src http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-security main multiverse restricted universe &gt;&gt; /etc/apt/sources.list \ &amp;&amp; awk &apos;NR==3&#123;print substr($1,18)&#125;&apos; /etc/lsb-release | xargs -I &#123;&#125; echo deb-src http://mirrors.aliyun.com/ubuntu/ &#123;&#125;-updates main multiverse restricted universe &gt;&gt; /etc/apt/sources.list# 更新源RUN apt-get -y update &amp;&amp; apt-get -y upgrade &amp;&amp; apt-get -y dist-upgrade# install general-toolsRUN apt-get install -y vim net-tools curl cron inetutils-ping wget lsof# vi 编辑器显示行号RUN /bin/bash -c echo &apos;set nu&apos;&gt;&gt;/etc/vim/vimrc# 安装python3.7# 安装相关依赖RUN apt-get install -y tzdata \ &amp;&amp; ln -snf /usr/share/zoneinfo/$TIME_ZONE /etc/localtime \ &amp;&amp; echo $TIME_ZONE &gt; /etc/timezone \ &amp;&amp; dpkg-reconfigure --frontend $DEBIAN_FRONTEND tzdata \ &amp;&amp; apt-get install -y gcc make zlib* \ &amp;&amp; apt-get install -y build-essential python-dev python-setuptools python-pip python-smbus \ &amp;&amp; apt-get install -y libncursesw5-dev libgdbm-dev libc6-dev zlib1g-dev tk-dev libsqlite3-dev \ &amp;&amp; apt-get install -y libssl-dev openssl libffi-dev WORKDIR /opt/RUN wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz &amp;&amp; tar -zxvf Python-3.7.4.tgzRUN cd /opt/Python-3.7.4 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install# 默认使用python3.7RUN rm /usr/bin/python RUN ln -s /usr/local/bin/python3.7 /usr/bin/pythonWORKDIR /opt/RUN rm Python-3.7.4.tgz# 安装Tomcat7RUN cd /opt/ \ &amp;&amp; wget http://archive.apache.org/dist/tomcat/tomcat-7/v7.0.96/bin/apache-tomcat-7.0.96.tar.gz \ &amp;&amp; tar -zxvf apache-tomcat-7.0.96.tar.gz \ &amp;&amp; mv apache-tomcat-7.0.96 tomcat-7.0.96 \ &amp;&amp; rm apache-tomcat-7.0.96.tar.gz # 安装Tomcat7（方式二）# 编译命令：docker build -f /opt/docker/Dockerfile2 -t ysl_env_tomcat:1.0 /opt/docker/ # 其中 命令最后的/opt/docker 指定的是 build 上下文，ADD指令中apache-tomcat-7.0.96.tar.gz 的位置是相对上下文的# ADD apache-tomcat-7.0.96.tar.gz /opt/# RUN cd /opt/ &amp;&amp; mv apache-tomcat-7.0.96 tomcat-7.0.96# 配置环境变量ENV CATALINE_HOME /opt/tomcat-7.0.96ENV CATALINE_BASE /opt/tomcat-7.0.96ENV PATH $PATH:$CATALINE_HOME:$CATALINE_HOME/lib:$CATALINE_HOME/bin# 安装redisRUN cd /opt/ \ &amp;&amp; wget http://download.redis.io/releases/redis-5.0.5.tar.gz \ &amp;&amp; tar -zxvf redis-5.0.5.tar.gz \ &amp;&amp; cd /opt/redis-5.0.5 \ &amp;&amp; make \ &amp;&amp; make install \ &amp;&amp; cp redis.conf redis.conf.bak \ &amp;&amp; sed -i &apos;s/bind 127.0.0.1/#bind 127.0.0.1/g&apos; /opt/redis-5.0.5/redis.conf \ &amp;&amp; sed -i &apos;s/protected-mode yes/protected-mode no/g&apos; /opt/redis-5.0.5/redis.conf \ &amp;&amp; sed -i &apos;s/daemonize no/daemonize yes/g&apos; /opt/redis-5.0.5/redis.conf \ &amp;&amp; cd /opt/ \ &amp;&amp; rm redis-5.0.5.tar.gz# 安装nginxRUN apt-get install -y nginx# 安装java8RUN apt install -y openjdk-8-jdkWORKDIR /# 对外暴露8080端口# EXPOSE 8080# 导致新的镜像不能启动# release info# ENTRYPOINT echo &quot;\n\n\n\t\t\t\t \# Welcome to ysl_prod_ubuntu:1.0 （based on Ubuntu 18.04） \n\# Dockerfile: https://blog.csdn.net/github_33809414 \n\# Build-data: 2019-08-21 \n\# Support: 614580167@qq.com&quot;# 导致新的镜像不能启动# ENTRYPOINT [&quot;/bin/echo&quot;, &quot;\n\n\n\t\t\t\t&quot;] # CMD [&quot;Welcome to ysl_ubuntu:1.0 （based on Ubuntu 18.04） \n\# Dockerfile: https://blog.csdn.net/github_33809414 \n\# Build-data: 2019-08-21 \n\# Support: 614580167@qq.com&quot;] #CMD echo &quot;该指令不会被执行,只会执行最后一条&quot;# Default commandCMD [&quot;/bin/bash&quot;] 将Dockerfile编译为镜像docker build -f Dockerfile的路径 -t 镜像名:tag 遇到的问题1. Dockerfile交互问题 如果是apt安装场合，使用-y 参数解决 1RUN apt-get install -y xxx 安装脚本场合比如我们用sh Anaconda3-2019.07-Linux-x86_64.sh来安装anaconda的时候，”烦人”的anaconda会问四个问题，我的回答顺序分别是Enter，yes，Enter，yes。于是你可以这样写： 1RUN sh -c &apos;/bin/echo -e &quot;\nyes\n\nyes&quot; | sh Anaconda3-2019.07-Linux-x86_64.sh&apos; Dockerfile中安装tzdata交互场合 123456789101112# 设置环境变量ENV DEBIAN_FRONTEND noninteractive # 非交互模式ENV TIME_ZONE Asia/Shanghai # 目标时区# 安装tzdata软件包（默认：Etc/UTC）RUN apt-get install -y tzdata # 建立到期望的时区的链接(ubuntu16.04以上需要)，并且修改/etc/timezoneRUN ln -snf /usr/share/zoneinfo/$TIME_ZONE /etc/localtime &amp;&amp; echo $TIME_ZONE &gt; /etc/timezone# 重新配置tzdata软件包，使得时区设置生效RUN dpkg-reconfigure --frontend $DEBIAN_FRONTEND tzdata 2. 对【每条指令都是在一个新的容器中执行】的理解 以下指令报错，/bin/sh: 1: ./configure: not found 1234WORKDIR /opt/Python-3.7.4RUN ./configureRUN makeRUN make install 是因为，在执行RUN ./configure指令时，已经在一个新的容器中了，而在新的容器中没有进入到/opt/Python-3.7.4目录，自然找不到configure 解决方法将指令写成一条指令即可RUN cd /opt/Python-3.7.4 &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install 3. Dockerfile中加入ENTRYPOINT指令，镜像能build成功，但是不能run起来 原因还没有找到，希望大神看到了告知]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile笔记]]></title>
    <url>%2F2019%2F08%2F22%2FDockerfile%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[DockerFile学习笔记 1. 是什么 Dockerfile是一个包含用于组合镜像指令的文本文档。Docker通过读取Dockerfile中的指令自动生成镜像。 2. 基础知识 每条指令由保留字和参数组成 指令中保留字都必须为大写，且后面要跟随至少一个参数 指令按照从上到下，顺序执行 每条指令都会创建一个新的镜像层，并对镜像进行提交 # 表示注释 3. 怎么用 docker build命令用于从Dockerfile构建映像。docker build -f /opt/docker/Dockerfile -t 镜像名称:tag /opt/docker/ -f：标志指向文件系统中任何位置的Dockerfile，-t：指定新镜像名称和tag，最后的/opt/docker指定的是 build 上下文。如在命令ADD apache-tomcat-7.0.96.tar.gz /opt/中，apache-tomcat-7.0.96.tar.gz 的位置是相对上下文的。即，虽然ADD指令中直接写的文件名，实际上该文件的真实路径为：/opt/docker/apache-tomcat-7.0.96.tar.gz 4. dockerfile build大致流程 docker从基础镜像运行一个容器 执行一条指令并对容器做出修改 执行类似docker commit的操作提交一个新的镜像层 docker 在基于刚提交的镜像运行一个新的容器 执行dockerfile中的下一条指令，直到所有指令都被执行 5. 保留字 保留字 作用 说明 用法 FROM 指定基础镜像 必须为第一个指令 FROM &lt;image&gt;[:&lt;tag&gt;] [AS &lt;name&gt;] MAINTAINER 指定镜像维护者信息 已弃用 MAINTAINER &lt;name&gt; RUN 构建镜像时执行的命令 RUN &lt;command&gt; RUN [“executable”, “param1”, “param2”] CMD 指定容器启动时要运行的命令 如果CMD用于为ENTRYPOINT 指令提供默认参数， 则应使用JSON数组格式指定CMD和ENTRYPOINT指令。 多个CMD指令时，只有最后一条生效 CMD [“executable”,”param1”,”param2”] （执行形式，这是首选形式） CMD [“param1”,”param2”] （作为ENTRYPOINT的默认参数） CMD command param1 param2 (shell form) ENTRYPOINT 指定容器启动时要运行的命令 与CMD区别：不会被docker run 后的参数替换掉 ENTRYPOINT [“executable”, “param1”, “param2”] (执行形式，首选) ENTRYPOINT command param1 param2 (shell form) EXPOSE 镜像启动后对外暴露的端口号 可被docker run -p 80:80/tcp -p 80:80/udp 覆盖 EXPOSE &lt;port&gt; [&lt;port&gt;/&lt;protocol&gt;…] WORKDIR 指定容器的工作目录 如果有多个WORKDIR指令时， 后一条是相对于前一条WORKDIR指令的路径 WORKDIR /path/to/workdir ENV 在镜像构建过程中设置环境变量 $&lt;key&gt; 引用变量 使用docker inspect和查看值， 并使用它们进行更改docker run –env = ENV &lt;key&gt; &lt;value&gt; ENV &lt;key&gt;=&lt;value&gt; (两者有区别) ADD 从源拷贝到容器中 tar类型文件会自动解压(网络压缩资源不会被解压)， 可以访问网络资源，类似wget ADD [–chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;… &lt;dest&gt; ADD [–chown=&lt;user&gt;:&lt;group&gt;] [“&lt;src&gt;”,… “&lt;dest&gt;”] COPY 从源拷贝到容器中 不会自动解压文件，也不能访问网络资源 CPOY [–chown=&lt;user&gt;:&lt;group&gt;] &lt;src&gt;… &lt;dest&gt; ADD [–chown=&lt;user&gt;:&lt;group&gt;] [“&lt;src&gt;”,… “&lt;dest&gt;”] VOLUME 指定宿主机上持久化目录 VOLUME [“/data”, “data2”] ONBUILD 当构建一个被继承的Dockerfile时运行的命令 父镜像在被子继承后，父镜像的ONBUILD触发 ONBUILD [INSTRUCTION] 传送门：官方文档 具体案例：Dockerfile使用-集成JAVA Python Redis Tomcat Nginx镜像，Based on Ubuntu18.04]]></content>
      <categories>
        <category>容器</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA操作CSV]]></title>
    <url>%2F2019%2F08%2F14%2FJAVA%E6%93%8D%E4%BD%9CCSV%2F</url>
    <content type="text"><![CDATA[记录一下java 操作csv的代码。 记录一下java 操作csv的代码。 1. 引入jar包12345&lt;dependency&gt; &lt;groupId&gt;net.sourceforge.javacsv&lt;/groupId&gt; &lt;artifactId&gt;javacsv&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt;&lt;/dependency&gt; 2. 创建csv123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package com.ysl;import com.csvreader.CsvWriter;import java.io.IOException;import java.nio.charset.Charset;import javax.servlet.http.HttpServletResponse;import com.ysl.CsvUtil;/** * 创建csv * @author YSL */public class CsvWriter &#123; public static void main(String[] args) throws IOException &#123; // 方式1. 直接创建本地的csv文件 localFileCsv(); // 方式2. 通过网页返回csv webResponseCsv(res); &#125; public static void localFileCsv() throws IOException &#123; String filePath = "/home/yang/桌面/test-中文.csv"; // 创建CSV写对象 CsvWriter csvWriter = new CsvWriter(filePath,',', Charset.forName("GBK")); // CsvWriter csvWriter = new CsvWriter(filePath); // 会中文乱码 // 表头 String[] headers = &#123;"列1","列2","列3\n"&#125;; csvWriter.writeRecord(headers); csvWriter.writeRecord(new String[]&#123;"value1", "value2", "value3" + "\n"&#125;); csvWriter.writeRecord(new String[]&#123;"value1", "value2", "中文" + "\n"&#125;); csvWriter.writeRecord(new String[]&#123;"value1", "value2", "中文"&#125;); csvWriter.writeRecord(new String[]&#123;"value1", "value2", "中文" + "\n"&#125;); csvWriter.close(); &#125; public static void webResponseCsv(HttpServletResponse response) &#123; // 导出csv 。CsvUtil见下文 CsvUtil.writeCsv("文件名称", response, new CsvUtil.WriteCsv() &#123; @Override public void write(CsvWriter csvWriter) throws IOException &#123; // csv表头 csvWriter.writeRecord(new String[]&#123;"列1","列2","列3",&#125;); csvWriter.write("val1"); csvWriter.write("val2"); csvWriter.write("值3"); csvWriter.endRecord(); &#125; &#125;); &#125; &#125; 3. 读取csv123456789101112131415161718192021222324252627282930import java.io.*;import java.util.ArrayList;import java.util.List;import com.ysl.CsvUtil;/** * 读取csv */public class CsvReader &#123; public static void main(String[] args) throws IOException &#123; // 这里需要注意csv文件需要保存为utf-8格式————可以用记事本打开，然后另存为，选择utf-8格式 InputStream in = getResourcesFileInputStream("test-中文.csv"); // 读取csv List&lt;String[]&gt; records = CsvUtil.readCsv(in); &#125; /** * 加载Resources目录下的文件 * @param fileName 文件名 * @return */ public static InputStream getResourcesFileInputStream(String fileName)&#123; // Resources目录的绝对路径 // String path = Thread.currentThread().getContextClassLoader().getResource("").getPath(); InputStream in = Thread.currentThread().getContextClassLoader().getResourceAsStream("" + fileName); return in; &#125;&#125; 4. CsvUtil工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152package com.ysl;import com.csvreader.CsvWriter;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import javax.servlet.http.HttpServletResponse;import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.io.OutputStream;import java.net.URLEncoder;import java.nio.charset.Charset;/** * Csv工具类 * @author:YSL */public class CsvUtil &#123; private static Logger logger = LoggerFactory.getLogger(CsvUtil.class); private static String DEFAULT_ENCODE = "UTF-8"; private static String GBK_ENCODE = "GBK"; /* * 自定义write csv 接口 */ public interface WriteCsv &#123; void write(CsvWriter csvWriter) throws Exception; &#125; /** * 导出csv * @param fileName 文件名称(不包括.csv后缀) * @param response * @param writeCsv 自定义接口，需实现write方法 * @return 直接将csv返回给前端 * @author YSL */ public static void writeCsv(String fileName, HttpServletResponse response, CsvUtil.WriteCsv writeCsv)&#123; try &#123; // 创建临时文件 File tempfile = File.createTempFile(fileName, ".csv"); // 创建 CsvWriter 对象 CsvWriter csvWriter = new CsvWriter(tempfile.getCanonicalPath(),',', Charset.forName(GBK_ENCODE)); // 接口需要实现的方法 writeCsv.write(csvWriter); // 关闭 CsvWriter 对象 csvWriter.close(); // 获取写入数据的临时文件 File fileLoad=new File(tempfile.getCanonicalPath()); // 返回给前端 CsvUtil.downLoadCsv(fileName+".csv", fileLoad, response); &#125;catch (Exception e)&#123; e.printStackTrace(); logger.error(e.getMessage(), e); &#125; &#125; /** * 导出csv文件 * @param fileName 文件名称，含后缀 * @param fileLoad 要输出的文件 * @param response * @author YSL */ public static void downLoadCsv(String fileName, File fileLoad, HttpServletResponse response) &#123; try &#123; byte[] b=new byte[1024 * 1024]; OutputStream out=response.getOutputStream(); response.reset(); response.setContentType("application/csv"); //re.setContentType("application/x-msdownload;"); response.setHeader("content-disposition", "attachment; filename="+ URLEncoder.encode(fileName,DEFAULT_ENCODE)); Long filelength=fileLoad.length(); response.setHeader("Content_Length",String.valueOf(filelength)); FileInputStream fileInputStream=new FileInputStream(fileLoad); int n; while ((n = fileInputStream.read(b)) != -1) &#123; out.write(b, 0, n); //每次写入out1024字节 &#125; // System.out.println(tempfile.getCanonicalPath()); fileInputStream.close(); out.flush(); out.close(); logger.info("export:"+fileName+" --- success"); &#125; catch (IOException e) &#123; e.printStackTrace(); logger.error(e.getMessage(), e); &#125; &#125; /** * 导出csv文件 * @param fileName 文件名称，含后缀 * @param content 逗号隔开的值，换行符：\r\n * @param response * @author YSL */ public static void downLoadCsv(String fileName, String content, HttpServletResponse response) &#123; try &#123; OutputStream out = response.getOutputStream(); //response.reset(); response.setContentType("application/csv"); response.setHeader("content-disposition", "attachment; filename="+StringUtil.encode(fileName,Constants.DEFAULT_GLOBAL_ENCODE)); response.setHeader("Content_Length",String.valueOf(content.length())); out.write(content.getBytes(GBK_ENCODE)); out.close(); logger.info("export"+fileName+"success"); &#125; catch (IOException e) &#123; e.printStackTrace(); logger.error(e.getMessage(), e); &#125; &#125; /** * 读csv * @param in * @return * @author YSL * @throws IOException */ public static List&lt;String[]&gt; readCsv(InputStream in) throws IOException &#123; List&lt;String[]&gt; records = new ArrayList&lt;String[]&gt;(); String record; // 设定UTF-8字符集，使用带缓冲区的字符输入流BufferedReader读取文件内容 BufferedReader file = new BufferedReader(new InputStreamReader(in, "UTF-8")); // file.readLine(); //跳过表头所在的行 // 遍历数据行并存储在名为records的ArrayList中，每一行records中存储的对象为一个String数组 while ((record = file.readLine()) != null) &#123; String fields[] = record.split(","); records.add(fields); &#125; // 关闭文件 file.close(); in.close(); return records; &#125;&#125;]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>CSV</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[office 2019 专业增强版 在线安装+激活]]></title>
    <url>%2F2019%2F08%2F14%2Foffice-2019-%E4%B8%93%E4%B8%9A%E5%A2%9E%E5%BC%BA%E7%89%88-%E5%9C%A8%E7%BA%BF%E5%AE%89%E8%A3%85-%E6%BF%80%E6%B4%BB%2F</url>
    <content type="text"><![CDATA[最新office2019,在线安装激活教程 注明：软件来自网络 1.在线安装+激活只激活，直接看第二点 链接：https://pan.baidu.com/s/1kGxTRLzjbdz_LRRBgCS3pA提取码：oqmp失效请留言 如图 2.只激活传送门：https://blog.csdn.net/bluewn/article/details/90480847]]></content>
      <categories>
        <category>破解</category>
      </categories>
      <tags>
        <tag>office</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Windows10+Ubuntu18.04+双硬盘(SSD+HDD)安装双系统]]></title>
    <url>%2F2019%2F08%2F13%2FWindows10-Ubuntu18-04-%E5%8F%8C%E7%A1%AC%E7%9B%98-SSD-HDD-%E5%AE%89%E8%A3%85%E5%8F%8C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[Windows10+Ubuntu18.04 双系统安装说明。 操作步骤主要参考以下两篇文章，很不错 windows10+ubuntu 16.04+双硬盘（SSD+HDD）分区（图文） win10+1080Ti+双硬盘（SSD+HDD）下安装Ubuntu16.04双系统 情况说明 采用的Windows中引导Ubuntu的方式。 Ubuntu的启动项：1024M（512M足以），挂载点为/boot，采用逻辑分区（主分区未测试），与挂载点/不在同一个物理硬盘上(同一个未测试)。 Ubuntu中引导Windows请自行尝试。 注意事项 采用的Windows中引导Ubuntu的方式，需要你，先安装Windows，再安装Ubuntu。 Ubuntu的启动项所在物理硬盘与Windows的启动项所在的物理硬盘是必须同一个。 后续工作 EasyBCD添加Ubuntu启动，参考上面的两篇文章。 解决win10和ubuntu16.04双系统时间不同步问题 Windows10系统下如何隐藏一个磁盘盘符，避免Linux盘符误被格式化。隐藏多个盘符：windows中隐藏centos7的磁盘分区 硬盘分区相关说明 一个硬盘至少1个主分区。 一个硬盘中，主分区个数+扩展分区个数&lt;=4。 一个硬盘中，扩展分区至多1个，可以没有。 逻辑分区占用的是扩展分区的空间。可理解为，逻辑分区在扩展分区中。 逻辑分区的个数无限制。 每个逻辑分区可以有自己的文件格式，如NTFS、EXT4等。 主分区转为逻辑分区，不影响到分区上的数据。逻辑分区转为主分区，会清空逻辑分区上的数据，甚至影响到其他已有主分区上的数据。(有一段时间了，记得不是很准确了。总之，数据无价，操作须谨慎)]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>双系统</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7升级OpenSSH到OpenSSH_8.0p1版本]]></title>
    <url>%2F2019%2F08%2F13%2FCentos7%E5%8D%87%E7%BA%A7OpenSSH%E5%88%B0OpenSSH-8-0p1%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[Centos7升级OpenSSH漏洞修复记录。 系统环境CentOS Linux release 7.6.1810 (Core)OpenSSH_7.9p1, OpenSSL 1.0.2k-fips 26 Jan 2017 升级准备 在打开ssh连接工具(如xshell)的同时打开文件传输工具(如xftp)。因为升级过程不能创建新的连接，以备手动上传文件需要。 将要升级的软件包预先下载好，如本次的openssl-1.1.1c.tar.gz、openssh-8.0p1.tar.gz。待升级完成后删除即可。 Openssl: https://ftp.openssl.org/source/ 任意进入一个个人目录，使用wget下载： 12cd /tmpwget https://ftp.openssl.org/source/openssl-1.1.1c.tar.gz Openssh: https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/ 任意进入一个个人目录，使用wget下载： 12cd /tmpwget https://openbsd.hk/pub/OpenBSD/OpenSSH/portable/openssh-8.0p1.tar.gz 注意事项 将/usr/lib64目录下的libssl.so.1.0.2k、libcrypto.so.1.0.2k进行备份。在升级openssl过程中着两个文件会被删除，导致yum、wget、ping等诸多命令不能使用。 123cd /usr/lib64cp libssl.so.1.0.2k /bak #/bak为备份目录cp libcrypto.so.1.0.2k /bak 关闭selinux 临时关闭：setenforce 0 # 采用临时关闭即可 永久关闭：vi /etc/sysconfig/selinux SELINUX=enforcing改为 SELINUX=disabled 重启服务 reboot 关闭防火墙防火墙相关操作 查看防火墙状态：firewall-cmd –state 开启防火墙： 12firewall-cmd –reload #重载配置，不会断开已有tcp会话。推荐使用systemctl restart firewalld #重启服务，会断开已有tcp会话。不推荐 停止防火墙：systemctl stop firewalld.service #建议升级过程中关闭 禁止开机启动：systemctl disable firewalld.service 开放指定端口(22为例)：firewall-cmd --permanent --add-port=22/tcp 安装telnetssh不能登录时的备用登录方案 安装：yum install xinetd telnet-server -y 配置telnet vi /etc/xinetd.d/telnet输入以下内容 1234567891011121314# default: on# description: The telnet server serves telnet sessions; it uses \# unencrypted username/password pairs for authentication.service telnet&#123; # if allow root login, disable = yes disable = no flags = REUSE socket_type = stream wait = no user = root server = /usr/sbin/in.telnetd log_on_failure += USERID&#125; 配置telnet登录的终端类型，在/etc/securetty文件末尾增加一些pts终端,如下 1234pts/0pts/1pts/2pts/3 启动telnet服务 1234567systemctl enable xinetdsystemctl enable telnet.socketCreated symlink from /etc/systemd/system/sockets.target.wants/telnet.socket to /usr/lib/systemd/system/telnet.socket.systemctl start telnet.socketsystemctl start xinetdnetstat -lntp|grep 23tcp6 0 0 :::23 :::* LISTEN 1/systemd 安装依赖包(后面升级需要)12yum install -y gcc gcc-c++ glibc make autoconf openssl openssl-devel pcre-devel pam-devel yum install -y pam* zlib* 升级openssl123456789101112131415161718cd /tmptar -zxvf openssl-1.1.1c.tar.gzcd penssl-1.1.1c./config --prefix=/usr/local/openssl #如果此步骤报错,需要安装perl以及gcc包echo $? #查看上一步命令是否报错0makeecho $? #查看上一步命令是否报错0make installecho $? #查看上一步命令是否报错0mv /usr/bin/openssl /usr/bin/openssl.bak #备份ln -sf /usr/local/openssl/bin/openssl /usr/bin/openssl #创建软连接echo "/usr/local/openssl/lib" &gt;&gt; /etc/ld.so.conf #写入 openssl 库文件的搜索路径ldconfig -v #设置/etc/ld.so.conf生效openssl versionOpenSSL 1.1.1c 28 May 2019 升级openssh 安装 12345678910cp -r /etc/ssh /etc/ssh.bak #备份cd /tmptar -zxvf openssh-8.0p1.tar.gzcd openssh-8.0p1./configure --prefix=/usr --sysconfdir=/etc/ssh --with-openssl-includes=/usr/local/openssl/include --with-ssl-dir=/usr/local/openssl --with-zlib --with-md5-passwords --with-pam #注意参数的路径echo $? #查看上一步命令是否报错0make &amp;&amp; make installecho $? #查看上一步命令是否报错0 配置 123456789101112131415161718192021vi /etc/ssh/sshd_config修改为以下内容PasswordAuthentication yesUseDNS nocd /tmp/openssh-8.0p1mv /etc/init.d/sshd /etc/init.d/sshd.bak #备份cp -a contrib/redhat/sshd.init /etc/init.d/sshd #使用新版本的执行文件cp -a contrib/redhat/sshd.pam /etc/pam.d/sshd.pam #不存在就不管sudo chmod +x /etc/init.d/sshd #添加执行权限，非root用户安装时需要chkconfig --add sshd #将sshd服务设置为开机启动systemctl enable sshd #允许sshd服务自启动mv /usr/lib/systemd/system/sshd.service /tmp #删除或移除原来sshd文件。否则影响sshd服务重启systemctl restart sshd #重启ssh服务。（修改服务配置后都需重启，使配置生效）#一些内容为可选#修改默认端口vi /etc/ssh/sshd_config将Port 修改为其他端口，如2222firewall-cmd --permanent --add-port=2222/tcp #防火墙开放2222端口firewall-cmd –reload #重启防火墙 验证 1234567891011121314chkconfig sshd on #查看服务运行级别#验证是否正常启动/etc/init.d/sshd restart /etc/init.d/sshd stop/etc/init.d/sshd startsystemctl restart sshdsystemctl stop sshdsystemctl start sshdnetstat -lntp #查看sshd服务是否启动以及监听的端口(默认22)ssh -VOpenSSH_8.0p1, OpenSSL 1.1.1c 28 May 2019 修复升级带来的问题 error while loading shared libraries: libssl.so.10: cannot open shared。由于libssl.so 、libcrypto.so 等库文件缺失，导致yum、ping、wget不能使用 1234567cd /usr/lib64cp /u01/libssl.so.1.0.2k ./cp /u01/libcrypto.so.1.0.2k ./ln -s libssl.so.1.0.2k ./libssl.so #创建软连接ln -s libssl.so.1.0.2k ./libssl.so.10ln -s libcrypto.so.1.0.2k ./libcrypto.soln -s libcrypto.so.1.0.2k ./libcrypto.so.10 恢复升级之外的临时配置 关闭telnet 确保重新使用ssh登录没问题后，关闭telnet服务。 12345678910systemctl disable xinetd.serviceRemoved symlink /etc/systemd/system/multi-user.target.wants/xinetd.service.systemctl stop xinetd.servicesystemctl disable telnet.socketsystemctl stop telnet.socket#查看telnet服务是否关闭，任选其一netstat -lntp #查看23端口是否被监听netstat -antp #查看23端口是否被监听chkconfig –list #没有telnet出现即可 恢复防火墙为初始状态]]></content>
      <categories>
        <category>应用软件</category>
      </categories>
      <tags>
        <tag>OpenSSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(十)-主从复制+哨兵模式]]></title>
    <url>%2F2019%2F08%2F11%2FRedis-%E5%8D%81-%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6-%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（十）。主从复制+哨兵模式 1. 是什么？ 主从复制：即 Master/Slave模式。Master主写，Slave主读。当Master数据一有更新就根据配置和策略自动同步到Slave上。 2. 两种种模式（非官方，自我总结） 主从模式：就是集群只有一个Master，所有的从机都直接与master相连接 主从主从模式：即 一台机器既是slave也是master。 举个例子，A是master，B是A的salve，与此同时，B又是C的master。使用info replication查看时，redis仍然认为B是slave，只不过它还有slave。当然此时的B也不能执行写操作。 3. 怎么用？ 原则：配从(库)不配主(库) 使用：执行命令slaveof master-ip master-port，确定master的ip与port即可 其他说明，如果是单机模拟多客户端的话，建议进行一下配置 拷贝多个redis.conf文件 设置属性为：daemonize yes 设置单独的Pid文件名称：pidfile /var/run/redis_6380.pid 设置单独的端口：port 6380 指定Log文件名称：logfile /opt/redis-5.0.5/logs/redis_6380.log 指定rdb文件名称：dbfilename dump_6380.rdb 指定aof文件名称：appendfilename "appendonly_6380.aof" 4. 相关命令 slaveof master-ip master-port：确定master注意：会清空从库数据，并且数据与主库保持同步此时，从库不允许写入 info replication：查看当前客户端主从情况即其他信息 slaveof no one：手动将slave变成master。保持之前slave的数据 redis-sentinel /opt/redis-5.0.5/sentinel.conf：启动哨兵（后面有介绍） 5. 复制原理 slave启动成功并连接到master后会发送一个sync命令 master接收到命令后启动后台的存盘进程，同时收集所有接收到的修改数据的命令。在后台执行完成后，master将传送整个数据文件到slave，就完成了一次主从复制 全量赋值：slave服务在接收到数据文件后，将其存盘并加载到内存中 增量复制：master继续将新的用于修改的命令依次传给slave，以完成同步。但是只要是重新连接master，就会执行一次全量复制 6. 故障处理说明 主机宕机后，从机原地待命，没有变成master。待master恢复后，就像什么都没发生一样，数据仍然保持与master同步。 从机宕机，再恢复，此时变成了master。 从机每次与master断开后，都需要重新连接主机，除非配置进redis.conf文件 在配置了哨兵的情况下，当master宕机，在master恢复前，选出了新的master后再恢复master，原master会自动被设置为slave 7. 主从复制的缺点 由于写操作都在master上执行，然后同步到slave上，所以存在一定的延迟。当系统cpu、内存吃紧时，问题更加严重。 slave数量增加也会导致这个问题。 8. 主从复制的好帮手-哨兵什么是哨兵？在后台监控master是否故障，如果master挂了，自动在slave中进行投票选举出新的master 配置 在自定义目录下，新建sentinel_test.conf文件。 在sentinel_test.conf文件中添加 123456789101112131415 # 指示 Sentinel 监视的主服务器的名称(随便写)、ip、port。 # poll_N：当master挂了，需要一个再次确认的过程。只有当至少有poll_N个 Sentinel判定master失效时，该master才会真的被判定为失效，并且才会执行故障迁移（投票选举新的master） sentinel monitor master_name master_ip master_port poll_N# 指定 Sentinel 认为服务器已经断线所需的毫秒数sentinel down-after-milliseconds master_name 60000 # 故障转移的超时时间（单位毫秒），如果故障转移在指定时间内无法完成，则认为故障转移失败sentinel failover-timeout master_name 180000# 指定在执行故障转移时， 最多可以有多少个从服务器同时对新的主服务器进行同步， 这个数字越小， 完成故障转移所需的时间就越长sentinel parallel-syncs master_name 1 # 根据实际情况选择，在sentinel_test.conf文件中指定sentinel实例的运行端口port 26380 执行命令：redis-sentinel /opt/redis-5.0.5/sentinel_test.conf 或 redis-server /opt/redis-5.0.5/sentinel_test.conf --sentinel启动哨兵 关于哨兵的其他知识点 一个sentinel实例能同时监控多个master，一个master能被多个sentinel实例同时监控。 一个 Sentinel 可以与其他多个 Sentinel 进行连接， 各个 Sentinel 之间可以互相检查对方的可用性， 并进行信息交换。 参考：http://www.redis.cn/topics/sentinel.html]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(九)-消息订阅与发布]]></title>
    <url>%2F2019%2F08%2F09%2FRedis-%E4%B9%9D-%E6%B6%88%E6%81%AF%E8%AE%A2%E9%98%85%E4%B8%8E%E5%8F%91%E5%B8%83%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（九）。Redis中消息订阅与发布 1. 定义Redis 发布订阅(pub/sub)：是一种进程间消息通信模式——发送者(pub)发送消息，订阅者(sub)接收消息。 2. 示意图来源：Redis发布订阅|菜鸟教程 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系：当有新消息通过 publish命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 3. 案例先订阅后发布才能接收到消息。Redis 客户端可以订阅任意数量的频道。 步骤： A-client 订阅渠道，一次可订阅多个。支持通配符： 12subscribe channel*subscribe channel1 channel2 channel3 B-client 在channel2发布消息，发布后订阅者将会收到该条消息 1publish channel2 hello-subscriber 4. Springboot中使用Redis的sub/pub Redis的基础信息配置——application.properties 123spring.redis.host=127.0.0.1 # Redis主机ipspring.redis.port=7989 # Redis服务端口spring.redis.password= # Redis密码，无的话，留空 订阅监听的配置类——RedisPubSubConfig.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.data.redis.listener.PatternTopic;import org.springframework.data.redis.listener.RedisMessageListenerContainer;import org.springframework.data.redis.listener.adapter.MessageListenerAdapter;/** * Redis订阅监听 */@Configurationpublic class RedisPubSubConfig &#123; /** * 1. 使用默认的工厂 初始化redis操作模板 */ @Bean StringRedisTemplate template(RedisConnectionFactory connectionFactory) &#123; return new StringRedisTemplate(connectionFactory); &#125; /** * 2. 创建消息监听器适配器，绑定消息处理器，利用反射技术调用消息处理器的业务方法 */ @Bean MessageListenerAdapter listenerAdapter(RedisReceiverHandler redisReceiverHandler) &#123; System.out.println("消息适配器进来了"); return new MessageListenerAdapter(redisReceiverHandler, "handle"); &#125; /** * 3. 创建redis消息监听器容器 * 可以添加多个监听不同话题的redis监听器，只需要把消息监听器和相应的消息订阅处理器绑定，该消息监听器 * 通过反射技术调用消息订阅处理器的相关方法进行一些业务处理 * @param connectionFactory 默认的连接工厂 * @param listenerAdapter 消息监听器适配器 * @return 消息监听容器 */ @Bean RedisMessageListenerContainer container(RedisConnectionFactory connectionFactory,MessageListenerAdapter listenerAdapter) &#123; // 创建消息监听容器 RedisMessageListenerContainer container = new RedisMessageListenerContainer(); // 设置连接工厂 container.setConnectionFactory(connectionFactory); //可以添加多个 messageListener container.addMessageListener(listenerAdapter, new PatternTopic("index")); return container; &#125; &#125; 收到消息后的处理类——RedisReceiverHandler.java 1234567891011121314151617import org.springframework.stereotype.Component;/** * 具体的消息处理类 */@Componentpublic class RedisReceiverHandler &#123; /** * 消息处理方法 * @param message 发布者发布的消息 */ public void handle(String message) &#123; System.out.println("消息来了："+message); //这里是收到通道的消息之后执行的方法 &#125;&#125; 消息发布类——RedisPublishMsg.java 123456789101112131415161718192021222324import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.StringRedisTemplate;import org.springframework.scheduling.annotation.EnableScheduling;import org.springframework.scheduling.annotation.Scheduled;import org.springframework.stereotype.Component;/** * 定时器，用于定时发布消息 */@EnableScheduling@Componentpublic class RedisPublishMsg &#123; @Autowired private StringRedisTemplate stringRedisTemplate; /** * 定时通过redis操作模板，向redis消息队列index通道发布消息 */ @Scheduled(fixedRate = 3000) public void pubMsg()&#123; stringRedisTemplate.convertAndSend("index",String.valueOf(Math.random())); &#125;&#125;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(八)-事务]]></title>
    <url>%2F2019%2F08%2F08%2FRedis-%E5%85%AB-%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（八）。Redis中的事务 1. Redis事务是什么 一次执行多个命令，本质是一组命令的集合。事务中的所有命令都会序列化，按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断 2. 事务相关的命令 watch key [key...]：监视一个或多个key。如果在事务执行之前这个或这些key被其他命令锁改动，那么事务会被打断 multi：标记一个事务块的开始 exec：执行所有事务块内的命令 discard：取消事务，放弃执行事务块内的所有命令 unwatch：取消watch命令对所有key的监视 3. 事务案例演示 正常执行1234567multi # 开启事务set k1 v1 # 入队set k2 v2 # 入队get k2 # 入队set k3 v3 # 入队.... # 入队exec # 执行事务 放弃事务 1234567multi # 开启事务set k1 v1 # 入队set k2 v2 # 入队get k2 # 入队set k3 v3 # 入队.... # 入队discard # 放弃事务 命令语句错误(全体连坐) 12345678multiset k1 v1set k2 v2 get k2get set excprisks # 这里会入队错误set k5 v5...exec 此时get set excprisks加入队列报错，导致当前事务中全部语句不能被执行 执行时出错(冤头债主) 1234567multiset k1 v1incr k1 # 此时v1为非数字，执行时会出错set k2 v2get k2...exec 此时incr k1 加入队列成功，只是执行的时候会报错。但是其他命令都会成功执行 由第3、4点得出：Redis的事务是部分支持 4. watch监控 注意：一定是先监控，再开启事务 案例 说明：此时Redis库中已有值balance-100、debt-0 123456watch balance # 此时balance为100，debt为0 # 假设，这里有其他客户端A将balance修改为200multidecrby balance 20incrby debt 20exec 本次事务执行失败。因为在事务执行之前balance数据已经不是watch时的值。此时，获取到的balance为200。 如果watch后，数据被人改了，执行unwatch命令，取消对所有key的watch。然后再重新watch，再执行事务。 总结watch 类似乐观锁，事务提交时，如果key的值被其他客户端修改了，整个事务队列不会执行。通过watch命令在事务之前监控了多个keys，如若在watch之后有任何被监视的key的值发生了变化，exec命令执行的事务都会被放弃，并返回Nullmulti-bulk告诉调用这事务执行失败。 5. Redis事务总结 单独的隔离操作 事务中所有命令都会序列化，按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念 队列中的命令没有提交之前都不会实际的被执行，也就不存在事务内的查询要看到事务里的更新。 不保证原子性 redis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(七)-RDB&AOF]]></title>
    <url>%2F2019%2F08%2F01%2FRedis-%E4%B8%83-RDB-AOF%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（七）。Redis之RDB与AOF对比 1. 使用建议 如果只使用Redis的缓存功能(数据只存在与服务器运行的时候)，可以不使用任何持久化 仍然建议同时开启RDB、AOF备份。通常AOF文件备份的数据比RDB文件备份的要更加完整，用AOF来保证数据不丢失，作为恢复数据的第一选择；用RDB来做不同程度的冷备，在AOF文件都丢失或损坏不可用的时候，可以使用RDB进行快速的数据恢复。 性能方面 建议只在slave上持久化RDB文件，而且值要15分钟备份一次就够了。 如果开启 AOF，好处是在最坏的情况下也只会丢失不超过2秒的数据，启动时只需要载入自己的AOF即可。 代价是带来了持续的I/O 二是AOF rewrite的最后将rewrite过程中产生的新数据写入到新文件造成的阻塞几乎是不可避免的。只要硬盘允许，应尽量减少AOF rewrite的频率，AOF重写的基础大小默认64M太小，可以设置到5G以上。默认超过原大小100%时进行重写，可以修改到适当的值。 如果不开启 AOF，仅靠master-slave Replication 实现高可用也是可以的，能省掉一大笔I/O, 同时较少了Rewrite带来的系统波动。 代价是，如果master和slave同时down掉，会丢失较长时间数据，启动是也要比较master/slave中的RDB文件，载入较新的那个。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(六)-持久化之AOF]]></title>
    <url>%2F2019%2F08%2F01%2FRedis-%E5%85%AD-%E6%8C%81%E4%B9%85%E5%8C%96%E4%B9%8BAOF%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（六）。Redis之AOF，重要！！！ 1. AOF是什么 AOF(append only file) ：以日志的形式来将Redis执行过的所有写指令记录到文件中，且只允许追加文件但不可以更改文件。默认文件名：appendonly.aof flushall也会被当做写操作，记录进文件 2. 如何启动AOF备份 默认是关闭AOF备份功能的，将redis.conf中的appendonly no改为appendonly yes，即可开启AOF备份 3. 如何使用AOF进行数据恢复 保证appendonly 被设置为yes 将AOF文件放在redis安装目录。（使用config get dir 命令获取目录） 启动redis 4. AOF恢复的原理 Redis启动之初会读取该文件重新构建数据。就是将日志文件中的指令从头到尾执行一次，以完成数据的恢复工作 5. AOF的同步策略 appendfsync always：每修改同步。每次发生数据变化，就立刻记录到aof文件中。性能差，但数据完整性高 appendfsync everysec：默认。每秒同步，异步操作。每秒将数据变化记录到aof文件中。服务器一秒内down掉，会有数据丢失。这个操作是由一个线程专门负责执行的 appendfsync no： 将aof缓冲区中的内容写入到AOF文件中，但并不对AOF同步，何时同步交给OS 6. AOF-Rewrite 由AOF的定义，我们不难知道AOF文件只会越来越大。为了避免这种清空，增加了重写机制 当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集 可以使用bgrewriteaof命令异步执行一个 AOF-Rewrite操作 原理 AOF文件大小超过设定的阈值时，Redis会fork一条新的进程来将文件重写(先写成临时文件再rename)。 Redis会重新遍历fork出进程的内存中的数据，记录每一个与变更数据有关的指令。 重写AOF文件的操作，并不是读取就的AOF文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的AOF文件。 何时触发aof-rewrite 默认配置是：当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发（Redis会记录上次重写是的AOF大小） 如何配置 auto-aof-rewrite-percentage 100：触发重写的基准值 auto-aof-rewrite-min-size 64mb：触发重写基准值 no-appendfsync-on-rewrite no：重写时是否可以运用同步策略(appendfsync)。用默认no即可，保证数据安全 7. 如何修复损坏的AOF文件 AOF文件存在错误时，Redis是无法启动的。使用redis-check-aof –fix appendonly.aof 命令进行修复 8. AOF优劣势 优势 AOF可以更好的保护数据不丢失，一般AOF会以每隔1秒，通过后台的一个线程去执行一次fsync操作，如果redis进程挂掉，最多丢失1秒的数据。 AOF以appen-only的模式写入，所以没有任何磁盘寻址的开销，写入性能非常高。 AOF 文件里面包含一个接一个的操作，具有很强的易读性。例如，即使你不小心错误地使用 FLUSHALL 命令清空一切，如果此时并没有执行重写，你仍然可以保存你的数据集，你只要停止服务器，删除最后一条命令，然后重启 Redis 就可以恢复 劣势 相同数据集的数据而言，AOF文件远大于RDB文件，恢复速度慢与RDB AOF运行效率慢与RDB，每秒同步策略效率较好，不同步效率和RDB相同 9. RDB&amp;AOF优先级 同时开启RDB和AOF持久化的情况下，Redis会优先且只载入AOF文件来恢复数据。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(五)-持久化之RDB]]></title>
    <url>%2F2019%2F07%2F30%2FRedis-%E4%BA%94-%E6%8C%81%E4%B9%85%E5%8C%96%E4%B9%8BRDB%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（五）。Redis之RDB，重要！！！ 1. RDB是什么 RDB（Redis Database）：在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是术语Snapshot快照，它恢复时是将快照文件直接读到内存。默认文件名：dump.rdb 2. RDB产生的过程 Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程结束后，再用这个临时文件替换上次持久化的文件。 整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能。 其中：fork的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等）数值都和原进程一致，并且作为原进程的子进程。 3. 何时会产生RDB文件 满足配置文件(redis/redis.conf)中默认的快照配置，就会产生一个新的dump.rdb文件。只需满足一个即可 save 900 1 # 900秒（15分钟）内有1次更改 save 300 10 # 300秒（5分钟）内有10次更改 save 60 10000 # 60秒内有10000次更改。 使用save命令，可以立刻执行序列化操作，产生新的dump.rdb文件。 此时redis只执行保存操作，其他功能阻塞，此时不能写入。 使用bgsave命令，redis在后台异步进行快照备份。 同时还可以响应客户端请求，允许写入。 可通过lastsave命令来获取最后一次成功执行快照的时间 使用shutdown命令，会立刻执行备份操作，产生新的rdb文件。（前提是开启了rdb备份） 使用flushall命令，也会立刻产生新的rdb文件。 会清空数据，无实际意义 flushdb命令不会产生新的rdb文件 4. 如何恢复RDB中的数据 将备份文件(dump.rdb)放在redis安装目录。（使用config get dir 命令获取目录） 启动服务即可完成恢复工作。 5.RDB优劣势 优势 适合大规模数据恢复。这要求是在对数据完整性和一致性要求不高的前提下 生成RDB文件的时候，redis主进程会fork()一个子进程来处理所有保存工作，主进程不需要进行任何磁盘IO操作，从而确保了极高的性能 与AOF相比，在恢复大数据集的时候更快一些 rdb劣势 redis意外挂掉的话，会丢失最后一次快照备份后的所有修改 没办法做到实时持久化。因为，每次备份时都要fork一个子进程。而fork的时候，内存中的数据被克隆了一份，会导致大约2倍的数据膨胀，耗内存耗时，可能会导致不能响应一些毫秒级的请求，这点需要考虑 RDB文件使用特定二进制格式保存，在Redis版本演进过程中有多个格式的RDB版本，存在老版本Redis服务无法兼容新版RDB格式的问题(版本不兼容) 6. 停止RDB备份 方式一：注释掉redis.conf中的 123save 900 1save 300 10save 60 10000 方式二：执行命令：redis-cli config set save “”只对本次启动有效 注意：只去掉redis.conf中save “”前面的#，是不行的 7. 其他 Redis默认也是采用RDB方式进行备份的，AOF功能是关闭的]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(四)-配置文件解读]]></title>
    <url>%2F2019%2F07%2F29%2FRedis-%E5%9B%9B-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（四）。Redis配置文件解读 修改前将默认的配置文件(redis/redis.conf)拷贝一份 redis.conf 配置项说明如下： 单位 1k =&gt; 1000 bytes 1kb =&gt; 1024 bytes 1m =&gt; 1000000 bytes 1mb =&gt; 10241024 bytes 1g =&gt; 1000000000 bytes 1gb =&gt; 10241024*1024 bytes 只支持bytes，不支持bit 单位是大小写不敏感的 有字母b和没有是有区别的 设置数据库的数量，默认数据库为0，可以使用SELECT 命令在连接上指定数据库id databases 16 Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程 daemonize no 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定 pidfile /var/run/redis.pid 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为notice loglevel notice 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile “” 指定日志文件路径（需要写权限） logfile /opt/redis-5.0.5/logs/redis.log 指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字port 6379 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save &lt;seconds&gt; &lt;changes&gt; Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改，60秒内有10000个更改。只要满足其一都会触发持久化操作。 指定内存快照文件(dump.rdb)存放目录（redis启动目录） dir ./ 修改日志文件目录(需要写权限) logfile /opt/redis-5.0.5/logs/redis.log 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf 是否把日志输出到系统日志。默认：nosyslog-enabled no 系统日志中的日志标识syslog-ident redis 输出日志的设备。local0-local7。默认：local0syslog-facility local0 集群中，各个redis之间keepalive的检测时间间隔。0：不进行keepalive检测tcp-keepalive 0 设置backlog，解决慢客户端连接问题。提示：backlog其实是一个连接队列，backlog队列总和=未完成三次握手队列+已完成三次握手队列。 在高并发环境下你需要 一个高backlog值来 避免 慢客户端连接问题。注意：Linux内核会将这个值减小到proc/sys/net/core/somaxconn的值，所以需要确认增大somaxconn和tcp_max_syn_backlog两个值tcp-backlog 511 过期策略。默认noeviction,不移除key LRU：近期最近最少使用 volatile-lru：使用lru算法移除key，只对设置了过期时间的键。 allkeys-lru：使用lru算法移除key random：随机 volatile-random：在过期集合中移除随机的key，只对设置了过期时间的键 allkeys-random：移除随机的key TTL(time to leave)：有限时间内 volatile-ttl：移除那些TTL值最小的key， 即那些最近要过期的key noeviction：不进行移除。针对写操作，只是返回错误信息 maxmemory-policy：noeviction 设置样本数量。LRU算法和最小TTL算法都并非是精确的算法，而是估算值，所以可以设置样本的大小。redis默认会检查这么多key并选择其中LRU的那个maxmemory-samples：5 绑定的主机地址 bind 127.0.0.1 当 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能timeout 300 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大rdbcompression yes 指定本地数据库文件名，默认值为dump.rdbdbfilename dump.rdb 设置当本机为slave服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步slaveof &lt;masterip&gt; &lt;masterport&gt; 当master服务设置了密码保护时，slave服务连接master的密码masterauth &lt;master-password&gt; 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH &lt;password&gt;命令提供密码，默认关闭requirepass foobared 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息maxclients 128 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区maxmemory &lt;bytes&gt; 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为noappendonly no 指定更新日志文件名，默认为appendonly.aofappendfilename appendonly.aof 指定更新日志条件，共有3个可选值：no：表示等操作系统进行数据缓存同步到磁盘（快）always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）everysec：表示每秒同步一次（折衷，默认值）appendfsync everysec 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中vm-enabled no 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享vm-swap-file /tmp/redis.swap 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0vm-max-memory 0 Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值vm-page-size 32 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，，在磁盘上每8个pages将消耗1byte的内存。vm-pages 134217728 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4vm-max-threads 4 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启glueoutputbuf yes 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法hash-max-zipmap-entries 64hash-max-zipmap-value 512 指定是否激活重置哈希，默认为开启activerehashing yes 后台序列化rdb文件报错后，是否停止前台写功能。默认yes stop-writes-on-bgsave-error yes 对于存储到磁盘中的快照(dump.rdb文件)，是否采用LZF的压缩算法。默认yes。如果不想损耗cpu来压缩的话，可以设置为no，关闭此功能rdbcompression yes 在存储快照后，是否采用CRC64算法来进行数据校验，默认yes。如果不想cpu增加大约10%的消耗，可以关闭次功能rdbchecksum yes 默认内存快照备份文件的名称dbfilename dump.rdb]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(三)-五大数据类型+操作命令]]></title>
    <url>%2F2019%2F07%2F27%2FRedis-%E4%B8%89-%E4%BA%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（三）。五大数据类型+对应的操作命令 常用五大数据类型 string、list、hash、set、sorted set 五大数据类型-String sting 是redis最基本的类型 是二进制安全的，即redis的string可以包含任何数据。如图片或者序列化的对象。 单个string类型的value大小限制为512M 常用操作 set key val：redis的insert操作。key存在时，值会被覆盖 get key：redis的select操作 del key：redis的删除操作 append key abc：将abc追加到原字符串后面，形成新的字符串。key不存在时会执行insert操作 strlen key：查看当前key字符串长度。一个汉字占3个字符 incr key：值加一。(要求字符串为数字) decr key：值减一。(要求字符串为数字) incrby key 3：值加3 decrby key 3：值减3 getrange key 0 3：得到字符串的前4个字符。(注意redis中是前后都取) setrange key 0 abc：字符串部分替换。本例：从下标0开始，使用abc替换原字符串中对应位置的字符串。如果原字符串为123456，执行后为，abc456 setex key 10 val：设置一个生命周期为10秒的kv对 setnx key val：如果key不存在，执行insert操作。否则，不执行任何操作 mset k1 v1 k2 v2 k3 v3：同时设置多个kv对。 (key已存在，值会被覆盖) mget k1 k2 k3：同时获取多个值 msetnx k1 v1 k2 v2 k3 v3：当k1，k2，k3都不存在时才执行insert操作。否则，不执行任何操作 五大数据类型-hash（重要） 是一个键值对集合，类似java里的Map 是一个string类型的key和value的映射表。即，value是一个键值对，其key为String类型 适合存储对象。类似java中Map&lt;String, Object&gt; 常用操作 hset user id 123 name ysl：新建一个hash，key：user，value包含id、name，值分别为123、ysl hget user id：获取user中id的值 hmset user id 123 name ysl age 23：新建一个hash，key：user，value包含id、name、age，值分别为123、ysl、23 hmget user id name age：批量获取值 hgetall user：获取user中所有的k-v对 hdel user age：删除user中age键值对 hlen user：user中包含的k-v对的个数 hexists user email：user中是否包含email这个key hkeys user：user中所有的key。(id、name、age) hvals user：user中所有的value。(123、ysl、23) hincrby user age 2：user中age对应的值+2 hincrbyfloat user score 1.5：user中score对应的值+1.5 hsetnx user age 3：user中不存在age时，执行插入操作 五大数据类型-list 一个字符串列表，按照插入顺序排序，可以在列表首尾添加元素 底层是一个链表。（操作链表首位效率高，中间低） 值全部移除，对应的列表也就没有了 常用操作 lpush list01 1 2 3 4 5：创建list01，并插入值。（正进反出） lrange list01 0 -1：从0(起始)到-1(结束位置)，查看列表中元素。结果为：5 4 3 2 1 rpush list02 1 2 3 4 5：创建list01，并插入值。(正进正出)。此时lrange list02 0 -1, 结果为：1 2 3 4 5 lpop list01 ：取列表顶部第一个元素。(list01得到5，list02得到1) rpop list01：取列表底部第一个元素。(list01得到1，list02得到5) lindex list01 2：取列表下标为2的元素 llen list01：列表长度 lrem list01 N val：删除N个Value ltrim list01 start end：截取下标从start到end的值，重新赋值给list01 rpoplpush list01 list02：将list01的底部元素移动到list02的顶部。(只移动一个) lset list01 idx value：修改列表指定下标的值为value linsert list01 before/after val1 val2：在值为val1之前/之后插入val2 五大数据类型-set set是string类型的元素集合，无序无重复，通过HashTable实现 常用操作 sadd set01 1 1 2 2 3 3：向集合set01(没有会创建)中添加1 2 3 smembers set01：查看集合中的元素 sismember set01 1：值1是否在集合set01中 scard set01：获取集合中元素个数 srem set01 val1：删除集合中指定元素 srandmember key N：从集合中，随机取N个元素 spop set01：随机取一个元素 smove set01 set02 val1：将set01中的val1移动到set02中 sdiff set01 set02 set03：求集合的差集。得到的数据是：在第一个set中，不在后面任何一个set中的元素项 sinter set01 set02 set03：求集合交集。得到的数据：所有集合中都有的元素项 sunion set01 set02：求集合并集。得到的数据：所有集中元素项去重后的结果 五大数据类型-sorted set (zset) 与set一样是string类型元素集合，有序无重复 每个元素都关联一个double类型的分数(score)，redis正是通过分数来为集合中的成员进行从小到大的排序, set的 成员是唯一的，但分数却可以重复 可以理解为，key与set中一样，只不过 value 变成了 score与数据值 常用操作 zadd zset01 60 val1 70 val2 80 val3：新建一个zset。key：zset01，value：60 val1、70 val2… zrange zset01 0 -1：获取zset01中所有的值 zrange zset01 0 -1 withscores：获取zset01中所有值，及其对应的分数 zrangebyscore zset01 60 (80：获取score从60到80之间的数据（包含60不包含80），并按照score排序 zrangebyscore zset01 60 80 limit 2 3：获取score从60到80之间数据，并按照score排序。再取结果中下标为2到5（2+3）的数据项作为最终的结果 zrem zset01 val1：删除zset01下值为val1的项 zcard zset01：统计zset01中元素个数 zcount zset01 60 80：统计score从60到80之间的元素个数 zrank zset01 val1：获取val1对应的下标 zscore zset01 val1：获取val1对应的score zrevrank key val1：逆序获取val1的下标 zrevrange zset01 0 -1：逆序获取集合 zrevrangebyscore zset01 80 60：获取zset01中结束score到开始score的元素项 说明：redis5.x新增了Stream流数据类型]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(二)-入门基础知识+基础命令]]></title>
    <url>%2F2019%2F07%2F22%2FRedis-%E4%BA%8C-%E5%85%A5%E9%97%A8%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86-%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（二）。入门基础知识+基础命令 基础知识 默认端口6379 redis索引从0开始 单进程模型处理客户端的请求。对读写时间的响应是通过对epoll函数的包装来实现的。所以，redis的实际处理速度完全依靠主进程的执行效率 默认16个数据库，下标从0开始，默认使用0号库。使用select x切换数据库 统一密码管理，对16个库都是同一个密码 redis默认不转义中文，如果在命令行中想要看到中文，启动redis时添加--raw参数。即，redis-cli -p 6379 -- raw redis中默认一个中文占3个字符 redis-cli config set save "" ：停止内存快照备份。即，不产生dump.rdb文件 /usr/local/bin/redis-benchmark： 压力测试工具。(间接测试电脑性能) /usr/local/bin/redis-check-aof：自动修复aof文件(数据库的持久化文件)。如，redis-check-aof --fix appendonly.aof /usr/local/bin/redis-check-rdb：自动修复rdb文件(数据库的持久化文件)。如，redis-check-rdb --fix dump.rdb 基础命令 置顶：Redis命令参考 set key val：redis的insert操作。key存在时，值会被覆盖 get key：redis的select操作 del key：redis的删除操作 dbsize： 查看当前库key的数量 flushdb：清空当前库。(不会产生新的dump.rdb文件) flushall: 清空全部库。(会产生新的dump.rdb文件) config get xxx：获取某配置参数。如，config get requirepass：获取redis的连接密码config get dir：获取redis快照文件（dump.rdb）的路径。（默认在redis启动路径） config set requirepass "123456"：设置连接密码 auth 123456：输入连接密码 key 关键字 keys *：查看当前库所有的key。支持通配符，如，keys myke? exists key：判断key是否存在 move key idx：将指定的key移动到指定下标的库 expire key 2：设置key在2秒后过期 ttl key：查看还有多少秒过期。-1表示永不过期，-2表示已过期 type key：查看key的类型]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux(Ubuntu 18.04 LTS)使用记录]]></title>
    <url>%2F2019%2F07%2F21%2FLinux-Ubuntu-18-04-LTS-%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[一篇关于Ubuntu的使用记录，包含分区，双系统安装教程，常用软件安装及配置… 系统：ubuntu18.04 LTS 置顶安利一个网站：https://www.lulinux.com/deepin-wine-QQ、百度网盘、WPS等应用解决方案：https://www.lulinux.com/archives/1319deepin软件仓库：http://packages.deepin.com/deepin/pool/non-free/双系统的安装：Windows10+Ubuntu18.04+双硬盘(SSD+HDD)安装双系统 1. 分区与挂载Linux新建分区https://blog.csdn.net/github_33809414/article/details/85208666/usr目录挂载到新分区https://blog.csdn.net/github_33809414/article/details/85207300/var目录挂载到新分区https://blog.csdn.net/github_33809414/article/details/85221274开机自动挂载https://blog.csdn.net/qwfys200/article/details/79737780 2. 连接校园网https://blog.csdn.net/github_33809414/article/details/85054911 3. 替换阿里源https://www.cnblogs.com/lyon2014/p/4715379.html 4. 安装搜狗输入法https://blog.csdn.net/fx_yzjy101/article/details/80243710重启后，搜狗拼音输入法候选栏无法显示中文（乱码）https://my.oschina.net/i1sfish/blog/1822964中英文无法切换我的解决方法是: fcitx 设置中加入英文键盘即可 5. firefox浏览器安装flashhttps://blog.csdn.net/weixin_40522162/article/details/80287296 6.安装百度云，失败，参考置顶https://jingyan.baidu.com/article/a24b33cdc99c1e19fe002b93.htmlhttps://blog.csdn.net/cheng_ku/article/details/80286172 7. 安装crossover，建议不要折腾windows应用很卡，还要收费，放弃https://my.oschina.net/u/3275937/blog/2995386https://blog.csdn.net/valecalida/article/details/81084210 8. 安装vmware!!不要升级内核，就用默认的(4.15.0-46-generic)。否则vmware不能运行。最终安装成功版本VMware Workstation Pro 15.0.2 for Linux，参照的第二个链接https://www.jianshu.com/p/5fc43bbbac5ahttps://blog.csdn.net/wy_bk/article/details/80017140需要安装gcc7https://blog.csdn.net/weixin_35762621/article/details/80336291问题：/vmware/Ubuntu.vmdk' or one of the snapshot disks it depends onhttps://blog.csdn.net/guomutian911/article/details/42673147 9. 内核的升级与卸载升级内核https://www.linuxidc.com/Linux/2018-06/152714.htm卸载内核https://blog.csdn.net/wf19930209/article/details/81879777 10. 安装/卸载mysqlmysql5.7安装https://segmentfault.com/a/1190000012703513 (目录名称都不要自定义)其中： /etc/my.cnf 中 pid-file=自定义一个有权限的目录 ，/var/run下重启后mysqld目录会消失，导致无法启动卸载https://www.cnblogs.com/jpfss/p/7932019.html问题：没有chkconfig命令https://blog.csdn.net/elim051/article/details/6173367You must reset your password using ALTER USER statement before executing this statement.https://www.cnblogs.com/debmzhang/p/5013540.html mysql8.0.16https://www.cnblogs.com/luoli-/p/9249769.html 11. 安装navicat在安装好deepin-wine的环境下：安装navicat11.2下载：http://packages.deepin.com/deepin/pool/non-free/n/navicat/navicat_11.2.9_amd64.deb进入安装目录 cd /usr/share/navicat/ 删除全部内容 sudo rm -r Navicat 将Windows中破解好的Navicat拷贝到 /usr/share/navicat/ 目录下 sudo cp -r Navicat /usr/share/navicat/ 注意：由于deepin的navicat是11.2.9的，所以windows下的破解版也要是11.2版本的才可以传送门：https://pan.baidu.com/s/1hsfBv2VtAd0jsekJwQGJuA 提取码：71nu 安装navicat12.1.18https://www.52pojie.cn/thread-966298-1-1.htmlhttps://github.com/DoubleLabyrinth/navicat-keygen/blob/windows/README_FOR_LINUX.zh-CN.md#navicat-keygen---for-linux 解决navicat中文乱码https://blog.csdn.net/qq_38250124/article/details/83898364https://blog.csdn.net/yuxiao97/article/details/84886921 12. 安装postmanhttp://packages.deepin.com/deepin/pool/non-free/p/postman/postman_6.0.9_amd64.deb 13. 安装ieaseMusichttps://github.com/trazyn/ieaseMusic/releases/download/v1.2.6/ieaseMusic-1.2.6-linux-amd64.deb 13. 安装qq，失败，参考置顶https://blog.csdn.net/qq_36428171/article/details/81209475 13. 安装微信，参考置顶网页版微信,加了个壳，没有电脑版好用 https://www.cnblogs.com/dunitian/p/9124806.html 14. 区域语言设置http://blog.sina.com.cn/s/blog_49f914ab0100sbsp.html 15. 静态iphttps://jingyan.baidu.com/article/29697b91558683ab20de3c95.htmlhttps://blog.csdn.net/qq_42975842/article/details/81705244 16. ssh免密登录https://blog.csdn.net/changhenshui1990/article/details/72896548https://www.cnblogs.com/ivan0626/p/4144277.html系统重装后清除原来的ssh信息：ssh-keygen -f "~/.ssh/known_hosts" -R "目标ip" 17. 安装openvpn安装http://www.seekswan.com/support_course_info_178开机自启openvpnhttps://www.jianshu.com/p/9af5d5a046f9 18. 卸载Ubuntu中wine安装的程序https://www.cnblogs.com/jackchiang/p/4072621.html 19. 安装markdown编辑器(Typora)http://packages.deepin.com/deepin/pool/non-free/t/typora/typora_0.9.53-1_amd64.deb 20. WPS for Linux（ubuntu）字体配置(字体缺失解决办法)https://www.cnblogs.com/liangml/p/5969404.html其他错误：INCORRECT PERMISSIONS ON /USR/LIB/PO1KIT-AGENT-HELPER-1(NEEDS TO BE SETUID ROOT)https://www.cnblogs.com/tl542475736/p/9461669.html 21. 在linux中使用vi 打开文件时，能显示行号https://blog.csdn.net/yuanyuan_186/article/details/51306202 22. 安装Virtualboxhttps://www.cnblogs.com/pealicx/p/9028414.htmlhttps://blog.csdn.net/weixin_43331296/article/details/83025126 23. 设置样式(桌面、顶栏、字体)https://www.jianshu.com/p/de37682ecbda]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis(一)-安装+HelloWorld]]></title>
    <url>%2F2019%2F06%2F30%2FRedis-%E4%B8%80-%E5%AE%89%E8%A3%85-HelloWorld%2F</url>
    <content type="text"><![CDATA[Redis学习笔记（一）。安装+HelloWorld Redis简介 Redis(Remote Dictionary Server: 远程字典服务器)本质是一个Key-Value类型的分布式内存数据库。 整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理约8万次写操作，10万次读操作，是已知性能最快的Key-Value DB。 Redis的出色不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像memcached只能保存1MB的数据。Redis提供String、List、Set、Sorted Set、hashes等数据结构的存储，可以用来实现很多有用的功能，比如用他的List来做FIFO(First In First Out)双向链表，实现一个轻量级的，高性能的，消息队列服务，用他的Set可以做高性能的tag系统等等。 另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当做一个功能加强版的memcached来用。同时，Redis支持数据的备份，即master-slave模式的数据备份。并且Redis的所有操作都是原子性的。 Redis的主要缺点是数据库容量受物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 下载与安装下载：Redis官网注意：这里下载的是Linux版本如果需要下载windows版本可以进入Redis中文网，但是不建议使用windows版本。 安装Redis环境 Ubuntu 18.04 LTS redis-4.0.11 安装 将下载好的redis-4.0.11.tar.gz压缩拷贝到/opt目录(Linux下第三方软件安装目录)下，并使用sudo tar -zxvf redis-4.0.11.tar.gz命令解压，接着进入redis-4.0.11目录 执行 sudo make命令。( 如果报错，就执行sudo apt-get install gcc安装gcc) 命令执行完后会出现如下结果。这里可以不执行make test测试，因为可能还需要你安装TCL插件，比较麻烦 最后执行make install完成最后的安装 常用配置： 进入/usr/local目录(Linux应用程序存放目录), 并新建一个目录用户存放redis的配置文件，如我这里叫myredis, 将/opt/redis-4.0.11/redis.conf文件拷贝到myredis目录，这样可以做可以使redis默认的配置文件不受影响 修改/usr/local/myredis/redis.conf文件 Redis_HelloWorld启动redis：进入/usr/local/bin目录，执行redis-server /usr/local/myredis/redis.conf查看redis服务是否启动：执行ps -ef | grep redis可以看到redis服务已经启动，并且端口是默认的6379端口连接到redis数据库：执行redis-cli -p 6379测试redis是否连接成功如上图即代表连接成功！ 体验redis 关闭redis 遇到的一点小麻烦，关闭redis时提示：(error) ERR Errors trying to SHUTDOWN. Check logs. 解决方法参考：https://blog.csdn.net/github_33809414/article/details/82531642]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NoSQL概述]]></title>
    <url>%2F2019%2F06%2F30%2FNoSQL%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[NoSQL的初步了解 引言 随着互联网web2.0网站的兴起，传统的关系数据库在应付web2.0网站，特别是超大规模和高并发的SNS类型的web2.0纯动态网站已经显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展。NoSQL数据库的产生就是为了解决大规模数据集合多重数据种类带来的挑战，尤其是大数据应用难题，包括超大规模数据的存储。(例如谷歌或Facebook每天为他们的用户收集万亿比特的数据)。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。 如今天我们可以通过第三方平台（如：Google,Facebook等）可以很容易的访问和抓取数据。用户的个人信息，社交网络，地理位置，用户生成的数据和用户操作日志已经成倍的增加。我们如果要对这些用户数据进行挖掘，那SQL数据库已经不适合这些应用了, NoSQL数据库的发展也却能很好的处理这些大的数据。 是什么? NoSQL(NoSQL = Not Only SQL )，意即”不仅仅是SQL”。泛指非关系型的数据库。是对不同于传统的关系型数据库的数据库管理系统的统称。 RDBMS vs NoSQL RDBMS(关系型) NoSQL(非关系型) 高度组织化结构化数据 代表着不仅仅是SQL 结构化查询语言（SQL） 没有声明性查询语言 数据和关系都存储在单独的表中 没有预定义的模式 数据操纵语言，数据定义语言 键 - 值对存储，列存储，文档存储，图形数据库 严格的一致性 最终一致性，而非ACID属性 基础事务 非结构化和不可预知的数据 &nbsp; CAP定理 &nbsp; 高性能，高可用性和可伸缩性 CAP定理（CAP theorem） CAP定理（CAP theorem）, 又被称作布鲁尔定理（Brewer’s theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点: 一致性(Consistency) (所有节点在同一时间具有相同的数据) 可用性(Availability) (保证每个请求不管成功或者失败都有响应) 分区容错性(Partition tolerance) (系统中任意信息的丢失或失败不会影响系统的继续运作) CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。 因此，根据 CAP 原理将 NoSQL 数据库分成了满足 CA 原则、满足 CP 原则和满足 AP 原则三 大类： CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。 CP - 满足一致性，分区容忍性的系统，通常性能不是特别高。 AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。CAP theorem ACID vs BASE 关系型数据库ACID规则 A(Atomicity) 原子性：在事务里的所有操作要么全部做完，要么都不做，事务成功的条件是事务里的所有操作都成功，只要有一个操作失败，整个事务就失败，需要回滚。 C (Consistency) 一致性：数据库要一直处于一致的状态，事务的运行不会改变数据库原本的一致性约束。 I (Isolation) 独立性：并发的事务之间不会互相影响，如果一个事务要访问的数据正在被另外一个事务修改，只要另外一个事务未提交，它所访问的数据就不受未提交事务的影响。 D (Durability) 持久性：事务提交后，它所做的修改将会永久的保存在数据库上，即使出现宕机也不会丢失。 非关系型数据库BASE思想 BASE：Basically Available, Soft-state, Eventually Consistent，是NoSQL数据库通常对可用性及一致性的弱要求原则: Basically Availble –基本可用 Soft-state –软状态/柔性事务。 “Soft state” 可以理解为”无连接”的, 而 “Hard state” 是”面向连接”的 Eventual Consistency – 最终一致性， 也是是 ACID 的最终目的。 ACID BASE 原子性(Atomicity) 基本可用(Basically Available) 一致性(Consistency) 软状态/柔性事务(Soft state) 隔离性(Isolation) 最终一致性 (Eventual consistency) 持久性 (Durable) &nbsp; NoSQL数据库分类 类型 部分代表 特点 列存储 Hbase、Cassandra、Hypertable 顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。 文档存储 MongoDB、CouchDB 文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有有机会对某些字段建立索引，实现关系数据库的某些功能。 key-value存储 Tokyo Cabinet / Tyrant、Berkeley DB、MemcacheDB、Redis 可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能） 图存储 Neo4J、FlockDB 图形关系的最佳存储(社交网络图、广告推荐系统)。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。 对象存储 db4o、Versant 通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。 xml数据库 Berkeley DB XML、BaseX 高效的存储XML数据，并支持XML的内部查询语法，比如XQuery,Xpath。 附：架构大致演变历程1. 单机MySQL时代 在90年代，一个网站的访问量一般都不大，用单个数据库完全可以轻松应付。在那个时候，更多的都是静态网页，动态交互类型的网站不多。单机MySQL 网站架构 上述架构下数据存储的瓶颈是什么？ 1.一个机器能存储数据量的总大小有限 2.数据库的索引与数据在同一个数据库，随着索引增加，效率反而降低 3.读写混合(在同一数据库实例)，并发有限 2. Memcached(缓存)+MySQL+垂直拆分 随着访问量的上升，几乎大部分使用MySQL架构的网站在数据库上都开始出现了性能问题，web程序不再仅仅专注在功能上，同时也在追求性能。程序员们开始大量的使用缓存技术来缓解数据库的压力，优化数据库的结构和索引。开始比较流行的是通过文件缓存来缓解数据库压力，但是当访问量继续增大的时候，多台web机器通过文件缓存不能共享，大量的小文件缓存也带了了比较高的IO压力。在这个时候，Memcached就自然的成为一个非常时尚的技术产品。Memcached+MySQL+垂直拆分 网站架构 Memcached 作为一个独立的分布式的缓存服务器，为多个web服务器提供了一个共享的、高性能缓存服务，在Memcached服务器上，又发展了根据hash算法来进行多台Memcached缓存服务的扩展，然后又出现了一致性hash来解决增加或减少缓存服务器导致重新hash带来的大量缓存失效的弊端。 3. MySQL主从读写分离 由于数据库的写入压力增加，Memcached只能缓解数据库的读取压力。读写集中在一个数据库上让数据库不堪重负，大部分网站开始使用主从复制技术来达到读写分离，以提高读写性能和读库的可扩展性。Mysql的master-slave模式成为这个时候的网站标配了。MySQL主从读写分离 网站架构 4. 分表分库+水平拆分+MySQL集群 在Memcached的高速缓存，MySQL的主从复制，读写分离的基础之上，这时MySQL主库的写压力开始出现瓶颈，而数据量的持续猛增，由于MyISAM使用表锁，在高并发下会出现严重的锁问题，大量的高并发MySQL应用开始使用InnoDB(行锁)引擎代替MyISAM。 同时，开始流行使用分表分库来缓解写压力和数据增长的扩展问题。这个时候，分表分库成了一个热门技术，也就在这个时候，MySQL推出了还不太稳定的表分区。虽然MySQL推出了MySQL&nbsp;Cluster集群，但性能也不能很好满足互联网的要求，只是在高可靠性上提供了非常大的保证。分表分库+水平拆分+MySQL集群 网站架构 5. MySQL的扩展性瓶颈 MySQL数据库也经常存储一些大文本字段，导致数据库表非常的大，在做数据库恢复的时候就导致非常的慢，不容易快速恢复数据库。比如1000万4KB大小的文本就接近40GB的大小，如果能把这些数据从MySQL省去，MySQL将变得非常的小。关系数据库很强大，但是它并不能很好的应付所有的应用场景。MySQL的扩展性差（需要复杂的技术来实现），大数据下IO压力大，表结构更改困难，正是当前使用MySQL的开发人员面临的问题。 6. 今天是什么样子 致谢： 尚硅谷-周阳老师的视频教程 NoSQL 简介 | 菜鸟教程]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>NoSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven简记]]></title>
    <url>%2F2019%2F06%2F27%2FMaven%E7%AE%80%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Maven基础知识学习笔记 构建相关命令需要在pom.xml所在目录执行 mvn clean 清理 mvn compile 编译主程序 mvn test-compile 编译测试程序 mvn test 执行测试 mvn package 打包 mvn install 项目安装到仓库 mvn site 生成站点 mvn deploy 自动发布(需要单独的配置) 坐标(定位项目) groupid: 一般是公司域名倒叙+项目名称 artifactid: 模块名 version: 版本 仓库 本地仓库:当前电脑上仓库目录，为本地所有maven工程服务 远程仓库 私服(Nexus): 搭建在局域网中，为局域网所有maven工程服务。可以将中央仓库中的项目下载下来并保存下来 中央仓库:为所有maven工程服务仓库中的内容 maven自身需要的插件 第三方框架或工具的jar包 自己开发的maven工程 生命周期 特点：每个生命周期都是从最开始开始执行，执行到相应命令结尾处 default lifeCircle(大致流程顺序) 处理资源文件 compile test package install deploy clean lifeCircle pre-clean: 执行在清理前完成的工作 clean: 移除上一次构建生成的文件 post-clean: 执行一些需要在clean之后立刻完成的工作 site lifeCircle: pre-site: 执行在生成站点文档之前的工作 clean: 生成项目的站点文档 post-site: 执行一些需要在生成站点文档之后完成的工作，为部署做准备 site-deploy: 将生成的站点文档部署到特定的服务器上 设置默认jdk版本进入maven/conf/目录，修改settings.xml文件的&lt;profile&gt;标签 123456789101112&lt;profile&gt; &lt;id&gt;jdk-1.8&lt;/id&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;jdk&gt;1.8&lt;/jdk&gt; &lt;/activation&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt; &lt;maven.compiler.compilerVersion&gt;1.8&lt;/maven.compiler.compilerVersion&gt; &lt;/properties&gt;&lt;/profile&gt; 依赖 对自己的maven工程执行mvn install，将项目放入本地仓库中 依赖范围(scope) compile(默认) 对主程序是否有效: YES 对测试程序是否有效: YES 是否参与打包: YES 部署时是否有效: YES test: 对测试程序有效 对主程序是否有效: NO 对测试程序是否有效: YES 是否参与打包: NO 部署时是否有效: YES 例子：junit provided(web工程使用的多): 对主程序是否有效: YES 对测试程序是否有效: YES 开发时是否有效: YES 部署时是否有效: NO 运行时是否有效: NO 例子：servlet-api.jar、jsp-api.jar Tips：如果在jsp翻译过来的java代码中报空指针异常，可能是jsp-api.jar的依赖范围问题。由compile修改为provided即可。 依赖的传递性 只有compile范围的依赖可以传递 jar包版本冲突时，maven默认使用依赖路径最短的版本 依赖路径相同时，maven采用先申明优先原则。(dependency书写的顺序) 依赖的排除Tips: 对当前项目和依赖此项目的项目生效 123456 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;&lt;groupId&gt; &lt;artifactId&gt;&lt;/artifactId&gt; &lt;/exclusion&gt;&lt;/exclusions&gt; 继承父工程中统一指定依赖的版本，子工程中不指定此依赖的版本，就会自动使用父工程的版本。Tips: 配置继承关系后，执行安装命令时要先安装父工程 操作步骤： 创建一个maven工程。打包方式为pom => pom 子工程中声明对父工程的引用(&lt;project节点下&gt;) 12345678&lt;!--子工程中声明父工程--&gt;&lt;parent&gt; &lt;groupId&gt;&lt;/groupId&gt; &lt;artifactId&gt;&lt;artifactId&gt; &lt;version&gt;&lt;/version&gt; &lt;!--以当前pom.xml为基准的父工程pom.xml文件的相对路径(可选)--&gt; &lt;relativePath&gt;&lt;/relativePath&gt;&lt;/parent&gt; 将子工程中坐标与父工程坐标重复的内容删除(&lt;project节点下&gt;) 12&lt;groupId&gt;&lt;/groupId&gt;&lt;version&gt;&lt;/version&gt; 在父工程中统一依赖管理(&lt;project节点下&gt;) 12345&lt;dependencyManagement&gt; &lt;dependencies&gt; .... &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在子工程中删除统一管理依赖的版本号(非必须，尽量统一) 聚合 作用: 一键安装模块工程 配置聚合 一般是配置在父工程中，也可以单独新建一个maven工程来配置1234&lt;modules&gt; &lt;module&gt;以当前pom.xml为基准的其他项目的相对路径&lt;/module&gt; &lt;module&gt;../MyMavenProject&lt;/module&gt;&lt;/modules&gt; 使用: 在配置了聚合的pom.xml的路径下执行mvn install即可 web工程的自动部署 配置(&lt;project节点下&gt;) 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!--配置当前工程构建过程中的特殊设置--&gt;&lt;build&gt; &lt;!--war包名称--&gt; &lt;finalName&gt;AtguiguWeb&lt;/finalName&gt; &lt;!--配置构建过程中需要使用的插件--&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;!--专门启动servlet容器的插件--&gt; &lt;groupId&gt;org.codehaus.cargo&lt;/groupId&gt; &lt;artifactId&gt;cargo-maven2-plugin&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt; &lt;!--针对这个插件的配置--&gt; &lt;configuration&gt; &lt;!--配置当前系统中容器的位置--&gt; &lt;container&gt; &lt;containerId&gt;tomcat6x&lt;/containerId&gt; &lt;home&gt;D:\DevInstall\apache-tomcat-6.0.39&lt;/home&gt; &lt;/container&gt; &lt;configuration&gt; &lt;!--确认容器位置--&gt; &lt;type&gt;existing&lt;/type&gt; &lt;home&gt;D:\DevInstall\apache-tomcat-6.0.39&lt;/home&gt; &lt;!-- 如果Tomcat端口为默认值8080则不必设置该属性 --&gt; &lt;properties&gt; &lt;cargo.servlet.port&gt;8989&lt;/cargo.servlet.port&gt; &lt;/properties&gt; &lt;/configuration&gt; &lt;/configuration&gt; &lt;!--配置插件在什么情况下执行--&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;cargo-run&lt;/id&gt; &lt;!--生命周期的阶段--&gt; &lt;phase&gt;install&lt;/phase&gt; &lt;goals&gt; &lt;!--插件的目标(执行的命令)--&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 使用: 进入pom.xml所在目录，执行mvn deploy即可]]></content>
      <categories>
        <category>构建工具</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA 读取Excel(支持xlsx、xls格式， 支持合并单元格)]]></title>
    <url>%2F2019%2F05%2F17%2FJAVA-%E8%AF%BB%E5%8F%96Excel-%E6%94%AF%E6%8C%81xlsx%E3%80%81xls%E6%A0%BC%E5%BC%8F%EF%BC%8C-%E6%94%AF%E6%8C%81%E5%90%88%E5%B9%B6%E5%8D%95%E5%85%83%E6%A0%BC%2F</url>
    <content type="text"><![CDATA[记录一下最近使用java操作excel的代码。 记录一下最近使用java操作excel的代码。 导包。注意两者版本需要一致 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi&lt;/artifactId&gt; &lt;version&gt;3.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.poi&lt;/groupId&gt; &lt;artifactId&gt;poi-ooxml&lt;/artifactId&gt; &lt;version&gt;3.9&lt;/version&gt;&lt;/dependency&gt; 解析excel代码 123456789101112131415161718192021222324252627282930313233343536373839public class ExcelDemo &#123; public static void main(String[] args) &#123; // 获取输入流 InputStream in = getResourcesFileInputStream("Test.xlsx"); //InputStream in = getResourcesFileInputStream("Test.xls"); // 解析---1 // 解析Excel所有sheet，所有行,所有列 List&lt;List&lt;String[]&gt;&gt; allData = ExcelUtil.read(in); // 解析---2 // 解析第一个sheet数据 ExcelUtil.readSheet(in, 0, sheet -&gt; &#123; //解析第3行到最后一行，第2列到第9列的数据 List&lt;String[]&gt; source = ExcelUtil.readSheetPart(sheet, 2, sheet.getLastRowNum(), 1, 8); &#125;); // 解析---3 // 完全自定义解析 ExcelUtil.readByCustomize(in, wb -&gt; &#123; // doSomeThing()...可参照ExcelUtil.readExcel()方法 &#125;); in.close(); &#125; /** * 加载Resources目录下的文件 * @param fileName 文件名 * @return */ public static InputStream getResourcesFileInputStream(String fileName)&#123; // Resources目录的绝对路径 // String path = Thread.currentThread().getContextClassLoader().getResource("").getPath(); InputStream in = Thread.currentThread().getContextClassLoader().getResourceAsStream("" + fileName); return in; &#125;&#125; 注意事项：获取的excel行数不正确解决方法 Excel工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366import org.apache.commons.lang3.StringUtils;import org.apache.poi.ss.usermodel.Cell;import org.apache.poi.ss.usermodel.WorkbookFactory;import org.apache.poi.openxml4j.exceptions.InvalidFormatException;import org.apache.poi.ss.usermodel.*;import org.apache.poi.ss.util.CellRangeAddress;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.io.InputStream;import java.util.*;/** * Excel工具类 */public class ExcelUtil &#123; private static Logger logger = LoggerFactory.getLogger(ExcelUtil.class); /** * 对excel单个sheet进行处理的接口 */ public interface SheetHandler &#123; /** * 对sheet进行处理 * @param sheet */ void handle(Sheet sheet); &#125; /** * 对excel进行处理的接口 */ public interface WorkBookHandler &#123; /** * 对excel进行处理 * @param wb * @return */ void handle(Workbook wb); &#125; /** * 读取excel指定sheet数据&lt;br&gt;未关闭io * @param in */ public static void readSheet(InputStream in, int sheetIdx, SheetHandler handler) throws IOException, InvalidFormatException &#123; // 创建工作簿 Workbook wb = createWorkBook(in); // 接口回调处理 handler.handle(getSheetByIdx(wb, sheetIdx)); &#125; /** * 读取excel全部sheet全部行数据&lt;br&gt;未关闭io * @param in * @return List&lt;List&lt;String[]&gt;&gt; &lt;br&gt;List&lt;String[]&gt;表示第i个sheet&lt;br&gt; String[]表示sheet某行 */ public static List&lt;List&lt;String[]&gt;&gt; read(InputStream in) throws IOException, InvalidFormatException &#123; // 创建工作簿 Workbook wb = createWorkBook(in); // 默认读取所有行，所有列 return readExcel(wb); &#125; /** * 读取excel&lt;br&gt;自定义解析方式&lt;br&gt;未关闭io * @param in * @return */ public static void readByCustomize(InputStream in, WorkBookHandler handler) throws IOException, InvalidFormatException &#123; // 创建工作簿 Workbook wb = createWorkBook(in); handler.handle(wb); &#125; /** * 读取指定sheet的范围行，范围列 * @param sheet excel 某个sheet * @param startRow 开始行下标 * @param endRow 结束行下标 * @param startCol 开始列下标 * @param endCol 结束列下标 * @return 指定范围的数据。一个String[]是一行数据 */ public static List&lt;String[]&gt; readSheetPart(Sheet sheet, int startRow, int endRow, int startCol, int endCol)&#123; if(sheet == null)&#123; logger.warn("excel sheet is not exist"); return null; &#125; int firstRow = sheet.getFirstRowNum(); int lastRow = sheet.getLastRowNum(); logger.info("sheet firstRow:"+firstRow); logger.info("sheet lastRow:"+lastRow); if(startRow &lt; 0 || startRow &gt; lastRow || startRow &gt; endRow)&#123; return null; &#125; if(endRow &gt; lastRow)&#123; return null; &#125; List&lt;String[]&gt; list = new ArrayList&lt;&gt;(); // 逐行解析 for (int i = startRow; i &lt;= endRow ; i++) &#123; Row row = sheet.getRow(i); // 过滤空行 if(row == null)&#123; continue; &#125; int firstCol = row.getFirstCellNum(); int lastCol = row.getLastCellNum(); logger.info("row("+i+") firstCol:"+firstCol); logger.info("row("+i+") lastCol:"+lastCol); if(startCol &lt; 0 || startCol &gt; lastCol || startCol &gt; endCol)&#123; return null; &#125; if(endCol &gt; lastCol)&#123; return null; &#125; int colIdx = 0; int objIdx = 0; String[] objs = new String[endCol-startCol+1]; // 逐列解析 for (Cell c : row) &#123; // 读取指定列数据 if(colIdx &gt;= startCol &amp;&amp; colIdx &lt;= endCol)&#123; c.setCellType(Cell.CELL_TYPE_STRING); boolean isMerge = isMergedRegion(sheet, i, c.getColumnIndex()); String data = ""; //判断是否具有合并单元格 if(isMerge) &#123; data = getMergedRegionValue(sheet, row.getRowNum(), c.getColumnIndex()); &#125;else &#123; data = c.getRichStringCellValue().toString(); &#125; objs[objIdx++]=data; &#125; colIdx++; &#125; list.add(objs); &#125; if(list.size() == 0)&#123; logger.warn("excel sheet data is space"); &#125; return list; &#125; /** * 读取excel文件 * @param wb * @return List&lt;List&lt;String[]&gt;&gt; &lt;br&gt;List&lt;String[]&gt;表示第i个sheet&lt;br&gt; String[]表示sheet某行 */ public static List&lt;List&lt;String[]&gt;&gt; readExcel(Workbook wb) &#123; // 总sheet数 int sheetNum = wb.getNumberOfSheets(); logger.info("excel sheetNum is :"+sheetNum); // 遍历sheet List&lt;List&lt;String[]&gt;&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; sheetNum ; i++) &#123; Sheet sheet = wb.getSheetAt(i); List&lt;String[]&gt; sheetData = readOneSheet(sheet); list.addAll(Collections.singleton(sheetData)); &#125; return list; &#125; /** * 读取单个sheet数据，封装为List&lt;String[]&gt;&lt;br&gt; * @warn 可能会有全是空串的数据 * @return List&lt;String[]&gt;,一个String[]是 */ public static List&lt;String[]&gt; readOneSheet(Sheet sheet)&#123; int startRow = sheet.getFirstRowNum(); int endRow = sheet.getLastRowNum(); logger.info("sheet startRow:"+startRow); logger.info("sheet endRow:"+endRow); List&lt;String[]&gt; list = new ArrayList&lt;&gt;(); // 遍历行 for (int i = startRow; i &lt;= endRow ; i++) &#123; Row row = sheet.getRow(i); if(row == null)&#123; continue; &#125; // int startColIdx = row.getFirstCellNum(); int endColIdx = row.getLastCellNum(); String[] objs = new String[endColIdx+1]; int colIdx = 0; // 遍历列 for (Cell c : row) &#123; c.setCellType(Cell.CELL_TYPE_STRING); String data = ""; boolean isMerge = isMergedRegion(sheet, i, c.getColumnIndex()); //判断是否具有合并单元格 if(isMerge) &#123; data = getMergedRegionValue(sheet, row.getRowNum(), c.getColumnIndex()); &#125;else &#123; data = c.getRichStringCellValue().toString(); &#125; objs[colIdx]=data; colIdx++; &#125; list.add(objs); &#125; if(list.size() == 0)&#123; logger.warn("excel sheet data is space"); &#125; return list; &#125; /** * 获取合并单元格的值&lt;br&gt; * 即获取合并单元格第一个cell的值 * @param sheet * @param row * @param column * @return */ public static String getMergedRegionValue(Sheet sheet ,int row , int column)&#123; // 获得一个 sheet 中合并单元格的数量 int sheetMergeCount = sheet.getNumMergedRegions(); // 遍历合并单元格 for(int i = 0 ; i &lt; sheetMergeCount ; i++)&#123; // 得出具体的合并单元格 CellRangeAddress ca = sheet.getMergedRegion(i); // 得到合并单元格的起始行, 结束行, 起始列, 结束列 int firstColumn = ca.getFirstColumn(); int lastColumn = ca.getLastColumn(); int firstRow = ca.getFirstRow(); int lastRow = ca.getLastRow(); // 获取合并单元格第一个cell的值 if(row &gt;= firstRow &amp;&amp; row &lt;= lastRow)&#123; if(column &gt;= firstColumn &amp;&amp; column &lt;= lastColumn)&#123; Row fRow = sheet.getRow(firstRow); Cell fCell = fRow.getCell(firstColumn); return getCellValue(fCell) ; &#125; &#125; &#125; return null ; &#125; /** * 判断指定的单元格是否是合并单元格 * @param sheet * @param row 行下标 * @param column 列下标 * @return */ public static boolean isMergedRegion(Sheet sheet,int row ,int column) &#123; // 得到一个sheet中有多少个合并单元格 int sheetMergeCount = sheet.getNumMergedRegions(); for (int i = 0; i &lt; sheetMergeCount; i++) &#123; // 得出具体的合并单元格 CellRangeAddress range = sheet.getMergedRegion(i); // 得到合并单元格的起始行, 结束行, 起始列, 结束列 int firstColumn = range.getFirstColumn(); int lastColumn = range.getLastColumn(); int firstRow = range.getFirstRow(); int lastRow = range.getLastRow(); // 判断该单元格是否在合并单元格范围之内, 如果是, 则返回 true if(row &gt;= firstRow &amp;&amp; row &lt;= lastRow)&#123; if(column &gt;= firstColumn &amp;&amp; column &lt;= lastColumn)&#123; return true; &#125; &#125; &#125; return false; &#125; /** * 判断sheet页中是否含有合并单元格 * @param sheet * @return */ public static boolean hasMerged(Sheet sheet) &#123; return sheet.getNumMergedRegions() &gt; 0 ? true : false; &#125; /** * 获取单元格的值 * @param cell * @return */ public static String getCellValue(Cell cell)&#123; if(cell == null) return ""; switch (cell.getCellType())&#123; case Cell.CELL_TYPE_STRING: return cell.getStringCellValue(); case Cell.CELL_TYPE_BOOLEAN: return String.valueOf(cell.getBooleanCellValue()); case Cell.CELL_TYPE_FORMULA: return cell.getCellFormula() ; case Cell.CELL_TYPE_NUMERIC: return String.valueOf(cell.getNumericCellValue()); default: return ""; &#125; &#125; /** * 创建工作簿 * @param in */ private static Workbook createWorkBook(InputStream in) throws IOException, InvalidFormatException &#123; Workbook wb = WorkbookFactory.create(in); return wb; &#125; /** * 获取sheet * @param wb * @param sheetIdx */ public static Sheet getSheetByIdx(Workbook wb, int sheetIdx)&#123; // 总sheet数 int sheetNum = wb.getNumberOfSheets(); // sheet不存在 if(sheetIdx &lt; 0 || sheetIdx &gt;= sheetNum)&#123; return null; &#125; // 获取sheet Sheet sheet = wb.getSheetAt(sheetIdx); return sheet; &#125; /*----------写excel相关----------*/ /** * 添加合并单元格 * @param sheet * @param firstRow 开始行 * @param lastRow 结束行 * @param firstCol 开始列 * @param lastCol 结束列 */ public static void mergeRegion(Sheet sheet, int firstRow, int lastRow, int firstCol, int lastCol) &#123; sheet.addMergedRegion(new CellRangeAddress(firstRow, lastRow, firstCol, lastCol)); &#125;&#125;]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQLdb批量执行sql, TypeError: not all arguments converted during string formatting]]></title>
    <url>%2F2019%2F05%2F05%2FMySQLdb%E6%89%B9%E9%87%8F%E6%89%A7%E8%A1%8Csql-TypeError-not-all-arguments-converted-during-string-formatting%2F</url>
    <content type="text"><![CDATA[使用Python MySQLdb批量执行sql时，总是报错TypeError: not all arguments converted during string formatting 错误代码12345678910111213141516171819202122232425import MySQLdbif __name__ == '__main__': dbItem=&#123;&#125; dbItem['host']='127.0.0.1' dbItem['port']=3306 dbItem['user']='root' dbItem['password']='123456' dbItem['database']='test' dbItem['charset']='utf8' dbItem['use_unicode']='False' conn = MySQLdb.connect(host=dbItem.get('host'), port=dbItem.get('port', 3306), user=dbItem.get('user'), passwd=dbItem.get('password'), db=dbItem.get('database'), charset=dbItem.get('charset'), use_unicode=dbItem.get('use_unicode')) cursor = conn.cursor() # 初始数据，第一个值为表名的一部分 list = [('classify', 130, 'classify1'),('content',14,'markdownContent')] insertsql = '''insert into atc_%s(id,data) values(%s, %s)''' "---这里报错---" cursor.executemany(insertsql, tuple(list)) conn.close() 错误信息TypeError: not all arguments converted during string formatting 查看executemany方法源码12345678910111213141516171819202122232425262728293031def executemany(self, query, args): del self.messages[:] db = self._get_db() if not args: return charset = db.character_set_name() if isinstance(query, unicode): query = query.encode(charset)"---！！重点就在这里---" m = insert_values.search(query) if not m: r = 0 for a in args: r = r + self.execute(query, a) return r p = m.start(1) e = m.end(1) qv = m.group(1) try: q = [ qv % db.literal(a) for a in args ] except TypeError, msg: if msg.args[0] in ("not enough arguments for format string", "not all arguments converted"): self.errorhandler(self, ProgrammingError, msg.args[0]) else: self.errorhandler(self, TypeError, msg) except: exc, value, tb = sys.exc_info() del tb self.errorhandler(self, exc, value) r = self._query('\n'.join([query[:p], ',\n'.join(q), query[e:]])) if not self._defer_warnings: self._warning_check() return r 找到关键点所在，m = insert_values.search(query) 1234567891011import rerestr = (r"\svalues\s*" r"(\(((?&lt;!\\)'[^\)]*?\)[^\)]*(?&lt;!\\)?'" r"|[^\(\)]|" r"(?:\([^\)]*\))" r")+\))")# 全部变量insert_values= re.compile(restr)...# executemany方法中m = insert_values.search(query) 这段代码的意思就是：截取出sql语句中values之后的字符串。我上面的代码截取到的就应该是:values(%s, %s)以上代码有疑惑的同学：传送门1&emsp;传送门2 解决方法问题出在哪里显而易见了，executemany方法中设置参数时没有截取到values之前的字段，导致参数个数与%s个数对不上。 将表名和要插入的数据封装成字典，再进行批量插入即可。修改后的代码： 1234567891011121314151617181920212223242526272829import MySQLdbif __name__ == '__main__': dbItem=&#123;&#125; ... conn = MySQLdb.connect(host=dbItem.get('host'), port=dbItem.get('port', 3306), user=dbItem.get('user'), passwd=dbItem.get('password'), db=dbItem.get('database'), charset=dbItem.get('charset'), use_unicode=dbItem.get('use_unicode')) cursor = conn.cursor() # 初始数据，第一个值为表名的一部分 list = [('classify', 130, 'classify1'),('content',14,'markdownContent')] # 封装成字典数据结构 value_dic = &#123;&#125; for item in list: valList = [] tbName = item[0] if value_dic.has_key(tbName): valList = value_dic.get(tbName) valList.append(item[1:]) value_dic[tbName] = valList # 分数据表批量执行 for tbName,valList in value_dic.items(): insertsql = '''insert into atc_&#123;tbName&#125;(id,data) values(%s, %s)'''.format(tbName=tbName) cursor.executemany(insertsql, tuple(valList)) conn.close()]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>批量执行SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL5.6 date类型排序，分页丢失数据问题]]></title>
    <url>%2F2019%2F04%2F21%2FMySQL5-6-date%E7%B1%BB%E5%9E%8B%E6%8E%92%E5%BA%8F%EF%BC%8C%E5%88%86%E9%A1%B5%E4%B8%A2%E5%A4%B1%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[MySQL 分页机制bug，导致丢数据问题 直接用例子说话，查询的是第2页的数据，每页20条。注意选中的数据。查询第3页的数据，很明显同一条数据出现了两次。那么丢数据是怎么回事呢？ 再来看看2019-03-20日的具体数据，注意选中的数据。很明显，在图一和图二中找不到该条数据。从而导致我们莫名其妙的就丢了数据了。 那么导致问题的原因是什么呢？这锅就应该MySQL背了，明显是MySQL分页机制的bug。后来，看了一下出问题的MySQL的版本：5.6.20-log。接着在5.7.16版本上测试了，没有该问题。 一般在生产环境中，版本升级不是那么容易的，那么在不能升级版本的情况下又该怎么解决这个问题呢？我在排序条件上新增了一列,ORDER BY stat_date DESC,cls3 desc,成功解决分页丢数据问题。]]></content>
      <categories>
        <category>数据库</category>
        <category>踩坑</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】深入理解Java中的String]]></title>
    <url>%2F2019%2F03%2F11%2F%E3%80%90%E8%BD%AC%E8%BD%BD%E3%80%91%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E4%B8%AD%E7%9A%84String%2F</url>
    <content type="text"><![CDATA[个人所见最齐全最透彻的关于JAVA String的总结。 个人所见最齐全最透彻的关于JAVA String的总结。原文：https://www.cnblogs.com/xiaoxi/p/6036701.html 深入理解Java中的String一、String类想要了解一个类，最好的办法就是看这个类的实现源代码，来看一下String类的源码： 1234567891011121314151617181920public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence&#123; /** The value is used for character storage. */ private final char value[]; /** The offset is the first index of the storage that is used. */ private final int offset; /** The count is the number of characters in the String. */ private final int count; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; ........&#125; 从上面可以看出几点： 1）String类是final类，也即意味着String类不能被继承，并且它的成员方法都默认为final方法。在Java中，被final修饰的类是不允许被继承的，并且该类中的成员方法都默认为final方法。 2）上面列举出了String类中所有的成员属性，从上面可以看出String类其实是通过char数组来保存字符串的。 下面再继续看String类的一些方法实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public String substring(int beginIndex, int endIndex) &#123; if (beginIndex &lt; 0) &#123; throw new StringIndexOutOfBoundsException(beginIndex); &#125; if (endIndex &gt; count) &#123; throw new StringIndexOutOfBoundsException(endIndex); &#125; if (beginIndex &gt; endIndex) &#123; throw new StringIndexOutOfBoundsException(endIndex - beginIndex); &#125; return ((beginIndex == 0) &amp;&amp; (endIndex == count)) ? this : new String(offset + beginIndex, endIndex - beginIndex, value);&#125;public String concat(String str) &#123; int otherLen = str.length(); if (otherLen == 0) &#123; return this; &#125; char buf[] = new char[count + otherLen]; getChars(0, count, buf, 0); str.getChars(0, otherLen, buf, count); return new String(0, count + otherLen, buf);&#125;public String replace(char oldChar, char newChar) &#123; if (oldChar != newChar) &#123; int len = count; int i = -1; char[] val = value; /* avoid getfield opcode */ int off = offset; /* avoid getfield opcode */ while (++i &lt; len) &#123; if (val[off + i] == oldChar) &#123; break; &#125; &#125; if (i &lt; len) &#123; char buf[] = new char[len]; for (int j = 0 ; j &lt; i ; j++) &#123; buf[j] = val[off+j]; &#125; while (i &lt; len) &#123; char c = val[off + i]; buf[i] = (c == oldChar) ? newChar : c; i++; &#125; return new String(0, len, buf); &#125; &#125; return this;&#125; 从上面的三个方法可以看出，无论是sub操、concat还是replace操作都不是在原有的字符串上进行的，而是重新生成了一个新的字符串对象。也就是说进行这些操作后，最原始的字符串并没有被改变。 在这里要永远记住一点：“String对象一旦被创建就是固定不变的了，对String对象的任何改变都不影响到原对象，相关的任何change操作都会生成新的对象”。 二、字符串常量池​ 我们知道字符串的分配和其他对象分配一样，是需要消耗高昂的时间和空间的，而且字符串我们使用的非常多。JVM为了提高性能和减少内存的开销，在实例化字符串的时候进行了一些优化：使用字符串常量池。每当我们创建字符串常量时，JVM会首先检查字符串常量池，如果该字符串已经存在常量池中，那么就直接返回常量池中的实例引用。如果字符串不存在常量池中，就会实例化该字符串并且将其放到常量池中。由于String字符串的不可变性我们可以十分肯定常量池中一定不存在两个相同的字符串（这点对理解上面至关重要）。 Java中的常量池，实际上分为两种形态：静态常量池和运行时常量池。所谓静态常量池，即.class文件中的常量池，class文件中的常量池不仅仅包含字符串(数字)字面量，还包含类、方法的信息，占用class文件绝大部分空间。而*运行时常量池**，则是jvm虚拟机在完成类装载操作后，将class文件中的常量池载入到内存中，并保存在方法区中，我们常说的常量池，就是指方法区中的运行时常量池。 来看下面的程序： 12String a = &quot;chenssy&quot;;String b = &quot;chenssy&quot;; a、b和字面上的chenssy都是指向JVM字符串常量池中的”chenssy”对象，他们指向同一个对象。 1String c = new String(&quot;chenssy&quot;); new关键字一定会产生一个对象chenssy（注意这个chenssy和上面的chenssy不同），同时这个对象是存储在堆中。所以上面应该产生了两个对象：保存在栈中的c和保存堆中chenssy。但是在Java中根本就不存在两个完全一模一样的字符串对象。故堆中的chenssy应该是引用字符串常量池中chenssy。所以c、chenssy、池chenssy的关系应该是：c—&gt;chenssy—&gt;池chenssy。整个关系如下： 通过上面的图我们可以非常清晰的认识他们之间的关系。所以我们修改内存中的值，他变化的是所有。 总结：虽然a、b、c、chenssy是不同的对象，但是从String的内部结构我们是可以理解上面的。String c = new String(“chenssy”);虽然c的内容是创建在堆中，但是他的内部value还是指向JVM常量池的chenssy的value，它构造chenssy时所用的参数依然是chenssy字符串常量。 下面再来看几个例子： 例子1： 123456789/** * 采用字面值的方式赋值 */public void test1()&#123; String str1="aaa"; String str2="aaa"; System.out.println("===========test1============"); System.out.println(str1==str2);//true 可以看出str1跟str2是指向同一个对象 &#125; 执行上述代码，结果为：true。分析：当执行String str1=”aaa”时，JVM首先会去字符串池中查找是否存在”aaa”这个对象，如果不存在，则在字符串池中创建”aaa”这个对象，然后将池中”aaa”这个对象的引用地址返回给字符串常量str1，这样str1会指向池中”aaa”这个字符串对象；如果存在，则不创建任何对象，直接将池中”aaa”这个对象的地址返回，赋给字符串常量。当创建字符串对象str2时，字符串池中已经存在”aaa”这个对象，直接把对象”aaa”的引用地址返回给str2，这样str2指向了池中”aaa”这个对象，也就是说str1和str2指向了同一个对象，因此语句System.out.println(str1 == str2)输出：true。 例子2： 123456789/** * 采用new关键字新建一个字符串对象 */public void test2()&#123; String str3=new String("aaa"); String str4=new String("aaa"); System.out.println("===========test2============"); System.out.println(str3==str4);//false 可以看出用new的方式是生成不同的对象 &#125; 执行上述代码，结果为：false。 分析： 采用new关键字新建一个字符串对象时，JVM首先在字符串池中查找有没有”aaa”这个字符串对象，如果有，则不在池中再去创建”aaa”这个对象了，直接在堆中创建一个”aaa”字符串对象，然后将堆中的这个”aaa”对象的地址返回赋给引用str3，这样，str3就指向了堆中创建的这个”aaa”字符串对象；如果没有，则首先在字符串池中创建一个”aaa”字符串对象，然后再在堆中创建一个”aaa”字符串对象，然后将堆中这个”aaa”字符串对象的地址返回赋给str3引用，这样，str3指向了堆中创建的这个”aaa”字符串对象。当执行String str4=new String(“aaa”)时， 因为采用new关键字创建对象时，每次new出来的都是一个新的对象，也即是说引用str3和str4指向的是两个不同的对象，因此语句System.out.println(str3 == str4)输出：false。 例子3： 1234567891011/** * 编译期确定 */public void test3()&#123; String s0="helloworld"; String s1="helloworld"; String s2="hello"+"world"; System.out.println("===========test3============"); System.out.println(s0==s1); //true 可以看出s0跟s1是指向同一个对象 System.out.println(s0==s2); //true 可以看出s0跟s2是指向同一个对象 &#125; 执行上述代码，结果为：true、true。 分析：因为例子中的s0和s1中的”helloworld”都是字符串常量，它们在编译期就被确定了，所以s0==s1为true；而”hello”和”world”也都是字符串常量，当一个字符串由多个字符串常量连接而成时，它自己肯定也是字符串常量，所以s2也同样在编译期就被解析为一个字符串常量，所以s2也是常量池中”helloworld”的一个引用。所以我们得出s0==s1==s2。 例子4： 123456789101112/** * 编译期无法确定 */public void test4()&#123; String s0="helloworld"; String s1=new String("helloworld"); String s2="hello" + new String("world"); System.out.println("===========test4============"); System.out.println( s0==s1 ); //false System.out.println( s0==s2 ); //false System.out.println( s1==s2 ); //false&#125; 执行上述代码，结果为：false、false、false。 分析：用new String() 创建的字符串不是常量，不能在编译期就确定，所以new String() 创建的字符串不放入常量池中，它们有自己的地址空间。 s0还是常量池中”helloworld”的引用，s1因为无法在编译期确定，所以是运行时创建的新对象”helloworld”的引用，s2因为有后半部分new String(”world”)所以也无法在编译期确定，所以也是一个新创建对象”helloworld”的引用。 例子5： 12345678910/** * 继续-编译期无法确定 */public void test5()&#123; String str1="abc"; String str2="def"; String str3=str1+str2; System.out.println("===========test5============"); System.out.println(str3=="abcdef"); //false&#125; 执行上述代码，结果为：false。 分析：因为str3指向堆中的”abcdef”对象，而”abcdef”是字符串池中的对象，所以结果为false。JVM对String str=”abc”对象放在常量池中是在编译时做的，而String str3=str1+str2是在运行时刻才能知道的。new对象也是在运行时才做的。而这段代码总共创建了5个对象，字符串池中两个、堆中三个。+运算符会在堆中建立来两个String对象，这两个对象的值分别是”abc”和”def”，也就是说从字符串池中复制这两个值，然后在堆中创建两个对象，然后再建立对象str3,然后将”abcdef”的堆地址赋给str3。 步骤：1)栈中开辟一块中间存放引用str1，str1指向池中String常量”abc”。2)栈中开辟一块中间存放引用str2，str2指向池中String常量”def”。3)栈中开辟一块中间存放引用str3。4)str1 + str2通过StringBuilder的最后一步toString()方法还原一个新的String对象”abcdef”，因此堆中开辟一块空间存放此对象。5)引用str3指向堆中(str1 + str2)所还原的新String对象。6)str3指向的对象在堆中，而常量”abcdef”在池中，输出为false。 例子6： 123456789101112131415/** * 编译期优化 */public void test6()&#123; String s0 = "a1"; String s1 = "a" + 1; System.out.println("===========test6============"); System.out.println((s0 == s1)); //result = true String s2 = "atrue"; String s3= "a" + "true"; System.out.println((s2 == s3)); //result = true String s4 = "a3.4"; String s5 = "a" + 3.4; System.out.println((s4 == s5)); //result = true&#125; 执行上述代码，结果为：true、true、true。 分析：在程序编译期，JVM就将常量字符串的”+”连接优化为连接后的值，拿”a” + 1来说，经编译器优化后在class中就已经是a1。在编译期其字符串常量的值就确定下来，故上面程序最终的结果都为true。 例子7： 12345678910/** * 编译期无法确定 */public void test7()&#123; String s0 = "ab"; String s1 = "b"; String s2 = "a" + s1; System.out.println("===========test7============"); System.out.println((s0 == s2)); //result = false&#125; 执行上述代码，结果为：false。 分析：JVM对于字符串引用，由于在字符串的”+”连接中，有字符串引用存在，而引用的值在程序编译期是无法确定的，即”a” + s1无法被编译器优化，只有在程序运行期来动态分配并将连接后的新地址赋给s2。所以上面程序的结果也就为false。 例子8： 123456789101112/** * 比较字符串常量的“+”和字符串引用的“+”的区别 */public void test8()&#123; String test="javalanguagespecification"; String str="java"; String str1="language"; String str2="specification"; System.out.println("===========test8============"); System.out.println(test == "java" + "language" + "specification"); System.out.println(test == str + str1 + str2);&#125; 执行上述代码，结果为：true、false。 分析：为什么出现上面的结果呢？这是因为，字符串字面量拼接操作是在Java编译器编译期间就执行了，也就是说编译器编译时，直接把”java”、”language”和”specification”这三个字面量进行”+”操作得到一个”javalanguagespecification” 常量，并且直接将这个常量放入字符串池中，这样做实际上是一种优化，将3个字面量合成一个，避免了创建多余的字符串对象。而字符串引用的”+”运算是在Java运行期间执行的，即str + str2 + str3在程序执行期间才会进行计算，它会在堆内存中重新创建一个拼接后的字符串对象。总结来说就是：字面量”+”拼接是在编译期间进行的，拼接后的字符串存放在字符串池中；而字符串引用的”+”拼接运算实在运行时进行的，新创建的字符串存放在堆中。 对于直接相加字符串，效率很高，因为在编译器便确定了它的值，也就是说形如”I”+”love”+”java”; 的字符串相加，在编译期间便被优化成了”Ilovejava”。对于间接相加（即包含字符串引用），形如s1+s2+s3; 效率要比直接相加低，因为在编译器不会对引用变量进行优化。 例子9： 12345678910/** * 编译期确定 */public void test9()&#123; String s0 = "ab"; final String s1 = "b"; String s2 = "a" + s1; System.out.println("===========test9============"); System.out.println((s0 == s2)); //result = true&#125; 执行上述代码，结果为：true。 分析：和例子7中唯一不同的是s1字符串加了final修饰，对于final修饰的变量，它在编译时被解析为常量值的一个本地拷贝存储到自己的常量池中或嵌入到它的字节码流中。所以此时的”a” + s1和”a” + “b”效果是一样的。故上面程序的结果为true。 例子10： 123456789101112131415/** * 编译期无法确定 */public void test10()&#123; String s0 = "ab"; final String s1 = getS1(); String s2 = "a" + s1; System.out.println("===========test10============"); System.out.println((s0 == s2)); //result = false &#125;private static String getS1() &#123; return "b"; &#125; 执行上述代码，结果为：false。 分析：这里面虽然将s1用final修饰了，但是由于其赋值是通过方法调用返回的，那么它的值只能在运行期间确定，因此s0和s2指向的不是同一个对象，故上面程序的结果为false。 三、总结1.String类初始化后是不可变的(immutable) String使用private final char value[]来实现字符串的存储，也就是说String对象创建之后，就不能再修改此对象中存储的字符串内容，就是因为如此，才说String类型是不可变的(immutable)。程序员不能对已有的不可变对象进行修改。我们自己也可以创建不可变对象，只要在接口中不提供修改数据的方法就可以。然而，String类对象确实有编辑字符串的功能，比如replace()。这些编辑功能是通过创建一个新的对象来实现的，而不是对原有对象进行修改。比如: 1s = s.replace("World", "Universe"); 上面对s.replace()的调用将创建一个新的字符串”Hello Universe!”，并返回该对象的引用。通过赋值，引用s将指向该新的字符串。如果没有其他引用指向原有字符串”Hello World!”，原字符串对象将被垃圾回收。 2.引用变量与对象 A aa;这个语句声明一个类A的引用变量aa[我们常常称之为句柄]，而对象一般通过new创建。所以aa仅仅是一个引用变量，它不是对象。 3.创建字符串的方式 创建字符串的方式归纳起来有两类： （1）使用””引号创建字符串 （2）使用new关键字创建字符串 结合上面例子，总结如下: （1）单独使用””引号创建的字符串都是常量,编译期就已经确定存储到String Pool中； （2）使用new String(“”)创建的对象会存储到heap中,是运行期新创建的； new创建字符串时首先查看池中是否有相同值的字符串，如果有，则拷贝一份到堆中，然后返回堆中的地址；如果池中没有，则在堆中创建一份，然后返回堆中的地址（注意，此时不需要从堆中复制到池中，否则，将使得堆中的字符串永远是池中的子集，导致浪费池的空间）！ （3）使用只包含常量的字符串连接符如”aa” + “aa”创建的也是常量,编译期就能确定,已经确定存储到String Pool中； （4）使用包含变量的字符串连接符如”aa” + s1创建的对象是运行期才创建的,存储在heap中； 4.使用String不一定创建对象 在执行到双引号包含字符串的语句时，如String a = “123”，JVM会先到常量池里查找，如果有的话返回常量池里的这个实例的引用，否则的话创建一个新实例并置入常量池里。所以，当我们在使用诸如String str = “abc”；的格式定义对象时，总是想当然地认为，创建了String类的对象str。担心陷阱！对象可能并没有被创建！而可能只是指向一个先前已经创建的对象。只有通过new()方法才能保证每次都创建一个新的对象。 5.使用new String，一定创建对象 在执行String a = new String(“123”)的时候，首先走常量池的路线取到一个实例的引用，然后在堆上创建一个新的String实例，走以下构造函数给value属性赋值，然后把实例引用赋值给a： 12345678910111213141516171819public String(String original) &#123; int size = original.count; char[] originalValue = original.value; char[] v; if (originalValue.length &gt; size) &#123; // The array representing the String is bigger than the new // String itself. Perhaps this constructor is being called // in order to trim the baggage, so make a copy of the array. int off = original.offset; v = Arrays.copyOfRange(originalValue, off, off+size); &#125; else &#123; // The array representing the String is the same // size as the String, so no point in making a copy. v = originalValue; &#125; this.offset = 0; this.count = size; this.value = v; &#125; 从中我们可以看到，虽然是新创建了一个String的实例，但是value是等于常量池中的实例的value，即是说没有new一个新的字符数组来存放”123”。 6.关于String.intern() intern方法使用：一个初始为空的字符串池，它由类String独自维护。当调用 intern方法时，如果池已经包含一个等于此String对象的字符串（用equals(oject)方法确定），则返回池中的字符串。否则，将此String对象添加到池中，并返回此String对象的引用。 它遵循以下规则：对于任意两个字符串 s 和 t，当且仅当 s.equals(t) 为 true 时，s.intern() == t.intern() 才为 true。 String.intern();再补充介绍一点：存在于.class文件中的常量池，在运行期间被jvm装载，并且可以扩充。String的intern()方法就是扩充常量池的一个方法；当一个String实例str调用intern()方法时，java查找常量池中是否有相同unicode的字符串常量，如果有，则返回其引用，如果没有，则在常量池中增加一个unicode等于str的字符串并返回它的引用。 12345678910111213141516/** * 关于String.intern() */public void test11()&#123; String s0 = "kvill"; String s1 = new String("kvill"); String s2 = new String("kvill"); System.out.println("===========test11============"); System.out.println( s0 == s1 ); //false System.out.println( "**********" ); s1.intern(); //虽然执行了s1.intern(),但它的返回值没有赋给s1 s2 = s2.intern(); //把常量池中"kvill"的引用赋给s2 System.out.println( s0 == s1); //flase System.out.println( s0 == s1.intern() ); //true//说明s1.intern()返回的是常量池中"kvill"的引用 System.out.println( s0 == s2 ); //true&#125; 运行结果：false、false、true、true。 7.关于equals和== （1）对于==，如果作用于基本数据类型的变量（byte,short,char,int,long,float,double,boolean ），则直接比较其存储的”值”是否相等；如果作用于引用类型的变量（String），则比较的是所指向的对象的地址（即是否指向同一个对象）。 （2）equals方法是基类Object中的方法，因此对于所有的继承于Object的类都会有该方法。在Object类中，equals方法是用来比较两个对象的引用是否相等，即是否指向同一个对象。 （3）对于equals方法，注意：equals方法不能作用于基本数据类型的变量。如果没有对equals方法进行重写，则比较的是引用类型的变量所指向的对象的地址；而String类对equals方法进行了重写，用来比较指向的字符串对象所存储的字符串是否相等。其他的一些类诸如Double，Date，Integer等，都对equals方法进行了重写用来比较指向的对象所存储的内容是否相等。 12345678910111213/** * 关于equals和== */public void test12()&#123; String s1="hello"; String s2="hello"; String s3=new String("hello"); System.out.println("===========test12============"); System.out.println( s1 == s2); //true,表示s1和s2指向同一对象，它们都指向常量池中的"hello"对象 //flase,表示s1和s3的地址不同，即它们分别指向的是不同的对象,s1指向常量池中的地址，s3指向堆中的地址 System.out.println( s1 == s3); System.out.println( s1.equals(s3)); //true,表示s1和s3所指向对象的内容相同&#125; 8.String相关的+： String中的 + 常用于字符串的连接。看下面一个简单的例子： 12345678910/** * String相关的+ */public void test13()&#123; String a = "aa"; String b = "bb"; String c = "xx" + "yy " + a + "zz" + "mm" + b; System.out.println("===========test13============"); System.out.println(c);&#125; 编译运行后，主要字节码部分如下： 1234567891011121314151617181920212223242526272829303132333435363738394041public static main([Ljava/lang/String;)V L0 LINENUMBER 5 L0 LDC "aa" ASTORE 1 L1 LINENUMBER 6 L1 LDC "bb" ASTORE 2 L2 LINENUMBER 7 L2 NEW java/lang/StringBuilder DUP LDC "xxyy " INVOKESPECIAL java/lang/StringBuilder.&lt;init&gt; (Ljava/lang/String;)V ALOAD 1 INVOKEVIRTUAL java/lang/StringBuilder.append (Ljava/lang/String;)Ljava/lang/StringBuilder; LDC "zz" INVOKEVIRTUAL java/lang/StringBuilder.append (Ljava/lang/String;)Ljava/lang/StringBuilder; LDC "mm" INVOKEVIRTUAL java/lang/StringBuilder.append (Ljava/lang/String;)Ljava/lang/StringBuilder; ALOAD 2 INVOKEVIRTUAL java/lang/StringBuilder.append (Ljava/lang/String;)Ljava/lang/StringBuilder; INVOKEVIRTUAL java/lang/StringBuilder.toString ()Ljava/lang/String; ASTORE 3 L3 LINENUMBER 8 L3 GETSTATIC java/lang/System.out : Ljava/io/PrintStream; ALOAD 3 INVOKEVIRTUAL java/io/PrintStream.println (Ljava/lang/String;)V L4 LINENUMBER 9 L4 RETURN L5 LOCALVARIABLE args [Ljava/lang/String; L0 L5 0 LOCALVARIABLE a Ljava/lang/String; L1 L5 1 LOCALVARIABLE b Ljava/lang/String; L2 L5 2 LOCALVARIABLE c Ljava/lang/String; L3 L5 3 MAXSTACK = 3 MAXLOCALS = 4&#125; 显然，通过字节码我们可以得出如下几点结论：(1).String中使用 + 字符串连接符进行字符串连接时，连接操作最开始时如果都是字符串常量，编译后将尽可能多的直接将字符串常量连接起来，形成新的字符串常量参与后续连接（通过反编译工具jd-gui也可以方便的直接看出）； (2).接下来的字符串连接是从左向右依次进行，对于不同的字符串，首先以最左边的字符串为参数创建StringBuilder对象，然后依次对右边进行append操作，最后将StringBuilder对象通过toString()方法转换成String对象（注意：中间的多个字符串常量不会自动拼接）。 也就是说String c = “xx” + “yy “ + a + “zz” + “mm” + b; 实质上的实现过程是： String c = new StringBuilder(“xxyy “).append(a).append(“zz”).append(“mm”).append(b).toString(); 由此得出结论：当使用+进行多个字符串连接时，实际上是产生了一个StringBuilder对象和一个String对象。 9.String的不可变性导致字符串变量使用+号的代价： 12345String s = "a" + "b" + "c"; String s1 = "a"; String s2 = "b"; String s3 = "c"; String s4 = s1 + s2 + s3; 分析：变量s的创建等价于 String s = “abc”; 由上面例子可知编译器进行了优化，这里只创建了一个对象。由上面的例子也可以知道s4不能在编译期进行优化，其对象创建相当于： 123StringBuilder temp = new StringBuilder(); temp.append(a).append(b).append(c); String s = temp.toString(); 由上面的分析结果，可就不难推断出String 采用连接运算符（+）效率低下原因分析，形如这样的代码： 12345678public class Test &#123; public static void main(String args[]) &#123; String s = null; for(int i = 0; i &lt; 100; i++) &#123; s += "a"; &#125; &#125;&#125; 每做一次 + 就产生个StringBuilder对象，然后append后就扔掉。下次循环再到达时重新产生个StringBuilder对象，然后 append 字符串，如此循环直至结束。 如果我们直接采用 StringBuilder 对象进行 append 的话，我们可以节省 N - 1 次创建和销毁对象的时间。所以对于在循环中要进行字符串连接的应用，一般都是用StringBuffer或StringBulider对象来进行append操作。 10.String、StringBuffer、StringBuilder的区别 （1）可变与不可变：String是不可变字符串对象，StringBuilder和StringBuffer是可变字符串对象（其内部的字符数组长度可变）。 （2）是否多线程安全：String中的对象是不可变的，也就可以理解为常量，显然线程安全。StringBuffer 与 StringBuilder 中的方法和功能完全是等价的，只是StringBuffer 中的方法大都采用了synchronized 关键字进行修饰，因此是线程安全的，而 StringBuilder 没有这个修饰，可以被认为是非线程安全的。 （3）String、StringBuilder、StringBuffer三者的执行效率：StringBuilder &gt; StringBuffer &gt; String 当然这个是相对的，不一定在所有情况下都是这样。比如String str = “hello”+ “world”的效率就比 StringBuilder st = new StringBuilder().append(“hello”).append(“world”)要高。因此，这三个类是各有利弊，应当根据不同的情况来进行选择使用：当字符串相加操作或者改动较少的情况下，建议使用 String str=”hello”这种形式；当字符串相加操作较多的情况下，建议使用StringBuilder，如果采用了多线程，则使用StringBuffer。 11.String中的final用法和理解 123456final StringBuffer a = new StringBuffer("111");final StringBuffer b = new StringBuffer("222");a=b;//此句编译不通过final StringBuffer a = new StringBuffer("111");a.append("222");//编译通过 可见，final只对引用的”值”(即内存地址)有效，它迫使引用只能指向初始指向的那个对象，改变它的指向会导致编译期错误。至于它所指向的对象的变化，final是不负责的。 12.关于String str = new String(“abc”)创建了多少个对象？ 这个问题在很多书籍上都有说到比如《Java程序员面试宝典》，包括很多国内大公司笔试面试题都会遇到，大部分网上流传的以及一些面试书籍上都说是2个对象，这种说法是片面的。 首先必须弄清楚创建对象的含义，创建是什么时候创建的？这段代码在运行期间会创建2个对象么？毫无疑问不可能，用javap -c反编译即可得到JVM执行的字节码内容： 很显然，new只调用了一次，也就是说只创建了一个对象。而这道题目让人混淆的地方就是这里，这段代码在运行期间确实只创建了一个对象，即在堆上创建了”abc”对象。而为什么大家都在说是2个对象呢，这里面要澄清一个概念，该段代码执行过程和类的加载过程是有区别的。在类加载的过程中，确实在运行时常量池中创建了一个”abc”对象，而在代码执行过程中确实只创建了一个String对象。因此，这个问题如果换成 String str = new String(“abc”)涉及到几个String对象？合理的解释是2个。个人觉得在面试的时候如果遇到这个问题，可以向面试官询问清楚”是这段代码执行过程中创建了多少个对象还是涉及到多少个对象“再根据具体的来进行回答。 13.字符串池的优缺点：字符串池的优点就是避免了相同内容的字符串的创建，节省了内存，省去了创建相同字符串的时间，同时提升了性能；另一方面，字符串池的缺点就是牺牲了JVM在常量池中遍历对象所需要的时间，不过其时间成本相比而言比较低。 四、综合实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.spring.test;public class StringTest &#123; public static void main(String[] args) &#123; /** * 情景一：字符串池 * JAVA虚拟机(JVM)中存在着一个字符串池，其中保存着很多String对象; * 并且可以被共享使用，因此它提高了效率。 * 由于String类是final的，它的值一经创建就不可改变。 * 字符串池由String类维护，我们可以调用intern()方法来访问字符串池。 */ String s1 = "abc"; //↑ 在字符串池创建了一个对象 String s2 = "abc"; //↑ 字符串pool已经存在对象“abc”(共享),所以创建0个对象，累计创建一个对象 System.out.println("s1 == s2 : "+(s1==s2)); //↑ true 指向同一个对象， System.out.println("s1.equals(s2) : " + (s1.equals(s2))); //↑ true 值相等 //↑------------------------------------------------------over /** * 情景二：关于new String("") * */ String s3 = new String("abc"); //↑ 创建了两个对象，一个存放在字符串池中，一个存在与堆区中； //↑ 还有一个对象引用s3存放在栈中 String s4 = new String("abc"); //↑ 字符串池中已经存在“abc”对象，所以只在堆中创建了一个对象 System.out.println("s3 == s4 : "+(s3==s4)); //↑false s3和s4栈区的地址不同，指向堆区的不同地址； System.out.println("s3.equals(s4) : "+(s3.equals(s4))); //↑true s3和s4的值相同 System.out.println("s1 == s3 : "+(s1==s3)); //↑false 存放的地区多不同，一个栈区，一个堆区 System.out.println("s1.equals(s3) : "+(s1.equals(s3))); //↑true 值相同 //↑------------------------------------------------------over /** * 情景三： * 由于常量的值在编译的时候就被确定(优化)了。 * 在这里，"ab"和"cd"都是常量，因此变量str3的值在编译时就可以确定。 * 这行代码编译后的效果等同于： String str3 = "abcd"; */ String str1 = "ab" + "cd"; //1个对象 String str11 = "abcd"; System.out.println("str1 = str11 : "+ (str1 == str11)); //↑------------------------------------------------------over /** * 情景四： * 局部变量str2,str3存储的是存储两个拘留字符串对象(intern字符串对象)的地址。 * * 第三行代码原理(str2+str3)： * 运行期JVM首先会在堆中创建一个StringBuilder类， * 同时用str2指向的拘留字符串对象完成初始化， * 然后调用append方法完成对str3所指向的拘留字符串的合并， * 接着调用StringBuilder的toString()方法在堆中创建一个String对象， * 最后将刚生成的String对象的堆地址存放在局部变量str3中。 * * 而str5存储的是字符串池中"abcd"所对应的拘留字符串对象的地址。 * str4与str5地址当然不一样了。 * * 内存中实际上有五个字符串对象： * 三个拘留字符串对象、一个String对象和一个StringBuilder对象。 */ String str2 = "ab"; //1个对象 String str3 = "cd"; //1个对象 String str4 = str2+str3; String str5 = "abcd"; System.out.println("str4 = str5 : " + (str4==str5)); // false //↑------------------------------------------------------over /** * 情景五： * JAVA编译器对string + 基本类型/常量 是当成常量表达式直接求值来优化的。 * 运行期的两个string相加，会产生新的对象的，存储在堆(heap)中 */ String str6 = "b"; String str7 = "a" + str6; String str67 = "ab"; System.out.println("str7 = str67 : "+ (str7 == str67)); //↑str6为变量，在运行期才会被解析。 final String str8 = "b"; String str9 = "a" + str8; String str89 = "ab"; System.out.println("str9 = str89 : "+ (str9 == str89)); //↑str8为常量变量，编译期会被优化 //↑------------------------------------------------------over &#125;&#125; 运行结果： 运行结果： s1 == s2 : trues1.equals(s2) : trues3 == s4 : falses3.equals(s4) : trues1 == s3 : falses1.equals(s3) : truestr1 = str11 : truestr4 = str5 : falsestr7 = str67 : falsestr9 = str89 : true]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>JAVA</tag>
        <tag>String</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven项目代码和jar包分开打包]]></title>
    <url>%2F2019%2F02%2F14%2FMaven%E9%A1%B9%E7%9B%AE%E4%BB%A3%E7%A0%81%E5%92%8Cjar%E5%8C%85%E5%88%86%E5%BC%80%E6%89%93%E5%8C%85%2F</url>
    <content type="text"><![CDATA[Java Maven工程代码和依赖包分开打包的配置.. 引言代码包和依赖包分开打包这样的好处是：如果只修改了逻辑代码，没有修改依赖，就只需要更新代码包即可。在pom.xml中下中做如下修改： 步骤1&lt;project&gt;节点中修改 1&lt;packaging&gt;war&lt;/packaging&gt;改成&lt;packaging&gt;jar&lt;/packaging&gt; 步骤2&lt;build&gt;下&lt;plugins&gt;节点中注释掉 1234&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;&lt;/plugin&gt; 步骤3&lt;build&gt;下&lt;plugins&gt;节点中添加如下配置： 1234567891011121314151617181920212223242526272829303132333435363738&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy-installed&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;artifactItems&gt; &lt;artifactItem&gt; &lt;groupId&gt;$&#123;project.groupId&#125;&lt;/groupId&gt; &lt;artifactId&gt;$&#123;project.artifactId&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;type&gt;$&#123;project.packaging&#125;&lt;/type&gt; &lt;/artifactItem&gt; &lt;/artifactItems&gt; &lt;outputDirectory&gt;target/lib&lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;copy-lib&lt;/id&gt; &lt;phase&gt;prepare-package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;outputDirectory&gt;$&#123;project.build.directory&#125;/lib&lt;/outputDirectory&gt; &lt;overWriteReleases&gt;false&lt;/overWriteReleases&gt; &lt;overWriteSnapshots&gt;false&lt;/overWriteSnapshots&gt; &lt;overWriteIfNewer&gt;true&lt;/overWriteIfNewer&gt; &lt;includeScope&gt;compile&lt;/includeScope&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 效果图 参考: https://www.cnblogs.com/lishan1/p/10317488.html]]></content>
      <categories>
        <category>构建工具</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot jar包作为Wrapper服务启动-Linux]]></title>
    <url>%2F2019%2F02%2F14%2FSpringBoot-jar%E5%8C%85%E4%BD%9C%E4%B8%BAWrapper%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-Linux%2F</url>
    <content type="text"><![CDATA[SpringBoot应用封装成Wrapper服务的教程-Linux 引言同步Windows版本教程参考：SpringBoot jar包作为Wrapper服务启动-Windows此教程为Linux上将springboot jar作为服务启动, 版本：SpringBoot-2.1.2RELEASE。该服务中web程序的context-path、port都会以springboot中配置的为准。 步骤 下载Java Service Wrapper。目前最新版本为：3.5.37。选择自己对应的操作系统和位数，一般是Linux+x86 cpu。 修改conf/wrapper.conf文件，最好先备份 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138#encoding=UTF-8&lt;font color=red&gt;##不知道为啥，这句去掉就不能启动服务&lt;/font&gt;#include ../conf/wrapper-license.conf#Specify the language and locale which the Wrapper should use.#需要取消注释，否则会去找wrapperjni_zh.mo文件，但是../lang/中没有该文件&lt;font color=red&gt;wrapper.lang=en_US # en_US or ja_JPwrapper.lang.folder=../lang&lt;/font&gt;#Java Applicationset.JAVA_HOME=/usr/local/java/jdk1.8.0_191wrapper.java.command=%JAVA_HOME%/bin/java#Java Library Path (location of Wrapper.DLL or libwrapper.so)wrapper.java.library.path.1=../lib#Tell the Wrapper to log the full generated Java command line.#wrapper.java.command.loglevel=INFO#老版本写法#wrapper.java.mainclass=org.tanukisoftware.wrapper.WrapperStartStopApp&lt;font color=red&gt;wrapper.java.mainclass=org.tanukisoftware.wrapper.WrapperSimpleApp&lt;/font&gt;#Java Classpath (include wrapper.jar) Add class path elements as needed starting from 1#自己的jar包路径&lt;font color=red&gt;wrapper.java.classpath.1=../lib/boot.jarwrapper.java.classpath.2=../lib/wrapper.jar&lt;/font&gt;#Java Bits. On applicable platforms, tells the JVM to run in 32 or 64-bit mode.wrapper.java.additional.auto_bits=TRUE#Java Additional Parameterswrapper.java.additional.1=#Initial Java Heap Size (in MB)#wrapper.java.initmemory=3#Maximum Java Heap Size (in MB)#wrapper.java.maxmemory=64#Application parameters. Add parameters as needed starting from 1&lt;font color=red&gt;wrapper.app.parameter.1=org.springframework.boot.loader.JarLauncher&lt;/font&gt;#如果代码包与依赖jar包分开打包也可以直接写main所在的入口类#wrapper.app.parameter.1=com.ysl.springboot.SpringbootApplication#Enables Debug output from the Wrapper.#wrapper.debug=TRUE#Format of output for the console. (See docs for formats)#wrapper.console.format=PMwrapper.console.format=LPDTM#Log Level for console output. (See docs for log levels)wrapper.console.loglevel=INFO#Log file to use for wrapper output logging.#wrapper.logfile=../logs/wrapper.log#wrapper 日志wrapper.logfile=../logs/boot_YYYYMMDD.logwrapper.logfile.rollmode=DATE#Format of output for the log file. (See docs for formats)#wrapper.logfile.format=LPTMwrapper.logfile.format=LPDTM#Log Level for log file output. (See docs for log levels)wrapper.logfile.loglevel=INFO#Maximum size that the log file will be allowed to grow to before#the log is rolled. Size is specified in bytes. The default value#of 0, disables log rolling. May abbreviate with the &apos;k&apos; (kb) or#&apos;m&apos; (mb) suffix. For example: 10m = 10 megabytes.wrapper.logfile.maxsize=0#Maximum number of rolled log files which will be allowed before old#files are deleted. The default value of 0 implies no limit.wrapper.logfile.maxfiles=0#Log Level for sys/event log output. (See docs for log levels)wrapper.syslog.loglevel=NONE#Allow for the use of non-contiguous numbered propertieswrapper.ignore_sequence_gaps=TRUE#Do not start if the pid file already exists.wrapper.pidfile.strict=TRUE#Title to use when running as a console &lt;u&gt;app.name in bin/testwrapper&lt;/u&gt;&lt;font color=red&gt;wrapper.console.title=@app.long.name@&lt;/font&gt;#********************************************************************#Wrapper JVM Checks#********************************************************************#Detect DeadLocked Threads in the JVM. (Requires Standard Edition)wrapper.check.deadlock=TRUEwrapper.check.deadlock.interval=10wrapper.check.deadlock.action=RESTARTwrapper.check.deadlock.output=FULL#Out Of Memory detection.#(Ignore output from dumping the configuration to the console. This is only needed by the TestWrapper sample application.)wrapper.filter.trigger.999=wrapper.filter.trigger.*java.lang.OutOfMemoryErrorwrapper.filter.allow_wildcards.999=TRUEwrapper.filter.action.999=NONE#Ignore -verbose:class output to avoid false positives.wrapper.filter.trigger.1000=[Loaded java.lang.OutOfMemoryErrorwrapper.filter.action.1000=NONE#(Simple match)wrapper.filter.trigger.1001=java.lang.OutOfMemoryError#(Only match text in stack traces if -XX:+PrintClassHistogram is being used.)#wrapper.filter.trigger.1001=Exception in thread &quot;*&quot; java.lang.OutOfMemoryError#wrapper.filter.allow_wildcards.1001=TRUEwrapper.filter.action.1001=RESTARTwrapper.filter.message.1001=The JVM has run out of memory.#Specify custom mail contentwrapper.event.jvm_restart.email.body=The JVM was restarted.\n\nPlease check on its status.\n#Name of the service. &lt;font color=red&gt;wrapper.name=@app.name@&lt;/font&gt;#Display name of the service &lt;u&gt;app.long.name in bin/testwrapper&lt;/u&gt;&lt;font color=red&gt;wrapper.displayname=@app.long.name@&lt;/font&gt;#Description of the service&lt;font color=red&gt;wrapper.description=Test Wrapper boot-Application Description&lt;/font&gt;#Service dependencies. Add dependencies as needed starting from 1wrapper.ntservice.dependency.1=#Mode in which the service is installed. AUTO_START, DELAY_START or DEMAND_STARTwrapper.ntservice.starttype=AUTO_START#Allow the service to interact with the desktop (Windows NT/2000/XP only).wrapper.ntservice.interactive=FALSEwrapper.ping.timeout=120 修改bin/testwrapper 在bin目录下执行启动服务：./testwrapper start停止服务：./testwrapper stop重启服务：./testwrapper restart 说在最后的话服务不是免费的 参考https://blog.csdn.net/myvernal/article/details/79104026https://my.oschina.net/u/3866531/blog/1845669/https://blog.csdn.net/sq287197314/article/details/82996012]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Wrapper服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot jar 作为Wrapper服务启动-Windows]]></title>
    <url>%2F2019%2F02%2F14%2FSpringBoot-jar-%E4%BD%9C%E4%B8%BAWrapper%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8-Windows%2F</url>
    <content type="text"><![CDATA[SpringBoot应用封装成Wrapper服务的教程-Windows 引言同步Linux版本教程参考：SpringBoot jar包作为Wrapper服务启动-Linux此教程为Windows上将SpringBoot jar作为服务启动, 版本：SpringBoot-2.1.2RELEASE。该服务中web程序的context-path、port都会以SpringBoot中配置的为准。 步骤 下载Java Service Wrapper。目前最新版本为：3.5.37。选择自己对应的操作系统和位数，一般是Windows+x86 cpu。 修改conf/wrapper.conf文件，最好先备份 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127#encoding=UTF-8##不知道为啥，这句去掉就不能启动服务#include ../conf/wrapper-license.conf#需要取消注释，否则会去找wrapperjni_zh.mo文件，但是../lang/中没有该文件&lt;font color=red&gt;wrapper.lang=en_US # en_US or ja_JPwrapper.lang.folder=../lang/&lt;/font&gt;#java相关路径set.JAVA_HOME=D:\jdks\jdk1.8_181wrapper.java.command=%JAVA_HOME%/bin/java#老版本写法#wrapper.java.mainclass=org.tanukisoftware.wrapper.WrapperStartStopApp&lt;font color=red&gt;wrapper.java.mainclass=org.tanukisoftware.wrapper.WrapperSimpleApp&lt;/font&gt;#自己的jar包路径&lt;font color=red&gt;wrapper.java.classpath.1=../lib/boot.jarwrapper.java.classpath.2=../lib/wrapper.jar&lt;/font&gt;#Java Library Path (location of Wrapper.DLL or libwrapper.so)wrapper.java.library.path.1=../lib#Java Bits. On applicable platforms, tells the JVM to run in 32 or 64-bit mode.wrapper.java.additional.auto_bits=TRUE#Java Additional Parameterswrapper.java.additional.1=#Initial Java Heap Size (in MB)#wrapper.java.initmemory=3#Maximum Java Heap Size (in MB)#wrapper.java.maxmemory=64#Application parameters. Add parameters as needed starting from 1#这里很重要，这是Spring boot启动的入口&lt;font color=red&gt;wrapper.app.parameter.1=org.springframework.boot.loader.JarLauncher&lt;/font&gt;#如果代码包与依赖jar包分开打包也可以直接写main所在的入口类#wrapper.app.parameter.1=com.ysl.springboot.SpringbootApplication#wrapper.console.format=PMwrapper.console.format=LPDTM#Log Level for console output. (See docs for log levels)wrapper.console.loglevel=INFO#Log file to use for wrapper output logging.#wrapper.logfile=../logs/wrapper.log#wrapper 日志wrapper.logfile=../logs/boot_YYYYMMDD.logwrapper.logfile.rollmode=DATE#Format of output for the log file. (See docs for formats)#wrapper.logfile.format=LPTMwrapper.logfile.format=LPDTM#Log Level for log file output. (See docs for log levels)wrapper.logfile.loglevel=INFO#Maximum size that the log file will be allowed to grow to before the log is rolled. Size is specified in bytes. #The default value of 0, disables log rolling. May abbreviate with the &apos;k&apos; (kb) or &apos;m&apos; (mb) suffix. For example: 10m = 10 megabytes.wrapper.logfile.maxsize=0#Maximum number of rolled log files which will be allowed before old files are deleted. The default value of 0 implies no limit.wrapper.logfile.maxfiles=0#Log Level for sys/event log output. (See docs for log levels)wrapper.syslog.loglevel=NONE#Allow for the use of non-contiguous numbered propertieswrapper.ignore_sequence_gaps=TRUE#Do not start if the pid file already exists.wrapper.pidfile.strict=TRUE#Title to use when running as a consolewrapper.console.title=Test boot-Application#Detect DeadLocked Threads in the JVM. (Requires Standard Edition)wrapper.check.deadlock=TRUEwrapper.check.deadlock.interval=10wrapper.check.deadlock.action=RESTARTwrapper.check.deadlock.output=FULL#Out Of Memory detection.#(Ignore output from dumping the configuration to the console. This is only needed by the TestWrapper sample application.)wrapper.filter.trigger.999=wrapper.filter.trigger.*java.lang.OutOfMemoryErrorwrapper.filter.allow_wildcards.999=TRUEwrapper.filter.action.999=NONE#Ignore -verbose:class output to avoid false positives.wrapper.filter.trigger.1000=[Loaded java.lang.OutOfMemoryErrorwrapper.filter.action.1000=NONE#(Simple match)wrapper.filter.trigger.1001=java.lang.OutOfMemoryError#(Only match text in stack traces if -XX:+PrintClassHistogram is being used.)#wrapper.filter.trigger.1001=Exception in thread &quot;*&quot; java.lang.OutOfMemoryError#wrapper.filter.allow_wildcards.1001=TRUEwrapper.filter.action.1001=RESTARTwrapper.filter.message.1001=The JVM has run out of memory.#Specify custom mail contentwrapper.event.jvm_restart.email.body=The JVM was restarted.\n\nPlease check on its status.\n#Name of the service#这里是你的服务的名字&lt;font color=red&gt;wrapper.name=wrapper-boot&lt;/font&gt;#Display name of the service#这里是你的服务显示的名称&lt;font color=red&gt;wrapper.displayname=boot-Application&lt;/font&gt;#Description of the service#这里是你的服务的描述&lt;font color=red&gt;wrapper.description=boot-Application Description&lt;/font&gt;#Service dependencies. Add dependencies as needed starting from 1wrapper.ntservice.dependency.1=#Mode in which the service is installed. AUTO_START, DELAY_START or DEMAND_STARTwrapper.ntservice.starttype=AUTO_START#Allow the service to interact with the desktop (Windows NT/2000/XP only).wrapper.ntservice.interactive=FALSEwrapper.ping.timeout=120 双击运行bin/InstallTestWrapper-NT.bat，然后ctrl+alt+del查看服务是否安装成功。找到自己的服务右键启动即可。 说在最后的话服务不是免费的 参考https://blog.csdn.net/myvernal/article/details/79104026https://my.oschina.net/u/3866531/blog/1845669/https://blog.csdn.net/sq287197314/article/details/82996012]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>Wrapper服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot在GlassFish上启动 ContainerBase.addChild: start: org.apache.catalina.LifecycleException]]></title>
    <url>%2F2019%2F02%2F13%2FSpringBoot%E5%9C%A8GlassFish%E4%B8%8A%E5%90%AF%E5%8A%A8-ContainerBase-addChild-start-org-apache-catalina-LifecycleException%2F</url>
    <content type="text"><![CDATA[SpringBoot应用不能再GlassFish4.x上启动的解决方法 遇到的问题在glassfish4.1.2版本中始终无法启动，报错： 12345678Error occurred during deployment: Exception while loading the app :java.lang.IllegalStateException: ContainerBase.addChild: start: org.apache.catalina.LifecycleException: org.apache.catalina.LifecycleException: org.springframework.context.ApplicationContextException: Unable to start web server; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'httpPutFormContentFilter' defined in class path resource [org/springframework/boot/autoconfigure/web/servlet/WebMvcAutoConfiguration.class]:Bean instantiation via factory method failed; nested exception is org.springframework.beans.BeanInstantiationException: Failed to instantiate [org.springframework.boot.web.servlet.filter.OrderedHttpPutFormContentFilter]: Factory method 'httpPutFormContentFilter' threw exception; nested exception is java.lang.VerifyError: Cannot inherit from final class. Please see server.log for more details. 测试了SpringBoot-2.0.5.RELEASE、SpringBoot-2.1.2.RELEASE都有此问题。 解决方法 GlassFish升级到GlassFish5.0就没问题了 将SpringBoot应用封装成Wrapper服务，再启动。参考：SpringBoot jar包作为Wrapper服务启动-LinuxSpringBoot jar包作为Wrapper服务启动-Windows]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
        <tag>GlassFish</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL中将一列以逗号分隔的值行转列]]></title>
    <url>%2F2019%2F01%2F26%2FMySQL%E4%B8%AD%E5%B0%86%E4%B8%80%E5%88%97%E4%BB%A5%E9%80%97%E5%8F%B7%E5%88%86%E9%9A%94%E7%9A%84%E5%80%BC%E8%A1%8C%E8%BD%AC%E5%88%97%2F</url>
    <content type="text"><![CDATA[MySQL行转列案例… 前言有时会遇到没有遵守第一范式设计模式的业务表。即一列中存储了多个属性值。如下表 pk value 1 ET,AT 2 AT,BT 3 AT,DT 4 DT,CT,AT 一般有这两种常见需求(测试数据见文末) 得到所有的不重复的值，如 value AT BT CT DT ET SQL如下 123456select distinct(substring_index(substring_index(a.col,',',b.help_topic_id+1),',',-1))from (select group_concat(distinct `value`) as col from `row_to_col`) as ajoin mysql.help_topic as bon b.help_topic_id &lt; (char_length(a.col) - char_length(replace(a.col,',',''))+1) 显示每个值及其对应主键，如 pk value 1 ET 1 AT 2 AT 2 BT 3 AT 3 DT 4 DT 4 CT 4 AT SQL如下 123456select a.pk,substring_index(substring_index(a.col,',',b.help_topic_id+1),',',-1)from (select `value` as col,pk from `row_to_col`) as ajoin mysql.help_topic as bon b.help_topic_id &lt; (char_length(a.col) - char_length(replace(a.col,',',''))+1) 实现思路 需求1 通过group_concat函数将value列的值拼接成一个逗号隔开的字符串，然后通过substring_index函数对字符串进行截取 通过substring_index函数特性，我们就需要知道字符串有多少个逗号，并且要告诉每个逗号的位置 逗号个数=char_length(字符串)-char_length(replace(字符串,',','')) 逗号位置=mysql.help_topic.id &lt; 逗号个数[+1] 最后通过distinct函数将截取后的单个值进行去重 注意 mysql.help_topic表的自增id是从0开始，所以在进行截取时要对id进行+1。见: substring_index(a.col,',',b.help_topic_id+1) value列最后一个字符不是逗号时：逗号个数+1是为了截取时不漏掉最后一个逗号后的值，即: char_length(a.col) - char_length(replace(a.col,',',''))+1value列最后一个字符是逗号时：逗号个数就不需要+1了，直接：char_length(a.col) - char_length(replace(a.col,',','')) 因为截取时id要+1，所以在连接时取的&lt; ，而不是 &lt;= 。见：b.help_topic_id &lt; (char_length(a.col) - char_length(replace(a.col,',',''))[+1]) mysql.help_topic(mysql version: 5.7.21-1)表的自增id，最大值为636。如果group_concat后的字符串中逗号个数大于该值，需要自己单独处理自增id的值 需求2：思路基本与需求1同，只是最后的查询不一样 涉及到的函数 length:返回字符串所占的字节数，是计算字段的长度。一个汉字或是中文符号是算三个字符,一个数字或字母或英文符号算一个字符。 char_length:返回字符串所占的字符数，不管汉字还是数字或者是字母或者符号(不分中英文)都算是一个字符。 replace(str,old_string,new_string): 将字符串中str中所有的old_string替换成new_string。 substring_index(被截取字段，关键字，关键字出现的次数)：截取字符串。如果关键字出现的次数是负数 ， 则是从后倒数，到字符串结束。 group_concat([DISTINCT] 要连接的字段(可多个,逗号隔开) [Order BY 排序字段 ASC/DESC] [Separator '分隔符'])：将group by产生的同一个分组中的值连接起来，返回一个字符串结果。分隔符，默认逗号。 测试数据1234567891011DROP TABLE IF EXISTS `row_to_col`;CREATE TABLE `row_to_col` ( `pk` int(11) NOT NULL AUTO_INCREMENT, `value` varchar(255) DEFAULT NULL, PRIMARY KEY (`pk`)) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;INSERT INTO `row_to_col` VALUES ('1', 'ET,AT');INSERT INTO `row_to_col` VALUES ('2', 'AT,BT');INSERT INTO `row_to_col` VALUES ('3', 'AT,DT');INSERT INTO `row_to_col` VALUES ('4', 'DT,CT,AT'); 参考：https://blog.csdn.net/liuzhoulong/article/details/51729168https://blog.csdn.net/ldl22847/article/details/47609727]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】Phoenix 异步建立二级索引失败，Hbase表名需大写]]></title>
    <url>%2F2019%2F01%2F25%2FPhoenix-%E5%BC%82%E6%AD%A5%E5%BB%BA%E7%AB%8B%E4%BA%8C%E7%BA%A7%E7%B4%A2%E5%BC%95%E5%A4%B1%E8%B4%A5%EF%BC%8CHbase%E8%A1%A8%E5%90%8D%E9%9C%80%E5%A4%A7%E5%86%99%2F</url>
    <content type="text"><![CDATA[Phoenix异步建立二级索引，必须要求Hbase表名需大写 环境：phoenix-5.0.0-Hbase-2.0.0关于怎么建立二级索引的原理及方法，我就不复述了，有需要的可以看看下面这些博客，挺好的转至：https://www.cnblogs.com/haoxinyue/p/6724365.html https://www.cnblogs.com/mario-nb/p/6350266.html?utm_source=itdadao&amp;utm_medium=referral 在这里我想强调的是，当我们的hbase表数据非常大时，测试大概5000万以上的数据建二级索引就会遇到连接超时的问题。 当然我们也可以调整连接时间如下：在客户端配置文件hbase-site.xml中，把超时参数设置大一些，足够build索引数据的时间。 12345678910111213141516171819202122232425262728&lt;property&gt;​&lt;name&gt;phoenix.query.timeoutMs&lt;/name&gt;&lt;value&gt;1200000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;​&lt;name&gt;phoenix.query.keepAliveMs&lt;/name&gt;&lt;value&gt;1200000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;​&lt;name&gt;hbase.rpc.timeout&lt;/name&gt;&lt;value&gt;1200000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;​&lt;name&gt;hbase.regionserver.lease.period&lt;/name&gt;&lt;value&gt;1200000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;​&lt;name&gt;hbase.client.operation.timeout&lt;/name&gt;&lt;value&gt;1200000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;​&lt;name&gt;hbase.client.scanner.caching&lt;/name&gt;&lt;value&gt;1000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;​&lt;name&gt;hbase.client.scanner.timeout.period&lt;/name&gt;&lt;value&gt;1200000&lt;/value&gt;&lt;/property&gt; 这种方式显然看起来就不太友好所以，我们就想到了用异步索引的方式去建立照着网上的方式:兴高采烈的去开干了，在linux输入以下命令： 123$&#123;HBASE_HOME&#125;/bin/hbase org.apache.phoenix.mapreduce.index.IndexTool --schema MY_SCHEMA --data-table MY_TABLE --index-table ASYNC_IDX --output-path ASYNC_IDX_HFILES 结果直接就报错了：它竟然说我的索引不是数表的索引，我当时就蒙了，这是咋回事呢？马上跑到phoenix client去查看，分明就是它的索引啊 ，可以通过命令查看 1select TABLE_NAME,DATA_TABLE_NAME,INDEX_TYPE,INDEX_STATE,INDEX_DISABLE_TIMESTAMP from system.catalog where INDEX_TYPE is not null; 那又是咋回事啊，想想是不是建索引出问题了，遂不信邪的再一次实验，继续出错，错误如出一辙，正当我准备放弃的时候。突然发现这好像有些不对，我的表分明是小写的t33，咋它说我的索引表却不是T33的index呢，是不是这个IndexTool有某种方式，将所有的表都转化为大写的呢，还是它只认识大写的表 接下来我就创建了一个大写的表去建索引，果然这次并没报错误，至于具体是哪种原因，没找到方法去研究。 结论：平时总喜欢建小写的hbase表，因为大写的看着别扭，这不自己把自己坑了几天，过程用同步索引都因为超时等各种原因，导致失败多次，让我哭会。。。。 大家又遇到这个问题么？ 原文链接：https://www.cnblogs.com/henyu/p/9179653.html]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Phoenix varchar 空串等于null的坑]]></title>
    <url>%2F2019%2F01%2F15%2FPhoenix-varchar-%E7%A9%BA%E4%B8%B2%E7%AD%89%E4%BA%8Enull%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[举例说明，Phoenix将空串(“”)处理成null的坑 场景描述 业务的部分数据来源于hive，部分数据来源于hbase。两边数据通过一些条件字段关联，可是始终有部分数据关联不上。 hive数据源的数据直接通过hive api进行读写，hbase数据源通过phoenix读写。而且写入hbase的数据最终来源于hive，没道理会关联不上。 之后查找了好久，终于发现问题所在。就在phoenix对空串(“”)的处理，或者说没做写入处理。 通过下面案例说明 测试(phoenix-5.0.0-hbase-2.0.0) 建表： 12345678create table "test"( rk varchar primary key, "cf1"."c1" varchar, "cf1"."c2" varchar, "cf1"."c3" unsigned_int, "cf1"."c4" unsigned_long, "cf1"."c5" unsigned_double); 测试数据 12upsert into "test" (rk,"cf1"."c1","cf1"."c3","cf1"."c4","cf1"."c5") values ('rk1','varchar',100,111,1.123);upsert into "test" (rk,"cf1"."c1") values ('rk2',''); 查询结果 总结 phoenix 没有对空串(“”)进行处理，所以结果仍为null。 phoenix 对varchar 类型的null显示为空白，对数值类型的null显示为null。]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hbase 启动失败，Please check the config value of 'hbase.procedure.store.wal.use.hsync']]></title>
    <url>%2F2019%2F01%2F08%2FHbase-%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5%EF%BC%8CPlease-check-the-config-value-of-hbase-procedure-store-wal-use-hsync%2F</url>
    <content type="text"><![CDATA[Hadoop+Zookeeper+Hbase环境搭建时遇到的坑… 环境：JDK1.8Hadoop 2.7.1Zookeeper 3.4.6Hbase 2.0.0Apache Phoenix-5.0.0-Hbase-2.0.0 从hadoop到hbase的环境搭建一切正常，可直接参照网上教程，这里不累述。 这里记录一下我在配置phoenix连接hbase的时候遇到的问题。直观的看是HMaster先能启动，过一会又自动挂掉。日志错误信息如下： 1234567891011ERROR [master/YSL-MASTER:16000] master.HMaster: Failed to become active masterjava.lang.IllegalStateException: The procedure WAL relies on the ability to hsync for proper operation during component failures, but the underlying filesystem does not support doing so. Please check the config value of 'hbase.procedure.store.wal.use.hsync' to set the desired level of robustness and ensure the config value of 'hbase.wal.dir' points to a FileSystem mount that can provide it. at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.rollWriter(WALProcedureStore.java:1043) at org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.recoverLease(WALProcedureStore.java:382) at org.apache.hadoop.hbase.procedure2.ProcedureExecutor.start(ProcedureExecutor.java:530) at org.apache.hadoop.hbase.master.HMaster.startProcedureExecutor(HMaster.java:1222) at org.apache.hadoop.hbase.master.HMaster.startServiceThreads(HMaster.java:1141) at org.apache.hadoop.hbase.master.HMaster.finishActiveMasterInitialization(HMaster.java:849) at org.apache.hadoop.hbase.master.HMaster.startActiveMasterManager(HMaster.java:2019) at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:553) at java.lang.Thread.run(Thread.java:748) 官网说明：http://hbase.apache.org/book.html#trouble.master.startup.hsync。 看了之后还是不知道怎么解决。 自己尝试了很多方法，换hadoop版本、修改hbase配置，替换hbase/lib目录下hadoop相关jar包(先删除原jar包)，这里记录一下替换命令：find /usr/local/hadoop-2.7.1/share/hadoop -name "hadoop*jar" | xargs -i cp {} /usr/local/hbase-2.0.0/lib/ 。网上也有说换hbase版本(未尝试)。网上最多的方法就是： 123456789101112131415hbase/conf/hbase-site.xml增加配置 &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt; Controls whether HBase will check for stream capabilities (hflush/hsync). Disable this if you intend to run on LocalFileSystem, denoted by a rootdir with the 'file://' scheme, but be mindful of the NOTE below. WARNING: Setting this to false blinds you to potential data loss and inconsistent system state in the event of process and/or node failures. If HBase is complaining of an inability to use hsync or hflush it's most likely not a false positive. &lt;/description&gt;&lt;/property&gt;该配置的作用是：如果你打算在本地文件系统中跑hbase，请禁掉此项。而我这里是将hbase跑在hdfs文件系统上，所以不能配置此项。 我的解决方法：删除每个节点hbase/lib目录下的phoenix-××-client.jar包，重启hbase即可。具体原因我也还没搞清楚，以后再更新。附上：hadoop-hbase 版本对应关系 记录下hbase/conf/hbase-site.xml中添加的配置： 123456789101112131415161718192021222324252627282930&lt;property&gt; &lt;name&gt;phoenix.schema.isNamespaceMappingEnabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;开启schema与namespace的对应关系&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;phoenix.schema.mapSystemTablesToNamespace&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;开启schema与namespace的对应关系&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;phoenix.functions.allowUserDefinedFunctions&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;允许用户自定义函数(UDF)&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.wal.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.IndexedWALEditCodec&lt;/value&gt; &lt;description&gt;开启phoenix 二级索引&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.regionserver.executor.openregion.threads&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;description&gt;打开用户表region的线程数量(默认3)&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;phoenix.default.column.encoded.bytes.attrib&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;description&gt;不对hbase列进行编码存储&lt;/description&gt;&lt;/property&gt;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Hbase</tag>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下 创建tomcat启动关闭快捷方式]]></title>
    <url>%2F2018%2F12%2F24%2FLinux%E4%B8%8B-%E5%88%9B%E5%BB%BAtomcat%E5%90%AF%E5%8A%A8%E5%85%B3%E9%97%AD%E5%BF%AB%E6%8D%B7%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Linux下创建Tomcat快捷方式 原文：https://blog.csdn.net/rankun1/article/details/52816942 安装参考：https://blog.csdn.net/sun8112133/article/details/79944531 在linux下搭建好tomcat之后，每次启动和关闭都要去tomcat的bin目录下执行./startup.sh和./shutdown.sh 这是很不方便的，下面介绍如何像执行ls mv cp等命令一样启动，关闭，重启tomcat 创建脚本脚本创建路径 /etc/init.d/tomcat首先执行: vi /etc/init.d/tomcat按 i 编辑，将下面内容根据你自己的情况（修改/opt/java/tomcat8为你的tomcat路径）修改后copy到编辑框中， esc 退出编辑，:wq 保存退出 12345678910111213141516171819202122232425262728# !/bin/bash # Description: start or stop the tomcat # Usage: tomcat [start|stop|restart] # export PATH=$PATH:$HOME/binexport BASH_ENV=$HOME/.bashrcexport USERNAME=&quot;root&quot;case &quot;$1&quot; instart)#startup the tomcat cd /opt/java/tomcat8/bin./startup.sh;;stop)# stop tomcat cd /opt/java/tomcat8/bin./shutdown.shecho &quot;Tomcat Stoped&quot;;;restart)$0 stop$0 start;;*)echo &quot;tomcat: usage: tomcat [start|stop|restart]&quot;exit 1esacexit 0 脚本添加执行权限,执行命令 chmod +x /etc/init.d/tomcat创建软连接,执行下面两条命令 12cd /usr/bin ln -s /etc/init.d/tomcat . 测试好了，用下面的命令愉快的去玩耍吧 123tomcat starttomcat stoptomcat restart]]></content>
      <categories>
        <category>服务器</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【转载】Linux /opt、/var挂载到单独分区]]></title>
    <url>%2F2018%2F12%2F23%2FLinux-opt%E3%80%81-var%E6%8C%82%E8%BD%BD%E5%88%B0%E5%8D%95%E7%8B%AC%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[Linux 上新建分区，参考教程：Linux 分区操作/opt 目录按照Linux /usr目录挂载到新分区此教程成功挂载。但是/var目录却一直未挂载成功。 下面给出/var目录的挂载步骤(/dev/sdb1为被挂载的分区)：1.创建一个挂载点：mkdir /storage。2.挂载 /dev/sdb1 到 /storage 目录上：mount /dev/sdb1 /storage。3.复制/var目录下的内容到/storage目录中：cp -pdr /var /storage。4.清空 /var目录 中的内容： rm -rf /var/* （通过ls命令看一下是否清空）。5.卸载 /dev/sdb1 : umount /dev/sdb1 。6.将 /dev/sdb1 挂载到 /var 上： mount /dev/sdb1 /var 。 然后通过 ls /var/ 命令查看 /var 中的内容。不过，我们会发现，原来的/var里的内容，现在被保存在了 /var/var/ 这样的目录下，而且还多了一个 lost+found 目录：为了和原来的 /var 保持一致，我们调整一下目录结构，依次执行下面三个命令：mv /var/var/* /var/rm -rf /var/varrm -rf /var/lost+found 设置开机自动挂载磁盘。打开 vim /etc/fstab 配置文件，在其后加上下面一句：/dev/sdb1 /var ext4 defaults 0 0不过，我们应该通过 blkid /dev/sdb1 查看一下磁盘分区UUID，将上面一句修改为：UUID=2d0a900b-4083-4d97-86f4-c66a0cd8249c /var ext4 defaults 0 0然后执行 mount -a 使得配置生效，或者重启。 参考原文https://ywnz.com/linuxjc/2219.html]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>目录挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux /usr目录挂载到新分区]]></title>
    <url>%2F2018%2F12%2F22%2FLinux-usr%E7%9B%AE%E5%BD%95%E6%8C%82%E8%BD%BD%E5%88%B0%E6%96%B0%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[一篇关于Linux下/usr目录挂载到指定分区的教程 一、备份 1sudo cp -r /usr /usrbak 二、为/usr新建一个分区：参考教程：Linux 分区操作确保新分区没有挂载到其他目录。分区需要格式化成ext4格式(会清空数据) 1sudo mkfs -t ext4 /dev/sdxx 三、将目标分区挂在到一个临时目录 1sudo mount /dev/sdxx /usrtmp 四、 将/usr的全部内容拷到/usrtmp目录 1sudo cp -r /usr/. /usrtmp 五、修改目录所有者及权限： 123sudo chown root:root /usrtmp/bin/sudosudo chmod 4755 /usrtmp/bin/sudosudo chmod +s /usr/lib/policykit-1/polkit-agent-helper-1 否则sudo命令不能使用 六、取消/usrtmp目录的挂载 1sudo umount /dev/sdxxx 七、挂载到/usr目录 1sudo mount /dev/sdxx /usr 八、设置开机自动挂载： 查看分区uuid：sudo blkid 执行以下命令：sudo vi /etc/fstab，添加： UUID=对应分区的uuid /usr ext4 defaults 0 1 九、重启或执行sudo mount -a 问题 ubuntu18.0.4 : 使用sudo命令 提示:sudo: /usr/bin/sudo必须属于用户 ID 0(的用户)并且设置 setuid 位參考: https://blog.csdn.net/u014696921/article/details/70057447 INCORRECT PERMISSIONS ON /USR/LIB/PO1KIT-AGENT-HELPER-1(NEEDS TO BE SETUID ROOT)參考: https://www.cnblogs.com/tl542475736/p/9461669.html deepin15.8 分区是挂载上了，应用菜单中fcitx应用图标不见了，没有这个工具就不能配置中文输入法了。 查看是否还有残留的fcitx相关安装包：sudo dpkg --get-selections | grep fcitx如果有，将列表中所有安装包删除，并清除相关依赖sudo apt autoremove 安装搜狗输入法: sudo apt-get install sogoupinyin也可以参考：https://blog.csdn.net/github_33809414/article/details/85055814 重启 文件管理器中右键open in new window as admin点击无效了，解决方法还未找到。有解决方法了以后再更新。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>目录挂载</tag>
        <tag>Linux目录挂载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 新建分区]]></title>
    <url>%2F2018%2F12%2F22%2FLinux-%E6%96%B0%E5%BB%BA%E5%88%86%E5%8C%BA%2F</url>
    <content type="text"><![CDATA[Linux下硬盘新建分区教程 对Linux分区的理解 硬盘接口：1.IDE接口：Linux对连接到IDE接口的硬盘使用/dev/hdx的方式命名，x一般为a,b,c等2.SCSI接口：Linux对连接到SCSI接口的硬盘使用/dev/sdx的方式命名，x一般为a,b,c等。其实Linux对连接到SCSI接口的硬盘是使用ID号区分的。ID范围是：0-15，ID为0的硬盘名为/dev/sda，类推。SCSI主控制器通常占用ID号7，所以SCSI接口最多可连接15个硬盘。 分区：1.分区是针对磁盘，并不是对某一个分区分区。2.硬盘使用前要进行分区，可分为主分区，扩展分区，逻辑分区。硬盘使用前要进行分区，可分为主分区，扩展分区，逻辑分区。3.一块硬盘最多4个主分区，最多1个扩展分区，逻辑分区可以有若干个。4.扩展分区会使用一个主分区的位置。理解为：主分区数+扩展分区数不能超过4个。5.主分区中不能再划分其他类型的分区，相当于一个逻辑磁盘。6.扩展分区本身不存储任何东西，不能直接使用，也不能格式化，只是用于区分逻辑分区。所以，扩展分区容量=n个逻辑分区容量之和，从而：硬盘的容量=主分区容量+扩展分区容量。7.其中1-4为主分区+扩展分区编号，逻辑分区编号从5开始。 查看当前磁盘及分区情况可以看到我这里有两个磁盘，sda、sdb。其中sda有5个分区(sda4为扩展分区，sda5为逻辑分区)、sdb只有1个分区。 对硬盘进行分区操作123456789101112131415161718192021222324252627282930fdisk /dev/sda //操作sda磁盘...Command (m for help): m //输出帮助信息 Command action a toggle a bootable flag //设置启动分区 b edit bsd disklabel //编辑分区标签 c toggle the dos compatibility flag //切换DOS兼容性标志 d delete a partition //删除一个分区 l list known partition types //列出分区类型 m print this menu //输出帮助信息 n add a new partition //建立一个新的分区 o create a new empty DOS partition table //创建一个新的空白DOS分区表 p print the partition table //打印分区表 q quit without saving changes //退出不保存设置 s create a new empty Sun disklabel 创建一个新的空太阳标签 t change a partition&apos;s system id //改变分区的ID u change display/entry units //改变显示的单位 v verify the partition table //检查验证分区表 w write table to disk and exit //保存分区表 x extra functionality (experts only) Command (m for help):n Command action e extended //扩展分区 p primary partition (1-4) //主分区（最多4个）pPartition number (1-4): 1 //分区号(卷标)First cylinder (1-2597, default 1): 1 // 起始扇区Last cylinder or +size or +sizeM or +sizeK (1-2597, default 2597):+100M // 结束扇区 或大小Command (m for help): w //保存刚才的配置信息。 案例 删除sda4、sda5以新建分区 新建扩展分区我这里应该是有碎片区域，导致默认的扇区起始位置没有紧接sda3扇区之后。一般扩展分区就需要包含剩下的全部空间，否则就浪费了。 在扩展分区中新建逻辑分区 继续分区，个数无限制，任意分配，我这里分配2个 其他：将分区格式化为NTFS格式：sudo mkfs -t ntfs /dev/sda6 参考：https://www.cnblogs.com/sangmu/p/6629594.htmlhttps://www.cnblogs.com/hanson1/p/7102206.htmlhttps://www.cnblogs.com/lbole/archive/2018/04/25/8904298.html]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>Linux分区</tag>
        <tag>分区</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[锐捷Linux客户端连接校园网]]></title>
    <url>%2F2018%2F12%2F17%2F%E9%94%90%E6%8D%B7Linux%E5%AE%A2%E6%88%B7%E7%AB%AF%E8%BF%9E%E6%8E%A5%E6%A0%A1%E5%9B%AD%E7%BD%91%2F</url>
    <content type="text"><![CDATA[Linux下锐捷客户端拨号上网教程 实验环境：成都信息工程大学(航空港校区)Deepin 15.8RG_Supplicant_For_Linux_V1.31.zip 一、下载并解压。 二、进入解压后的目录(rjsupplicant.sh所在目录)，打开终端(Terminal)，执行 1sudo ./rjsupplicant.sh --help 翻译如下： 认证客户端 - 使用帮助 -a --auth 认证方式(带参数，0表示无线认证，1表示有线认证；不指定时 采用上一次认证方式或者为有线认证) -d --dhcp dhcp方式(带参数，0表示静态IP认证，1表示使用动态获取IP方 式认证；不使用-d默认使用上一次配置) -n --nic 认证网卡(参数为网卡名称(参考-l结果)，不指定时采用上一次 使用的网卡或者第一张网卡) -s --service 服务(参数为服务名称(参考-l结果)，不指定时采用上一次使用 的服务或者第一个服务) -I --ssid 无线认证ssid(参数为ssid名称(参考-l结果)，不指定时采用上 一次使用的SSID或者第一个SSID，无线认证有效) -w --wlan 扫描无线网络: 无参数，可使用-n指定无线网卡，查看无线网络 列表 -u --user 用户名(参数为用户名，不指定时采用上一次使用的用户名) -p --password 密码(参数为密码，若未设定密码，运行程序后可重设定，默认 为空除非已经保存了密码) -S --save 保存密码(带参数，0表示不保存密码，1表示保存密码，不指定 时采用上一次配置) -q --quit 退出程序(不带参数，使用-q命令退出后台运行的客户端) -l --list 查看模式：无参数，指定该模式时，只查看信息，不认证。内容 包括：版本号，当前认证方式，当前网卡，当前服务(可选)，用 户名，服务列表(可选)，网卡列表。默认查看当前认证方式下信 息。 --comments 后台运行方式，输出日志在"/home/steven/下载/rjsupplicant/ x64/log/run.log"中 三、解绑原MAC地址(如果没有请忽略)进入校园网自助服务平台：http://10.254.241.18:8080/selfservice/ 我这里已经解绑过了。如果没有这一步，在连接时可能报错，如提示用户欠费等。 四、连接网络 根据自己情况设置连接参数。 -u 后跟的时用户名，一般是学号 -p 后跟的时密码 提示，Terminal窗口关闭，网络也会断开！！]]></content>
      <categories>
        <category>应用软件</category>
      </categories>
      <tags>
        <tag>校园网</tag>
        <tag>锐捷客户端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis第一次启动，SHUTDOWN时提示： (error) ERR Errors trying to SHUTDOWN. Check logs.]]></title>
    <url>%2F2018%2F09%2F13%2FRedis%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%90%AF%E5%8A%A8%EF%BC%8CSHUTDOWN%E6%97%B6%E6%8F%90%E7%A4%BA%EF%BC%9A-error-ERR-Errors-trying-to-SHUTDOWN-Check-logs%2F</url>
    <content type="text"><![CDATA[今天我也是第一次玩redis，从下载到安装到HelloWorld一切顺风顺水，谁知在最后关闭redis时，出现了(error) ERR Errors trying to SHUTDOWN. Check logs.错误。下面贴出我的解决过程（可能有点繁琐） 今天我也是第一次玩redis，从下载到安装到HelloWorld一切顺风顺水，谁知在最后关闭redis时，出现了(error) ERR Errors trying to SHUTDOWN. Check logs.错误。下面贴出我的解决过程（可能有点繁琐） 环境Ubuntu 18.04 LTSredis-4.0.11 错误重现 解决过程很明显不保存就能关闭，多半跟日志文件有关系，就直接查看配置文件中关于日志的配置，如下：进行如下修改：再次启动，居然报错：于是查看日志文件权限，发现只有root用户有写权限：修改日志文件权限：再次启动：再次关闭，问题依然，感觉一朝回到解放前：继续找问题, 大致跟刚才同样的思路，找那种没有写权限的地方，发现如下：于是，查看一下/usr/local/myredis文件夹权限，与之前日志文件权限一样：索性直接把目录改成自己定义的：修改/usr/local/myredis/db_files/目录权限：查看结果：希望能对你有所帮助！]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库字段级权限设计]]></title>
    <url>%2F2018%2F08%2F27%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%97%E6%AE%B5%E7%BA%A7%E6%9D%83%E9%99%90%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[单纯个人设计的，关于数据库字段权限控制的解决方案，并附代码实现。应该有很多不足，有不好的地方可以指出，或者有更好的想法可以评论，一起交流^-^ 一、 引言 业务场景 在一个类似数据统计的系统中，由于统计的数据较多，就有较多的表，每个表有较多的字段，但是又不想让每个用户都看到全部的表或者表的全部字段，比如一些重要的统计数据，应该是只有管理员才能看到的。 举个例子(无实际意义) ID name day_amount week_amount month_amount 1 脑点子 1,000 7,000 30,000 A用户只能看到name，day_amount 两个字段；B用户只能看到name，month_amount两个字段。 二、 解决方案 方案一 用户表、用户-表名-字段映射表优点：实现简单缺点：每个用户，每个表，每个字段都需单独设置，较为繁琐 方案二 用户表、角色表、用户-角色映射表、角色-表名-字段映射表(优先级低)、用户-表名-字段映射表(优先级高)优点：可通过角色实现批量设置权限；且也可对某个用户权限进行单独设置缺点：权限局限于数据库单个表。 方案三 用户表、角色表、用户-角色映射表、数据类型表、类型-表名-字段映射表(优先级低)、角色-类型映射表、用户-表名-字段映射表(优先级高)优点：在方案二的基础上，可实现跨表的权限控制。缺点：实现较为繁琐，要不要采用主要还是根据用户的需求 对外权限分配接口 三种都按照表名作为一级节点，字段作为二级节点来分配权限。 方案一只能按照单个用户来分配权限 方案二按照角色/单个用户来分配 方案三按照类型/单个用户来分配 存在的问题 1）数据库结构变更时，与该表相关的所有权限都需要重新设置 2）部分不在数据库的字段，如根据多列的值计算出来的属性字段的权限不好处理。 3）只支持单张表的查询，不支持多表查询 部分解决思路 问题2）的暂时的解决思路是：再单独建一张表——用户/角色/类型-JAVA类名-属性名映射表，再配合JSON序列化的属性过滤来实现权限控制。这种方式基本跟完全写死没什么区别，对后期的维护及扩展极不友好！ 问题3）的解决思路是：放弃掉SQL语句的连接查询，全部改成由代码控制 三、 代码最初是打算在Spring Aop的前置通知中通过修改目标方法参数来实现，但是通过源码发现封装目标方法参数的类是用final修饰的，所以后面换了种思路。 使用Spring的AbstractAutoProxyCreator自动代理实现，思路是通过条件判断决定是否要使用自动代理，要使用代理的话，就需要自己实现MethodInterceptor接口并重写其中invoke方法。 下面我贴出核心代码，文章最后会给出整个demo的链接 继承AbstractAutoProxyCreator类，重写getAdvicesAndAdvisorsForBean()方法 1234567891011121314151617181920212223242526public class BeanTypeAutoProxyCreator extends AbstractAutoProxyCreator &#123; @Override protected Object[] getAdvicesAndAdvisorsForBean(Class&lt;?&gt; beanClass, String beanName, TargetSource customTargetSource) throws BeansException &#123; return isMatch(beanClass) ? PROXY_WITHOUT_ADDITIONAL_INTERCEPTORS : DO_NOT_PROXY; &#125; /** * 判断是否是需要被代理的对象 * @param clazz 代理对象的类型 * @return */ private boolean isMatch(Class&lt;?&gt; clazz) &#123; //有两个Class类型的类象，一个是调用isAssignableFrom方法的类对象（后称对象a）， // 以及方法中作为参数的这个类对象(称之为对象b)，这两个对象如果满足以下条件则返回true，否则返回false： //a对象所对应类信息是b对象所对应的类信息的父类或者是父接口，简单理解即a是b的父类或接口 //a对象所对应类信息与b对象所对应的类信息相同，简单理解即a和b为同一个类或同一个接口 if (BaseMapper.class.isAssignableFrom(clazz)) &#123; return true; &#125; return false; &#125;&#125; 实现MethodInterceptor接口，重写invoke()方法 123456789101112131415161718192021222324252627282930313233public class MyMethodInterceptor implements MethodInterceptor &#123; @Autowired private SysAccess sysAccess; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; // 权限封装类 SysAccessCriteria result = null; int flag = -1; // 目标方法的参数 Object[] args = invocation.getArguments(); for (int i=0; i&lt;args.length; i++) &#123; // 只修改权限条件类型的参数 if(args[i] instanceof SysAccessCriteria)&#123; SysAccessCriteria sysAccessCriteria = (SysAccessCriteria) args[i]; result = sysAccess.getUserAceess(sysAccessCriteria); flag = i; &#125; &#125; // 修改目标参数 if(flag &gt;= 0 &amp;&amp; result != null)&#123; args[flag] = result; &#125; // 执行目标方法 Object object = invocation.proceed(); return object; &#125;&#125; 配置到Spring配置文件中 12345678910&lt;bean id="myMethodInterceptor" class="com.ysl.access.proxy.MyMethodInterceptor"&gt;&lt;/bean&gt;&lt;!--配置自动代理--&gt;&lt;bean id="myBeanTypeAutoProxyCreator" class="com.ysl.access.proxy.BeanTypeAutoProxyCreator"&gt; &lt;!--父类属性--&gt; &lt;property name="interceptorNames"&gt; &lt;list&gt; &lt;value&gt;myMethodInterceptor&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; Demo完整代码(数据库文件在resources目录下)：https://github.com/andus-top/columns-accsss单纯个人设计，应该有很多不足，有不好的地方可以指出，或者有更好的想法可以评论，一起交流。 参考：https://blog.csdn.net/lilongjiu/article/details/78047051]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>Mysql</tag>
        <tag>权限设计</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vue-axios 前后端分离 跨域访问的实现]]></title>
    <url>%2F2018%2F08%2F22%2Fvue-axios-%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB-%E8%B7%A8%E5%9F%9F%E8%AE%BF%E9%97%AE%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[一篇关于vue-axios与ssm工程跨域配置的记录… 一. 基本环境 前端vue：2.5.6axios：0.18使用vue脚手架构建项目。参照：webstorm搭建vue项目 后台ssm框架 前后端数据采用json格式传输 二. 前端配置 axios配置 安装 1npm install axios --save 在所需组件中引入axios 1import axios from 'axios'; 在&lt;script&gt;中配置axios参数 123456const myaxios = axios// 不跨域的axios// 如果要跨域的话, 对axios进行一些设置,当前使用的是跨域的const axiosInstance = axios.create(&#123; headers: &#123;'Content-Type': 'application/json;charset=utf-8'&#125;,// 设置传输内容的类型和编码 withCredentials: true,// 指定某个请求应该发送凭据。允许客户端携带跨域cookie，也需要此配置&#125;); 代理配置打开config/index.js文件，找到dev对象里面的proxyTable修改为如下内容： 12345678910proxyTable: &#123; '/api': &#123; target:'http://127.0.0.1:80', // secure: false, // 如果是https接口，需要配置这个参数 changeOrigin:true, pathRewrite:&#123; '^/api': '' &#125; &#125;&#125;, target 的参数就是你要访问的服务器地址, 你在代码里面写/api就等于写了这个地址 , 比如我要访问http://127.0.0.1:80/vue-ssm/login这个接口在代码里面只需写/api/vue-ssm/login就可以了 使用axios发送请求 123456789// 跨域访问axiosInstance.post('/api/vue-ssm/login', 请求的参数,).then( response =&gt; &#123; console.log(response.data);// response.data为后端返回的具体数据 alert("请求成功");&#125;).catch( error =&gt; &#123; alert("请求失败");&#125;); axios详细参数配置可以参考：Axios 中文说明 三. 后端配置 使用SpringMvc的HandlerInterceptorAdapter拦截器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class RquestInterceptor extends HandlerInterceptorAdapter &#123; /** * 预处理回调方法，实现处理器的预处理（如检查登陆），第三个参数为响应的处理器，自定义Controller * 返回值：true表示继续流程（如调用下一个拦截器或处理器）；false表示流程中断（如登录检查失败）， * 不会继续调用其他的拦截器或处理器，此时我们需要通过response来产生响应； */ @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 允许客户端携带跨域cookie // 当Access-Control-Allow-Credentials设为true的时候，Access-Control-Allow-Origin不能设为星号 response.setHeader("Access-Control-Allow-Credentials", "true"); // 允许指定域访问跨域资源 //response.setHeader("Access-Control-Allow-Origin", "http://127.0.0.1:9006, http://127.0.0.1:8080"); response.setHeader("Access-Control-Allow-Origin", request.getHeader("Origin"));// * // 允许浏览器发送的请求消息头 response.setHeader("Access-Control-Allow-Headers", request.getHeader("Access-Control-Request-Headers"));// * // 允许浏览器在预检请求成功之后发送的实际请求方法名 response.setHeader("Access-Control-Allow-Methods", request.getHeader("Access-Control-Request-Method")); // 设置响应数据格式 response.setHeader("Content-Type", "application/json"); // 查看请求方法 String method= request.getMethod(); System.out.println(method); return true; &#125; /*下面的方法可以不重写*/ /** * 后处理回调方法，实现处理器的后处理（但在渲染视图之前），此时我们可以通过modelAndView（模型和视图对象） * 对模型数据进行处理或对视图进行处理，modelAndView也可能为null。 */ @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; super.postHandle(request, response, handler, modelAndView); &#125; /** * 整个请求处理完毕回调方法，即在视图渲染完毕时回调，如性能监控中我们可以在此记录结束时间并输出消耗时间， * 还可以进行一些资源清理，类似于try-catch-finally中的finally，但仅调用处理器执行链中 */ @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123; super.afterCompletion(request, response, handler, ex); &#125; /** * 处理异步请求 * 不是HandlerInterceptor的接口实现，是AsyncHandlerInterceptor的 * AsyncHandlerInterceptor实现了HandlerInterceptor */ public void afterConcurrentHandlingStarted(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; super.afterConcurrentHandlingStarted(request, response, handler); &#125;&#125; 配置到SpringMvc配置文件 123456789&lt;mvc:interceptors&gt; &lt;!-- 注意拦截器的执行顺序，会按照这里配置顺序执行 --&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path="/**" /&gt;&lt;!--匹配所有路径--&gt; &lt;bean class="com.ysl.interceptor.RquestInterceptor" /&gt; &lt;/mvc:interceptor&gt; &lt;!-- 其他拦截器 --&gt; &lt;mvc:interceptor&gt;&lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 好了，这就是我使用axios实现跨域访问的全过程，希望对大家有所帮助。 参考：https://blog.csdn.net/huang100qi/article/details/77132096https://blog.csdn.net/qq_22844483/article/details/78661030https://www.jianshu.com/p/1e8d088c2be9https://segmentfault.com/a/1190000015597029]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>Vue</tag>
        <tag>axios</tag>
        <tag>跨域</tag>
      </tags>
  </entry>
</search>
